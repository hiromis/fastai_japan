0:00:00.520,0:00:04.020
それでは皆さん、こんにちは。そして、

0:00:04.690,0:00:07.319
コーダのための実践的ディープラーニングへお帰りなさい。今回はレッスン2です。

0:00:08.260,0:00:10.000
そして、

0:00:10.000,0:00:12.599
前回のレッスンでは

0:00:13.570,0:00:19.500
最初のモデルのトレーニングを開始しました。実際にトレーニングがどのようなものかは考えずに

0:00:19.500,0:00:22.439
より高いレベルでどうなっているかを見ていました。

0:00:23.109,0:00:24.519
そして、

0:00:24.519,0:00:26.519
私たちは

0:00:26.560,0:00:28.240
機械学習とは

0:00:28.240,0:00:30.510
何であるか、そしてどのように機能するかを学びました。

0:00:31.390,0:00:33.390
さらに

0:00:34.180,0:00:35.860
私たちは

0:00:35.860,0:00:37.420

0:00:37.420,0:00:43.260
機械学習がどう動くかに基づいて考えると、そもそも機械学習にはできることの制限があることを認識しました。

0:00:43.719,0:00:46.259
そして、そういった制限についても話しました。

0:00:46.260,0:00:50.820
さらに、機械学習モデルを訓練した後に、

0:00:51.309,0:00:56.249
通常のプログラムのように振る舞うプログラムができあがることについても話しました：入力、中間にあるもの（モデル）、そして

0:00:56.829,0:00:58.449
出力です。

0:00:58.449,0:01:00.449
さて、今日は

0:01:00.850,0:01:02.850
この話を終わらせてから

0:01:03.280,0:01:05.820
実際に訓練したモデルを

0:01:06.400,0:01:09.119
本番環境にのせる方法とそうする上での問題点について話します。

0:01:10.000,0:01:12.000
そして

0:01:12.939,0:01:14.939
2冊の本について再度説明したいです。

0:01:15.369,0:01:19.438
失礼、実際にはノートブックですね。

0:01:20.590,0:01:23.100
一つは、fast bookで

0:01:24.009,0:01:29.249
オライリー本の執筆で実際に使ったノートブックのレポジトリで、

0:01:30.220,0:01:32.939
実際に

0:01:33.460,0:01:35.939
私が伝えることとプラスアルファを動かすことができます。

0:01:36.430,0:01:43.319
そして、もう一つはこのコースのレポジトリで、レッスンで扱うものと同じノートブックですが、

0:01:43.320,0:01:46.680
あなた方の理解のために説明文などは消してあります。

0:01:47.140,0:01:52.439
ですので、それらのノートブックは授業を聞きながら実行したり、自分で色々試したりできます。

0:01:52.780,0:01:58.949
そして、ビデオと本を行ったり来たりすることも、どちらか片方を先に終わらせてからもう片方を進めることもできます。

0:01:59.530,0:02:04.320
それから、このノートブックを見て、

0:02:04.320,0:02:09.239
セクションの内容を思い出して、コードを実行して、何が起きるかを見て、コードを少し変えるとどうなるかを確認するといった使い方をしてください。

0:02:11.830,0:02:13.830
さて、私たちが

0:02:16.780,0:02:19.860
見ていたこの行ではデータセットを

0:02:20.800,0:02:22.800
作るために

0:02:23.080,0:02:26.790
おそらく最も重要であろうラベルの付け方を教えたり

0:02:27.370,0:02:31.230
私たちはラベル付けの重要性についても話しましたね

0:02:31.230,0:02:35.250
そして、今回は、犬と猫のデータセットを使っています。

0:02:35.290,0:02:39.059
このデータセットではファイル名の頭文字が大文字か小文字かでラベルが決まっています。

0:02:39.970,0:02:43.139
これは、このデータセットがそう作られているからです。

0:02:44.320,0:02:50.699
そして、特に、このvalid_pct=0.2というものを確認しました。

0:02:50.700,0:02:55.679
これは、バリデーションセットを作るもので、バリデーションセットについては今後詳しく話したいと思っています。

0:02:56.740,0:03:00.540
まず最初に指摘しておきたいのは、この特定のラベリング関数は

0:03:04.330,0:03:07.320
真か偽のどちらかを返すということです。

0:03:09.610,0:03:15.179
このデータセットには、後ほど見るように、37種類の猫と犬の実際の品種も

0:03:15.910,0:03:20.429
このデータセットには、後ほど見るように、37種類の猫と犬の実際の品種も含まれており、ファイル名から取得できます。

0:03:21.910,0:03:29.249
これら2つのケースでは、カテゴリを予測します。それらは、「猫なのか犬なのか？」、

0:03:30.190,0:03:34.739
「ジャーマンシェパードなのかビーグルなのかラグドールなのか？」といったものです。

0:03:35.440,0:03:42.389
カテゴリーを予測しようとしているとき、つまりラベルがカテゴリーであるとき、それを分類モデルと呼びます。

0:03:43.750,0:03:45.519
一方で、

0:03:45.519,0:03:50.879
動物の年齢や身長、

0:03:51.640,0:03:52.959
あるいは、

0:03:52.959,0:03:58.469
そういったものを予測しようとするとき、対象は13.2や26.5といった連続値です。

0:03:59.110,0:04:05.729
数字を予測しようとするときはいつでも、そのラベルは回帰と呼ばれる数字です。

0:04:05.730,0:04:07.860
これらが主に用いられる2種類のモデルで、

0:04:08.410,0:04:13.919
分類と回帰と呼ばれるものです。これは非常に重要な専門用語なのでしっかり覚えてください。回帰モデルは、

0:04:14.769,0:04:19.379
温度や場所などの1つ以上の数値量を予測しようとします。

0:04:20.400,0:04:27.229
これは少し紛らわしいですね。時々、回帰という言葉は

0:04:27.810,0:04:31.310
線形回帰と呼ばれる特定のモデルをさすことがあります。

0:04:33.180,0:04:39.139
しかし、線形回帰は回帰ではないので、大変紛らわしいです。

0:04:39.419,0:04:43.189
線形回帰は特定の種類の回帰に過ぎないことに注意してください。

0:04:44.580,0:04:48.590
回帰について話し始めると、線形回帰ではないのに多くの人は線形回帰のことを話していると思い込むでしょう。

0:04:50.790,0:04:57.589
さて、私はこのvalid_percent 0.2について話したいと思います。先ほど説明したように、

0:04:58.470,0:05:04.850
先ほど説明したように、valid_percentはデータの20パーセントを別のバケツに入れておき、

0:05:05.220,0:05:07.220
モデルを訓練するときには

0:05:07.650,0:05:10.190
そのデータを使いません。

0:05:10.770,0:05:17.780
そのデータは、モデルがどれだけ正確であるかを示すためだけに使用されます。

0:05:20.010,0:05:22.010
ですから、もしあなたがあまりにも長い間、

0:05:23.070,0:05:27.229
あるいは十分なデータを持たずに、あるいはパラメータが多すぎるモデルを使って

0:05:27.599,0:05:33.889
トレーニングした場合、しばらくするとモデルの精度は実際に悪くなります。これをオーバーフィットと呼びます。

0:05:34.349,0:05:38.419
そこで、オーバーフィットしていないことを確認するために検証セット

0:05:38.940,0:05:40.940
を使用します。

0:05:42.000,0:05:45.019
次に見たコードの行はこの行で、

0:05:45.750,0:05:51.500
Learnerと呼ばれるものを作成しました。Learnerについては今後深く学ぶ予定ですが、基本的には

0:05:51.780,0:05:53.780
データセットと

0:05:54.990,0:05:56.130
あなたの

0:05:56.130,0:05:59.630
アーキテクチャ、つまり最適化する数学的な関数を持ちます。

0:06:00.630,0:06:07.699
つまり、learnerとは関数がデータのラベルに最も適合するような

0:06:07.949,0:06:11.779
パラメータを見つけ出すものです。

0:06:12.449,0:06:14.689
詳しく説明しましたが、要するに、

0:06:15.240,0:06:19.760
このresnet34という関数がアーキテクチャの名前で、

0:06:19.979,0:06:23.449
このアーキテクチャは、コンピュータビジョンに適している

0:06:24.030,0:06:24.930
ことが知られています。

0:06:24.930,0:06:30.829
本当の名前はResNetで34は層の数を表していて、この数字が大きくなると、

0:06:31.080,0:06:38.330
より多くのパラメータが学習に使われるのでよりメモリを消費し、さらに過学習しやすくなりますが、

0:06:39.990,0:06:41.990
より複雑なモデルを作れます。

0:06:42.480,0:06:46.759
しかし、今はmetrics=error_rateに注目しましょう。

0:06:47.580,0:06:54.199
ここでは、バリデーション時に使いたい関数を列挙し、

0:06:54.330,0:06:56.330
それらの結果は毎エポック後に

0:06:56.430,0:06:58.430
プリントされます。

0:06:58.770,0:07:00.120
エポックとは、

0:07:00.120,0:07:04.730
データセットの全ての画像を見ることを意味します。

0:07:05.670,0:07:12.259
つまり、データセットを1周するたびに、モデルに関する情報がプリントされ、

0:07:12.450,0:07:17.300
最も重要なことですが、これらのメトリクスの結果もプリントされます。

0:07:17.550,0:07:23.240
error_rateはメトリクスの1つで、バリデーションセットのうち，何%の画像が

0:07:23.520,0:07:26.570
誤って分類されたかをプリントします。

0:07:28.920,0:07:34.820
つまり、バリデーションセットを使って予測の質を計算しているのです。

0:07:35.370,0:07:40.370
1 - error_rateで計算される正確度も一般的なメトリクスです。

0:07:41.190,0:07:45.410
先週のレッスンで大変重要なことを話しました。アーサー・サミュエルは

0:07:46.680,0:07:53.329
機械学習における重要な損失という考えを持っていました。モデルの性能がどれくらい良いかを把握するためには

0:07:54.300,0:08:01.759
何らかの方法が必要で、パラメータを変更したときに、どのパラメータのセットが性能を良くするか悪くするかを

0:08:02.880,0:08:04.880
把握することができます。

0:08:05.190,0:08:07.669
パフォーマンスの基準は損失ですが、

0:08:08.190,0:08:09.900
必ずしも

0:08:09.900,0:08:13.489
メトリクスと同じとは限りません。

0:08:14.670,0:08:19.850
その理由は少し微妙で、今後のレッスンで数学を掘り下げていくと

0:08:19.850,0:08:21.850
詳細が見えてきますが 、

0:08:22.560,0:08:26.869
基本的には関数が必要で、

0:08:27.390,0:08:33.470
パラメータを少しだけ上げたり下げたりして、 損失が少し良くなったか悪くなったか

0:08:33.470,0:08:39.739
を見ることができる損失関数が必要です。

0:08:39.990,0:08:43.370
予測が犬から猫に変わるほどパラメータを変化させるわけではない

0:08:43.920,0:08:49.499
ので精度やエラーレートでは不十分なのです。

0:08:49.500,0:08:53.609
予測が同じならエラーレートも同じはずです。

0:08:54.400,0:08:59.249
損失とメトリックは密接に関連していますが、メトリックはあなたが気にしているもので、

0:08:59.980,0:09:07.110
損失はパフォーマンスの測定値としてコンピュータがパラメータをどのように更新するかを決定するために使用しているものです。

0:09:08.920,0:09:10.920
損失はパフォーマンスの測定値として

0:09:11.860,0:09:17.490
コンピュータがパラメータをどのように更新するかを決定するために使用しているものです。

0:09:17.860,0:09:21.210
FastAIは常に検証セットを使用してメトリクスを出力します。

0:09:22.480,0:09:29.970
オーバーフィッティングは重要で、学習しているデータだけでなく、

0:09:30.640,0:09:37.949
学習アルゴリズムが見たことのないデータに適合するモデルをどうやって見つけるかということです。

0:09:41.170,0:09:42.760
つまり

0:09:42.760,0:09:43.960
過学習は

0:09:43.960,0:09:45.520
モデルが

0:09:45.520,0:09:47.520
不正行為が

0:09:47.890,0:09:51.059
している結果とも言えます。

0:09:51.060,0:09:51.670


0:09:51.670,0:09:56.969
モデルは、「これと同じ写真を見たことがある、それは猫の写真だ」と言うことでごまかすことができます。

0:09:57.400,0:10:00.719
つまり、一般的に猫がどのように見えるかを学習していないかもしれませんが

0:10:00.720,0:10:01.360
、画像1〜4と8が猫で、

0:10:01.360,0:10:07.050
画像2〜3と5が犬であることを覚えているだけで

0:10:07.450,0:10:12.989
猫がどういうものかという概念は学習していないのです。これが、絶対に避けたいタイプのズルです。

0:10:12.990,0:10:14.990
つまり、モデルが特定のデータセットを

0:10:15.250,0:10:17.250
丸暗記するのは避けたいのです。

0:10:17.980,0:10:19.980
なので、バリデーションデータを作るのです。

0:10:20.530,0:10:25.559
画面に表示されている文章は本からコピーしてきました。

0:10:26.950,0:10:29.009
検証データを分割して、

0:10:30.010,0:10:35.729
モデルが学習中にそれを見ないようにすれば、完全にデータに汚染されないので、ごまかすことはできません。

0:10:36.790,0:10:38.849
そうではありません。ごまかすことはできます。

0:10:39.460,0:10:45.599
ごまかす方法としては、モデルをフィットさせて結果と検証セットを見て、

0:10:46.120,0:10:48.719
何かを少し変更して別のモデルをフィットさせて検証セットを見て、

0:10:48.720,0:10:52.200
何かを少し変更して、検証セットが一番良さそうなものが見つかるまで

0:10:53.050,0:10:55.770
100回くらい繰り返してみることができます。

0:10:56.350,0:10:58.710
そうするとバリデーションセットに適合しているかもしれませんね。

0:10:59.230,0:11:04.779
ですから、もしこれを本当に厳密に行いたいのであれば、テストセットと呼ばれる3番目のデータを用意しておくべきです。

0:11:05.330,0:11:11.470
訓練にもメトリクス計算にも使わないテストデータを作るのです。

0:11:12.320,0:11:15.249
これは実際には、プロジェクト全体が終了するまで見ることはありません。

0:11:15.920,0:11:19.149
これは、Kaggleのようなコンペテイションプラットフォームで使われているものです。

0:11:20.060,0:11:23.380
Kaggleでは、コンペテイション後、

0:11:24.200,0:11:26.560
見たことのないデータセットに対して

0:11:27.260,0:11:29.500
パフォーマンスが測定されます。

0:11:30.410,0:11:32.410


0:11:32.990,0:11:34.990
これは本当に有益なアプローチで、

0:11:35.390,0:11:40.030
自分でモデリングをしていなくても、実際にそうするのは素晴らしいアイデアです。

0:11:40.700,0:11:42.230


0:11:42.230,0:11:44.529
もしあなたがベンダーを見ていて、

0:11:44.530,0:11:51.550
今日はIBMかGoogleかMicrosoftにしようと決めようとしていて、彼らのモデルがどれだけ素晴らしいかを見せてくれているなら、

0:11:52.820,0:11:56.379
あなたがすべきことは、「OK、あなたはモデルを作成して、

0:11:56.780,0:12:02.019
私は私のデータの10％を保持して、あなたには全く見せないようにします。

0:12:02.450,0:12:08.050
あなたたちが作成したモデルをあなたたちが使えなかったテストデータで評価します。

0:12:09.320,0:12:11.320


0:12:12.470,0:12:14.210
さて、検証セットとテストセットを

0:12:14.210,0:12:16.840
引き出すのはちょっと微妙ですが、

0:12:17.690,0:12:22.149
これは簡単な小さなデータセットの例で、

0:12:23.000,0:12:28.419
これはレイチェルが書いた、効果的な検証セットの作成についての素晴らしいブログもってきました。

0:12:29.390,0:12:35.170
基本的には、ある種の季節性をもつデータセットであることがわかります。

0:12:35.750,0:12:38.529
さて、「Ok, FastAI, valid_percent of 0.2で

0:12:39.230,0:12:43.570
data loaderを作りたい」と言うと、次のようになります。

0:12:44.420,0:12:48.250
ランダムにいくつかのドットを間引いていますね？

0:12:49.190,0:12:51.190


0:12:51.380,0:12:53.210
これは良くありません。

0:12:53.210,0:12:58.449
ドットが他のドットの真ん中にあるので、ズルをすることができます。

0:12:58.940,0:13:00.670
こんなことは実世界では起きませんよね？

0:13:00.670,0:13:05.110
実際に何が起こるかというと、これは日付ごとの売上高ですが、

0:13:05.110,0:13:08.620
来週の売上高を予測したいと思います。対象は

0:13:09.200,0:13:12.539
14日前、18日前、29日前の売上ではありません。

0:13:13.089,0:13:18.178
ここで効果的な検証セットを作成するために実際に必要なのは、ランダムに行うのではなく、

0:13:19.179,0:13:20.649
最後の部分を

0:13:20.649,0:13:22.149
切り落とすことです。

0:13:22.149,0:13:29.368
これは全てのKaggleコンテストで起こることで、例えば、時間が関係していて、

0:13:29.369,0:13:34.979
予測しなければならないのは、最後に与えられたデータポイントの２週間後くらいのことで、

0:13:35.889,0:13:38.459
あなた達のテストセットでもこうすべきです。

0:13:38.920,0:13:42.569
繰り返しになりますが、ベンダーを抱えていて、

0:13:42.569,0:13:48.269
あなた方のモデリングが完了したら、私たちが提供したデータセットの1週間後の

0:13:48.269,0:13:50.789
データセットでモデルを評価します。

0:13:51.339,0:13:55.018
あなた達は再学習などはできません。なぜならこれが実世界で起きることだからです。

0:13:56.619,0:14:00.119
質問があります。過学習は訓練誤差が

0:14:00.639,0:14:07.048
バリデーション誤差より小さくなることだと聞いたことがあります。この経験則はあなたの説明と同等ですか？

0:14:07.569,0:14:09.420
良い質問ですね。

0:14:09.420,0:14:14.429
おそらく、質問は、訓練誤差と検証誤差の比較に関してですね。

0:14:15.189,0:14:17.519
fastaiでは訓練誤差をプリントせず、

0:14:18.040,0:14:24.449
エポックの終わりに、トレーニングセットとバリデーションセットに対する損失関数の値を

0:14:24.449,0:14:26.789
プリントします。

0:14:28.329,0:14:32.428
十分長く訓練を続けた場合、

0:14:33.160,0:14:36.449
訓練誤差も検証誤差も小さくなるでしょう。

0:14:37.480,0:14:39.480
なぜなら、定義上、

0:14:40.989,0:14:45.449
損失関数は、損失関数が低いほど良いモデルであるように定義されているからです。

0:14:46.329,0:14:48.329
過学習し始めたら

0:14:48.369,0:14:55.949
パラメータは改善されているので訓練誤差は減少し続けますが、

0:14:57.790,0:15:00.360
検証誤差が増加し始めます。

0:15:01.149,0:15:06.928
なぜなら、モデルがトレーニングセットの特定のデータに適合し始めているからです。そうすれば訓練誤差は小さくなりますから。

0:15:06.929,0:15:10.319
しかし検証誤差に関しては当てはまりません。ですので検証誤差は悪くなります。

0:15:11.470,0:15:13.329
とはいえ、

0:15:13.329,0:15:14.769
このことは必ずしも

0:15:14.769,0:15:20.128
過学習、もしくは悪い意味での過学習しているとは言い切れません。

0:15:20.379,0:15:23.639
実際に、検証誤差は増え始めているが、

0:15:24.279,0:15:30.118
検証正確度あるいは誤差率などのメトリクスは良くなっていることもありうるのですから。

0:15:31.269,0:15:33.220
過学習を数式的に

0:15:33.220,0:15:37.290
表せすには損失関数について学ぶ必要があるのでここでは行いません。

0:15:37.290,0:15:43.379
検証誤差ではなく、メトリクスの値が悪化していないか見るのが

0:15:44.110,0:15:48.389
大事と認識しておいてください。

0:15:49.749,0:15:51.749
大変素晴らしい質問でした。

0:15:55.689,0:16:03.178
次に学ぶべき重要なことは、転移学習と呼ばれるものです。

0:16:03.699,0:16:05.699
なぜlearn.fitなんでしょう？

0:16:06.220,0:16:08.459
fine tuningは転移学習で行うものです。

0:16:08.980,0:16:16.559
転移学習では、異なるタスクで訓練された学習済みモデルを活用します。

0:16:16.809,0:16:23.969
専門用語の説明にさらに専門用語が必要ですね。学習済みモデルとは何でしょう？

0:16:23.970,0:16:26.850
先ほど、今使っているアーキテクチャはresnet34とお伝えしました。

0:16:27.819,0:16:32.279
resnet34を持ってくる時、それはただの数式で、

0:16:32.619,0:16:37.169
大量のパラメータが使われていて、私たちが機械学習で調整しようとします。

0:16:40.420,0:16:46.649
ImageNetという1000クラス合計130万枚の画像データセットがあります。

0:16:47.949,0:16:52.618
キノコや動物、飛行機やハンマーなどが含まれます。

0:16:54.730,0:16:59.189
かつて、ImageNetのコンペテイションが毎年開催されていて、研究者達はより

0:16:59.529,0:17:03.448
より良い分類器を作ろうと努力していました。

0:17:04.240,0:17:05.889
そこで作られたモデル

0:17:05.889,0:17:12.359
のパラメータはインターネットで公開されていて、今では誰もが使えます。

0:17:12.610,0:17:17.130
つまり、それらをダウンロードする時、アーキテクチャではなく学習済みモデルが手に入ります。

0:17:17.230,0:17:20.730
そのモデルは写真に写っている1000種類の物体を

0:17:21.880,0:17:24.510
識別できます。

0:17:25.779,0:17:31.169
これは、たまたまその1000クラスを正確に認識するものが必要な場合を除いては、あまり有用ではないでしょう。

0:17:32.260,0:17:34.260
しかし、実際には、

0:17:36.370,0:17:42.209
その重みから始めて、新しいデータセットでさらに数エポック訓練すれば、

0:17:43.000,0:17:46.949
事前に訓練されたモデルから始めなかった場合よりも

0:17:47.230,0:17:53.610
はるかに正確なモデルができあがります。なぜかはすぐに確認します。

0:17:54.610,0:17:58.829
この移動学習のアイデアは、直感的に理解できますよね？

0:18:00.429,0:18:04.649
ImageNetにはすでに猫と犬が登録されていて、

0:18:04.650,0:18:06.779
これは猫でこれは犬だと言うことができますが、

0:18:06.779,0:18:10.889
ImageNetには登録されていない多くの犬種を認識するようなことをしたいと思います。

0:18:11.529,0:18:18.719
猫と犬、飛行機とハンマーを認識するためには、

0:18:19.720,0:18:26.339
金属はどんな形をしているのか、毛皮はどんな形をしているのか、などを理解する必要があります。ですから、

0:18:26.340,0:18:31.770
この犬種は耳が尖っている、これは金属だから犬のはずがないといったことがわかります。

0:18:32.529,0:18:37.709
これらの概念はすべて、事前に訓練されたモデルによって暗黙的に学習されます。

0:18:38.470,0:18:42.839
ですから、事前に訓練されたモデルから始めれば、これらすべての特徴をゼロから

0:18:43.630,0:18:47.849
学習する必要はありません。

0:18:48.370,0:18:55.679
少ないデータと計算力でより良いモデルを獲得できるということは非常に重要ですから

0:18:56.080,0:19:01.500
FastAIライブラリとこのコースは転移学習にフォーカスしています。

0:19:03.820,0:19:10.590
質問です。誤差率とメトリクスの違いを説明してください。

0:19:14.559,0:19:16.559
わかりました。

0:19:19.029,0:19:21.029
誤差率は

0:19:21.460,0:19:27.840
メトリクスの1つです。ラベルの候補はたくさんあります。例えば、犬と猫の年齢などです。

0:19:28.360,0:19:30.599


0:19:31.240,0:19:33.220


0:19:33.220,0:19:34.510
この場合、

0:19:34.510,0:19:39.569
メトリクスは、予測と実際の年齢のズレでしょう。

0:19:41.590,0:19:48.209
これがメトリクスになるでしょうが、一方で犬と猫の識別を行う場合、

0:19:48.700,0:19:50.669
メトリクスは

0:19:50.669,0:19:52.669
誤分類の割合になるでしょう。

0:19:53.230,0:20:00.809
今説明したものが誤差率です。誤りは1つのメトリクスです。それは、

0:20:01.359,0:20:05.879
モデルがどの程度できるかを測り、私たちが一番気にするものです。

0:20:06.039,0:20:09.959
ですから、メトリクスとはfastaiが提供しているものを使うか自分で定義してください。

0:20:11.019,0:20:13.019


0:20:15.820,0:20:17.769
損失は

0:20:17.769,0:20:22.049
前回のレッスンで説明しましたが、簡単におさらいしましょう。

0:20:22.059,0:20:24.059
もし覚えてなかったら前回のレッスンを確認してください。

0:20:24.970,0:20:29.939
アーサー・サミュエルは、機械学習モデルにはパフォーマンスの指標が必要で、

0:20:31.269,0:20:38.638
それは私たちがパラメータを調整した時にパフォーマンスがどう変化したか確認できるものとしています。

0:20:39.609,0:20:41.609
先ほど申し上げましたが、

0:20:42.519,0:20:44.519
中には

0:20:44.710,0:20:50.999
パラメータが変わっても全く変わらないメトリクスもありますが、 それはメトリクスとしては不適切です。

0:20:51.609,0:20:56.789
なぜなら、パラメータを調整してより良いパフォーマンスのものを見つけるのが目的ですから。

0:20:56.789,0:20:59.309
よく異なる関数を使います。

0:20:59.309,0:21:01.309
それは損失関数というもので、

0:21:01.389,0:21:07.139
パフォーマンスの指標で、アルゴリズムがより良いパラメータを見つけるのに使う関数です。

0:21:07.509,0:21:13.229
損失関数はメトリクスに似たもので、

0:21:13.869,0:21:20.159
どんな微小なパラメータの変化をも検知する必要があります。

0:21:20.159,0:21:21.159
ですので、

0:21:21.159,0:21:22.779
損失関数がどうなっているかを

0:21:22.779,0:21:27.659
知るためには数学を勉強しなければならないので、次回以降のレッスンで教えます。

0:21:31.450,0:21:33.450
素晴らしい質問をありがとう。

0:21:35.499,0:21:41.579
fine tuningは転移学習のテクニックで、

0:21:42.519,0:21:45.508
スライドじゃなく、写真を見せている時に、

0:21:51.350,0:21:56.600
fine tuningは転移学習のテクニックで、これは、重み、これは厳密には不正確で

0:21:56.600,0:22:02.809
パラメータと言うべきですが、事前学習済みモデルのパラメータが追加の数エポックの学習で更新します。

0:22:03.030,0:22:05.689
この時は事前学習とは違うタスクのデータセットを使います。

0:22:05.690,0:22:13.400
今回は事前学習はImageNetで私たちのタスクは犬か猫を認識するものです。

0:22:15.990,0:22:17.990


0:22:18.690,0:22:24.650
標準では、fastaiはこの微調整を1エポックのみ行います。

0:22:25.799,0:22:32.779
つまり、データセットの画像を全て見て、事前学習済みモデルを部分的に調整して

0:22:33.390,0:22:36.349
パフォーマンスを改善します。このモデルは

0:22:36.870,0:22:39.409
新たなデータセットで機能するものです。

0:22:40.020,0:22:41.730
その後、

0:22:41.730,0:22:44.900
好きなだけモデル全体を更新しますが、

0:22:45.000,0:22:48.319
これは少し発展的なものです。

0:22:48.320,0:22:52.460
これがどう機能するかは今後のレッスンで説明します。

0:22:53.789,0:22:57.139
さて、転移学習はなぜうまくいくのでしょうか？

0:22:58.049,0:23:04.099
2012年のImageNetコンペティションの優勝者であるZeilerとFurgusの論文を

0:23:05.700,0:23:07.700
チェックしましょう。

0:23:08.340,0:23:09.809
興味深いことに、

0:23:09.809,0:23:16.309
彼らの考えは、モデル内部の可視化に基づいています。可視化はしばしば

0:23:16.470,0:23:19.459
素晴らしい結果を得る上で非常に重要です。

0:23:20.520,0:23:25.489
彼らができたことは、resnet34は34層ありますが、

0:23:26.400,0:23:28.320
彼らは

0:23:28.320,0:23:34.909
7層からなり、当時はそれだけで巨大とされていましたが、ImageNetのチャンピオンモデルであるAlexNetを見ました。

0:23:35.580,0:23:40.580
著者らは、最初の層のパラメータは

0:23:40.950,0:23:46.340
どのようなものかを疑問に思い、可視化する方法を編み出しました。

0:23:46.340,0:23:49.340
最初の層に対応する図は

0:23:50.970,0:23:56.900
沢山あるのですが、ここには9つ載せてあり、

0:23:57.960,0:24:00.620
それぞれどんな見た目かがわかります。

0:24:00.620,0:24:04.219
1つは左上から右下にかけての斜め線を認識しています。

0:24:04.470,0:24:04.960


0:24:04.960,0:24:08.039
また別のものは左下から右上への線を見つけます。

0:24:08.320,0:24:12.179
また別のものは、上のオレンジから下の青に向かう勾配を見つけます。

0:24:12.909,0:24:18.389
そして、また別のものは緑色のものに反応します。

0:24:19.030,0:24:22.530
それぞれは、

0:24:24.549,0:24:26.939
フィルターと呼ばれますが、特徴を抽出します。

0:24:28.659,0:24:32.009
彼らが行ったことでもっとも興味深いことは、彼らが9つそれぞれを良く見たことです。

0:24:32.799,0:24:38.999
私たちはこれらの実態を数学的に学ぶ予定です。

0:24:39.730,0:24:41.650
ここでは、

0:24:41.650,0:24:46.169
このことを認識して、これは斜め線が見ている、や、これは勾配を見ているということを確認し、

0:24:46.690,0:24:50.280
ImageNetの画像でもこれらがあることを知っておいてください。

0:24:52.390,0:24:59.339
左上のフィルターにはそのフィルターにマッチする実際の写真の９つのパッチがあります。

0:25:00.130,0:25:04.559


0:25:04.559,0:25:11.399
そして、ここには4つの緑のフィルターがあり、緑のフィルターにマッチした写真の一部があります。

0:25:11.950,0:25:18.210
第一層は単純です。ここで注目すべきは、

0:25:18.730,0:25:24.030
グラデーションや色や線のパッチを認識できるものは、imagenetだけでなく

0:25:24.030,0:25:28.169
他の多くのタスクにも使える可能性があるということです。

0:25:29.020,0:25:31.020
こういうことができると

0:25:31.419,0:25:34.079
ImageNet以外のタスクもこなせる可能性があるのです。

0:25:35.380,0:25:37.380
これが第二層で、

0:25:37.390,0:25:41.579
第一層で得た特徴を受け取り、組み合わせて

0:25:42.429,0:25:48.029
エッジだけでなく、角や

0:25:49.299,0:25:57.089
カーブの繰り返す、半円や円を見つけます。例えば、ここには

0:25:58.990,0:26:02.760
第二層以降を正確に可視化するのは

0:26:04.419,0:26:09.359
難しいと思います。

0:26:10.150,0:26:17.009
しかし、第二層のフィルターが写真のどんな部分に発火しているかの例があります。

0:26:17.840,0:26:19.840


0:26:20.030,0:26:22.869
そして、ご覧の通り、円のようなものを見つけていて、

0:26:23.390,0:26:30.490
興味深いことにこのぼやけたようなグラデーションは夕日を見つけるのに非常に適しているようです。

0:26:30.950,0:26:35.649
そして、この繰り返しの縦のパターンは、カーテンや小麦畑などを見つけるのに非常に適しています。

0:26:36.380,0:26:38.330


0:26:38.330,0:26:44.139
このようにして、レイヤー3をさらに進めると、レイヤー2にあるすべての種類の特徴を組み合わせることができるようになります。

0:26:44.540,0:26:50.920
ここでは12種類の特徴しか見ていませんが、実際には数百個の特徴があるでしょう。

0:26:50.920,0:26:53.320
AlexNetが実際にいくつの特徴があるか覚えていませんが、AlexNetにはたくさんの特徴があります。

0:26:53.750,0:27:00.700
しかし、第二層の要素を組み合わせて第三層に到達することには、

0:27:01.700,0:27:06.189
テキストを見つけることができて、この特徴は画像のテキストを見つけられます。

0:27:07.130,0:27:09.130
これはすでに

0:27:10.130,0:27:13.780
幾何学的な繰り返しを見つけることができます。お分かりのとおり、

0:27:15.710,0:27:18.340
特定のピクセルパターンとのマッチングではありません。

0:27:19.190,0:27:24.249
これは意味的な概念のようなもので，円、四角形、六角形の繰り返しを見つけられます。

0:27:24.920,0:27:28.450
素晴らしいですね。これが計算で、ただのテンプレートマッチング

0:27:29.240,0:27:36.700
ではりません。ニューラルネットワークは計算可能な任意の関数を解けることを覚えておいてください。確かにこういった計算ができます。

0:27:38.960,0:27:41.710
第4層は第3層のフィルタを

0:27:42.290,0:27:48.489
一度にすべて組み合わせることができます。そして第4層では、例えば犬の顔を見つけることができるものができます。

0:27:49.220,0:27:51.220


0:27:51.470,0:27:53.470
そして、このように、

0:27:54.410,0:27:59.499
各レイヤーでは、より応用的に洗練された機能が得られるようになっています。

0:27:59.630,0:28:04.360
これがニューラルネットワークを強力たらしめている要因です。

0:28:05.150,0:28:11.200
それはまた、転移学習が非常にうまく機能する理由でもあります。なぜなら、

0:28:11.990,0:28:15.550
もし本を見つけたいとしても、本はImageNetにはないと思いますが、

0:28:15.680,0:28:21.549
学習済みモデルはすでに本を見つけるのに必要なフィルターを獲得しているからです。

0:28:21.560,0:28:24.700
もしかしたら図書館や本棚のカテゴリーはあるかもしれません。

0:28:26.390,0:28:32.660
ですから、転移学習では事前に学習した

0:28:33.150,0:28:38.089
特徴や既存の特徴の組み合わせを使うことができるのです。

0:28:38.310,0:28:45.440
これが、従来のアプローチに比べて、転移学習が高速かつ少データで機能する理由です。

0:28:47.250,0:28:53.119
ここで重要なことは、コンピュータビジョンのためのこれらの技術は写真を認識するのが得意なだけではないということです。

0:28:53.700,0:28:55.170


0:28:55.170,0:29:01.430
画像で表現できるものは多くあります。例えば、これらは

0:29:01.980,0:29:08.329
時間の経過とともに周波数を表現して画像化した音です。

0:29:09.120,0:29:13.880
そして、音をこのような画像に変換すれば、

0:29:14.580,0:29:18.380
音検知の最高精度を達成できるのです。

0:29:19.260,0:29:22.489
単に既に見てきたのと同様ResNetと

0:29:23.160,0:29:25.160
learnerを使うことでできるのです。

0:29:25.860,0:29:29.449
9:45ですが、休憩しますか？

0:29:30.720,0:29:32.720
転移学習の非常に良い例は

0:29:33.000,0:29:37.640
、これは1年目のFastAIの生徒がやったことだと思いますが、

0:29:38.880,0:29:45.499
Splunkでの仕事ですが、ユーザのマウスの動きを写真にして、詐欺対策を行いました。

0:29:45.500,0:29:48.530
私の記憶が正しければ、彼らはマウスの動くを画像にして、

0:29:48.740,0:29:55.310
その動きの速さに応じて色分けしました。

0:29:55.830,0:29:59.630
丸は左もしくは右クリックを表します。

0:30:00.570,0:30:05.389
それから、彼が実際にコースのプロジェクトとして行ったことは、

0:30:06.210,0:30:12.560
作成した画像に対して転移学習が使えるかどうかを確認しました。

0:30:12.560,0:30:17.839
レッスン1で詐欺行為防止モデルを作り、それが動くことを見ましたが、

0:30:17.840,0:30:24.620
Splunkは転移学習を活用したプロダクトで特許を取得しています。

0:30:24.620,0:30:30.859
そして、このことについてのブログが公開されており、このアプローチが

0:30:32.190,0:30:37.219
私たちの優秀で創造的な受講生がレッスン1の後に思いついたものに基づいていることも書いてあります。

0:30:39.180,0:30:43.580
もう一つの素晴らしい例は、様々な

0:30:44.880,0:30:46.380
ウイルスを調べて、

0:30:46.380,0:30:51.109
同様に画像に変換したものです。これは論文からの引用ですが、

0:30:51.630,0:30:53.630
本を参照してください。

0:30:54.240,0:30:58.729
彼らは、VB80と呼ばれるウイルスのサンプルを3つと、

0:30:59.070,0:31:05.179
fake rienと呼ばれるウイルスの例をもってきて、画像に変換しました。

0:31:05.850,0:31:06.800
これによって

0:31:06.800,0:31:11.900
彼らは、プログラムシグナルのようなものを画像に変換し、画像認識を行うことで、

0:31:12.300,0:31:16.610
ウイルス検知の最高精度を達成しました。

0:31:20.040,0:31:25.820
本には、これまでに紹介してきた最も重要な単語のリスト

0:31:25.820,0:31:29.270
がその意味と一緒に掲載されています。ここでは読み上げませんが、

0:31:29.270,0:31:35.270
みなさんはチェックしてください、というのもここに載っている単語は今後使いますし、

0:31:35.520,0:31:37.520
意味も含めて把握してください。

0:31:37.710,0:31:43.789
というのも、これから説明するラベル、アーキテクチャ、モデル、パラメータ

0:31:44.130,0:31:50.359
には特定の意味があり、その意味で使われるからです。

0:31:51.990,0:31:58.909
確認ですが、ここまでがアーサー・サミュエルのアプローチに関して、学んだことで、

0:31:59.970,0:32:05.299
彼の用語を私たちの用語に置き換えると、アーキテクチャというものがあり、

0:32:05.970,0:32:12.380
それは、パラメータとデータを入力として受け取ります。

0:32:13.200,0:32:16.669
つまり、アーキテクチャはモデルのパラメータと、

0:32:17.850,0:32:20.480
データを受け取って予測を計算します。

0:32:20.820,0:32:26.990
予測は損失関数によってラベルと比較され、損失関数はパラメータの更新に使われます。

0:32:27.600,0:32:32.449
この処理は損失が小さくなまで何回も繰り返されます。

0:32:34.890,0:32:37.279
ここまでがfast bookの第一章です。

0:32:38.010,0:32:44.059
そして、クイズに答えてみることを強くお勧めします。なぜなら、クイズは、みなさんが

0:32:45.640,0:32:52.020
その章から私たちが理解してほしいことをちゃんと学んだかを確認するためにあり、

0:32:53.380,0:32:57.599
不確かなものについてはその章に説明があるので、

0:32:57.640,0:33:02.640
少し戻って、答えを見つけてください。

0:33:04.390,0:33:10.950
そして、はじめのいくつかの章に関しては発展的なリサーチの節もありますが、非常に単純です。

0:33:11.050,0:33:16.380
それらは、興味深く楽しめるものと思って書きました。ただ本を読むだけでは答えがわからず、

0:33:16.900,0:33:22.410
自分で検索したり、実験しないと答えはでないと思います。

0:33:23.260,0:33:30.600
さらに後の方の章ではこういったリサーチは重たくなってくるので、数日、場合によっては数週間かかるかもしれません。

0:33:31.210,0:33:32.980


0:33:32.980,0:33:35.939
はい、それらは、きっと理解の助けになるので必ずトライしてみてください。

0:33:36.820,0:33:38.820


0:33:41.200,0:33:45.990
もう一つ、この本を最大限活用する上でのポイントは章を読み終えるごとに、

0:33:46.150,0:33:47.550


0:33:47.550,0:33:54.419
少し時間をとって、その章の知識で自分のプロジェクトに取り組むことです。

0:33:55.090,0:33:59.699
つまり、ノートブックの内容を新しいデータで再実行するのです。

0:33:59.700,0:34:04.169
第一章でこれをするのは少々難しいかもしれませんが、

0:34:04.540,0:34:07.440
第二章以降ではできるようになると思います。

0:34:08.920,0:34:15.089
それでは、9:55まで休憩をとりましょう。

0:34:16.390,0:34:18.390


0:34:18.970,0:34:25.770
さて、再開しましょう。次に進む前に質問に答えましょう。Rachel、お願いします。

0:34:26.530,0:34:29.130
モデルは再学習した場合、

0:34:29.130,0:34:35.430
事前学習でのデータセットに対するパフォーマンスが悪化するのでしょうか？

0:34:36.340,0:34:38.340
素晴らしい質問ですね。

0:34:39.460,0:34:47.310
質問の内容は、ImageNetで訓練したモデルを犬と猫のデータセットで

0:34:47.890,0:34:54.299
fine tuningすると、犬と猫を認識するのには優れているが、

0:34:56.380,0:34:58.900
ImageNetのデータセットに対してはそれほど優れていないのではないか？ということです。

0:34:58.910,0:35:03.879
確かに、飛行機やハンマーを認識するのはそれほど上手ではないでしょうし、これは

0:35:05.810,0:35:07.810
破滅的忘却と呼ばれるものです。

0:35:07.850,0:35:12.190
考えは、事前に見たデータセットと異なるものを見れば見るほど

0:35:12.260,0:35:16.960
事前に見たものを忘れていくというものです。

0:35:18.860,0:35:22.150
もしこの破滅的忘却を防ぎたければ、つまり、

0:35:22.850,0:35:27.009
新たなタスクと以前のタスクの両方で優れているモデルを作りたければ、

0:35:27.010,0:35:30.700
fine tuningを行う際に、事前のタスクで使ったサンプルも与える必要があります。

0:35:33.800,0:35:38.890
パラメータとハイパーパラメータの違いはなんですか？

0:35:40.070,0:35:46.150
入力として犬の画像を与え、ハイパーパラメータであるバッチサイズやモデルを変えた場合、

0:35:46.280,0:35:48.370
何がパラメータですか？

0:35:49.880,0:35:52.150
パラメータは、

0:35:53.540,0:35:58.240
レッスン1で伝えたように、アーサー・サミュエルによれば、

0:35:59.870,0:36:01.870
それは、

0:36:02.060,0:36:08.769
モデルがすることやアーキテクチャがすることを変えるものです。私たちは、無限に柔軟な関数から始め、

0:36:08.770,0:36:11.860
それこそがニューラルネットワークですが、それはなんでもできるものですが、

0:36:12.860,0:36:14.600


0:36:14.600,0:36:15.740


0:36:15.740,0:36:21.009
実際にニューラルネットワークができることを調整するのがパラメータです。

0:36:21.010,0:36:27.070
パラメータは関数に渡す数値です。関数に渡す数字は2種類あります。

0:36:27.070,0:36:34.059
1つは入力データ、例えば犬の写真のピクセルを表す数字でもう1つは学習で得られた

0:36:35.720,0:36:37.720
パラメータです。

0:36:38.150,0:36:42.430
ニューラルネットワーク以外の例としては、アーサー・サミュエルが60年代に

0:36:43.070,0:36:49.330
使ったかもしれないチェッカープログラムがあるでしょう。そのパラメータは

0:36:50.600,0:36:52.600
もしあれば、

0:36:53.240,0:36:57.099
ピースをとるか、ボードの端まで進むかの確率を表していたでしょう。

0:36:57.950,0:37:01.599
どちらの選択肢をより多くとるようにするかというものです。

0:37:01.870,0:37:07.120
例えば、一方より他方が2倍、3倍重要であるというものです。

0:37:09.060,0:37:10.410


0:37:10.410,0:37:12.000
ニューラルネットワークにおいては

0:37:12.000,0:37:19.069
パラメータはより抽象的なもので、詳しく理解するのは次回か次次回のレッスンになるでしょう。

0:37:19.590,0:37:23.990
ですが、基本的なアイディアは同じで、それは、

0:37:25.020,0:37:28.640
モデルが何か認識する動作を変更する数字であるということです。

0:37:29.550,0:37:35.060
悪性腫瘍であったり、犬か猫、カラー写真か白黒写真を認識するというようなものです。

0:37:36.810,0:37:38.810
一方で、ハイパーパラメータは

0:37:39.420,0:37:41.420
次のようなものに関する選択です。

0:37:41.940,0:37:43.110


0:37:43.110,0:37:49.670
例えば、関数を調整する際、どのようなプロセスを経るかといったものです。

0:37:50.580,0:37:52.020


0:37:52.020,0:37:57.140
このコースの進度が気になります。本の内容は全てカバーするのでしょうか？

0:37:58.350,0:38:02.420
答えは全ての内容として、何を意味するかに依りますが、本当に全てをカバーするのは無理でしょう。

0:38:03.720,0:38:05.720

0:38:07.110,0:38:10.370
実際には7回のレッスンでカバーできるものをカバーします。

0:38:11.790,0:38:18.440
つまり、本全体をカバーすることはできないでしょう。本全体をカバーするには、例年だと、

0:38:19.320,0:38:23.600
追加で2もしくは3つのコースが必要でした。通常はコース2つで本全体ですね。

0:38:24.750,0:38:28.909
ですが、どうなるかはわかりません。というのもこの本は500ページもありますから。

0:38:29.190,0:38:32.569
コース2つというのは、レッスン14回ということですか？

0:38:32.570,0:38:35.870
はい、14か21回で本が終わると思います。

0:38:37.530,0:38:39.740
先週の授業でお伝えした通り、

0:38:39.840,0:38:45.350
授業と独立して読み進めるのも理解の助けにはなると思います。

0:38:45.780,0:38:47.930
そして、コミュニティ、

0:38:49.200,0:38:54.049
つまり、フォーラムに参加して、質問を投稿したり回答することも有用です。

0:38:56.580,0:39:03.620
このコースの第二部では、作成したモデルをどうプロダクションに載せるかについても話します。

0:39:05.280,0:39:09.830
ですので、深層学習の限界や能力について理解することも

0:39:11.100,0:39:13.100
必要になってきます。

0:39:13.290,0:39:16.250
例えば、どんなプロジェクトが本番環境に載せるのに適しているかといったことですね。

0:39:17.100,0:39:20.809
そして、このコースと本の中で言及すべき大事なことは、

0:39:20.940,0:39:23.669
最初の2〜3回もしくは章では、

0:39:24.310,0:39:27.600
コーダだけでなく、全ての人が対象の

0:39:28.150,0:39:33.359
内容、例えば深層学習を動かすのに必要なものも含まれています。

0:39:34.270,0:39:37.409


0:39:37.450,0:39:42.329
例えば、それらは、深層学習が現時点で得意とするものです。

0:39:43.870,0:39:46.740
ですから、本に書いてあることをまとめました。

0:39:47.560,0:39:50.250
4つの分野があります。

0:39:51.100,0:39:54.539
この分野はfastaiのapplicationモジュールにあるもので、

0:39:54.850,0:40:02.309
コンピュータビジョン、テキスト，テーブル、そして推薦システムのための協調フィルタリングです。

0:40:02.890,0:40:10.589
質問です。ImageNet以外の事前学習済みモデルで利用可能なものはありますか？

0:40:10.750,0:40:14.520
いつImageNet以外のものを使うべきかというのは良い質問です。

0:40:15.370,0:40:16.810


0:40:16.810,0:40:20.730
たくさんの事前学習済みモデルがあります。

0:40:21.370,0:40:26.280
いくつか探し方がありますが、良い探し方は、

0:40:27.370,0:40:29.819
つまり、最初に見るべきは、

0:40:31.540,0:40:33.540
model zooです。

0:40:33.580,0:40:38.430
これは多種多様なモデルがある場所で，

0:40:39.760,0:40:43.289
あなたが探しているもの、特に事前学習済みモデルなら

0:40:44.920,0:40:46.920
見つけられます。

0:40:49.030,0:40:53.609
たくさんありますが、ここには残念なことに、

0:40:55.750,0:41:03.599
それほど多様ではなく、多くはImageNetやそれに類似したもので

0:41:04.240,0:41:06.689
医用画像のものはほとんどありませんね。

0:41:08.980,0:41:14.010
ドメイン固有の事前学習済みモデルを作る機会は多くあると思いますが、

0:41:14.010,0:41:17.549
十分多くの人が転移学習に取り組んでいるようではないのですね。

0:41:20.569,0:41:27.489
さて，4つのアプリケーションがあると話しました、そして、

0:41:28.880,0:41:34.569
深層学習はこれら全てにおいて非常に優れていますが、

0:41:36.529,0:41:43.268
スプレッドシートやデータベースのテーブルにおいては深層学習が最善とは限りません。

0:41:43.549,0:41:47.528
しかし、深層学習は高濃度変数が含まれる場合に有効です。

0:41:47.599,0:41:52.509
高濃度とは、郵便番号やプロダクトIDのように非常に多くのカテゴリ

0:41:53.059,0:41:55.239
をもつ変数のことです。

0:41:56.079,0:41:59.349
深層学習はそれを除けば大変素晴らしく、例えば

0:42:02.779,0:42:07.599
自然言語処理においては、分類や翻訳で良いパフォーマンスを発揮しています。

0:42:08.059,0:42:14.169
しかし、会話は苦手ですし、このことは多くの企業をがっかりさせてきました。

0:42:14.170,0:42:16.359
以前、作ろうとしたこともありますが、

0:42:17.299,0:42:19.509
深層学習は会話において、

0:42:20.329,0:42:22.309
正確な情報を提供するのが苦手なのです。

0:42:22.309,0:42:26.319
でもそれっぽく聞こえるものを返すのは得意です。

0:42:26.390,0:42:29.680
しかし、返答の正確性を正しいことを確認する方法がありません。

0:42:33.559,0:42:37.959
また、推薦システムにおける問題点の一つは

0:42:38.960,0:42:41.019
真相学習は予測に注力していることで、

0:42:42.259,0:42:48.459
予測というのは必ずしも良い推薦をすることを意味しません。すぐにどういうことか確認します。

0:42:49.789,0:42:52.569
深層学習はまた、マルチモーダルも得意です。

0:42:53.269,0:42:55.269
マルチモーダルとは、

0:42:55.970,0:43:02.139
数種類のデータを入力とするものです。例えば、テキストとの列を含むテーブルデータと画像、

0:43:03.650,0:43:05.710
そして、協調フィルタリングのデータ、

0:43:06.349,0:43:09.699
の組み合わせからなるデータなどが深層学習が得意とするものです。

0:43:10.519,0:43:12.519
例えば、

0:43:13.309,0:43:19.569
写真のキャプション生成は深層学習が大変得意としているものですが、

0:43:19.569,0:43:21.289
正確であるというのは不得意です。

0:43:21.289,0:43:26.289
ですから、実際には3羽の鳥が映っている写真に対して2羽の鳥であると言うといったものです。

0:43:27.829,0:43:29.829
そして、

0:43:30.880,0:43:31.930
他にも、

0:43:31.930,0:43:38.550
深層学習でできる想像的なことは沢山ありますし

0:43:39.160,0:43:46.080
Application based approaches for example an approach that we developed for natural language processing called you LM fit

0:43:46.080,0:43:48.080
これらはどれもコースで説明します。

0:43:48.670,0:43:55.200
例えば、タンパク質分析も素晴らしいです。もし、様々なタンパク質を異なる単語ととらえれば、

0:43:56.200,0:44:00.030
それらは列の中である種の状態や意味をもち、

0:44:00.490,0:44:07.409
ULMFitがタンパク質分析に使えることが分かったのです。

0:44:08.350,0:44:13.649
ですので、あなたが作ろうと思ったものに深層学習が使えるかもしれませんし、

0:44:14.290,0:44:17.969
実際に試してみるのも良いでしょう。

0:44:20.800,0:44:26.370
そして、検索してみれば、似たようなことに挑戦した人々を見つけられるかもしれません。

0:44:27.040,0:44:30.060
また、見つからないからうまく行かないと言うことではありません。

0:44:31.780,0:44:38.790
協調フィルタリングの弱点に言及しましたが、それは、推薦と予測は同じではないと言うことです。

0:44:39.310,0:44:41.310
この例はアマゾンで

0:44:41.440,0:44:45.870
よく起きています。私がTerry Pratchettの本を買った後、

0:44:46.210,0:44:50.610
アマゾンは数ヶ月に渡ってTerry Pratchettの本を推薦していましたが、

0:44:50.830,0:44:58.620
それはアマゾンの予測モデルがTerry Patcherの本を買った人はまた買うと言っているからでしょう。

0:44:58.620,0:45:00.520


0:45:00.520,0:45:04.530
しかし，推薦というのは購買行動を多少左右するべきでしょう？

0:45:05.200,0:45:07.800
おそらく、ある本が好きだったら、

0:45:07.800,0:45:11.999
私がその著者が好きで、その著者の別の本が好きであることも分かっているでしょう。

0:45:12.000,0:45:15.510
ですから、そういった本は買うでしょう。

0:45:16.450,0:45:19.980
これはアマゾンの例ですがあまり賢くないですね。単に私に

0:45:20.680,0:45:25.919
推薦システムの最適化を行うのではなく協調フィルタリングの予測結果をただ見せているだけなのです。

0:45:26.500,0:45:30.510
最適化された推薦システムというのは、どちらかというと最寄りの書店の

0:45:31.120,0:45:35.399
店員のように振舞うものでしょう。あなたはTerry Pratchettが好きなんですね、

0:45:35.770,0:45:42.840
なら、コメディ、ファンタジー、SFなどから同じようなテイストで聞いたことがないかもしれない本を教えましょう。

0:45:44.200,0:45:49.139
この推薦と予測の違いは大変重要です。

0:45:50.619,0:45:55.079
ですので、モデル解釈とこのような重要な問題について話したいのです。

0:45:56.170,0:46:01.200
ケーススタディとして、今非常に重要なことを取りあげましょう。

0:46:01.530,0:46:03.689
この論文におけるモデルはどちらでしょう？

0:46:03.690,0:46:06.960
このコースで挑戦することの1つは論文の読み方を学ぶことです。

0:46:07.720,0:46:11.730
裕美　ー　ここから
So here is a paper, which you I would love for everybody to read

0:46:12.310,0:46:18.179
Called high temperature and high humidity reduce the transmission of kovat 19. Now. This is a very important issue

0:46:18.730,0:46:21.419
Because if the claim of this paper is true

0:46:21.420,0:46:23.420
then that would mean that this is going to be a

0:46:23.530,0:46:29.159
Seasonal disease and if this is a seasonal disease and it's going to have massive policy implications

0:46:30.880,0:46:36.540
So, let's try and find out how this was modeled and understand how to interpret this model

0:46:38.590,0:46:40.590
So, this is a

0:46:41.530,0:46:45.659
Key picture from the paper and what they've done here is they've

0:46:46.270,0:46:51.119
taken a hundred cities in China and they've plotted the temperature on one axis in

0:46:51.490,0:46:59.070
Celsius and are on the other axis where R is a measure of transmissibility. It says for each person that has this

0:46:59.590,0:47:00.970
disease

0:47:00.970,0:47:02.970
How many people on average will they?

0:47:03.790,0:47:09.749
infect so if R is under 1 then the disease will not spread is over R is

0:47:10.720,0:47:14.010
Higher than like 2 it's going to spread incredibly quickly

0:47:15.369,0:47:20.459
And basically R is going to you know, any high R is going to create an exponential transmission impact

0:47:21.670,0:47:25.590
And you can see in this case. They have plotted a best fit line

0:47:26.350,0:47:28.240
through here

0:47:28.240,0:47:30.750
Them and then they've made a claim that there's some particular

0:47:32.260,0:47:38.699
Relationship in terms of a formula that R is one point nine nine minus 0.02 three times temperature

0:47:39.760,0:47:41.470
so

0:47:41.470,0:47:45.780
very obvious concern I would have looking at this picture is that

0:47:47.560,0:47:53.340
This might just be random maybe there's no relationship at all, but just if you picked a hundred

0:47:54.010,0:47:58.709
Cities at random perhaps they were sometimes show this level of relationship

0:48:00.160,0:48:06.480
So one simple way to kind of see that would be to actually do it in a spreadsheet

0:48:07.780,0:48:08.859
so

0:48:08.859,0:48:16.619
Yes, here is a spreadsheet where what I did was I kind of eyeballed this data and I guessed about what is the main?

0:48:16.839,0:48:24.029
Degrees centigrade I think it's about five and what's about the standard deviation of centigrade? I think it's probably about five as well

0:48:24.819,0:48:28.979
and then I did the same thing for our I think I mean our looks like it's about

0:48:29.260,0:48:34.020
1.9 to me and it looks like the standard deviation of R is probably about 0.5

0:48:35.589,0:48:40.409
So what I then did was I just jumped over here and I created a

0:48:41.440,0:48:42.819
random

0:48:42.819,0:48:47.519
Normal value so a random value from a normal distribution from a normal distribution

0:48:47.530,0:48:53.849
So a bell curve with that particular mean and standard deviation of temperature and that particular

0:48:54.339,0:48:59.699
Mean and standard deviation of R. And so this would be an example of

0:49:00.549,0:49:02.020
a city that

0:49:02.020,0:49:07.889
Might be in this data set of a hundred cities something with nine degrees Celsius and an hour of 1.1

0:49:07.890,0:49:13.349
So that would be nine degrees Celsius and an hour of 1.1. So something about here

0:49:14.950,0:49:18.359
And so then I just copied that formula down

0:49:20.380,0:49:22.380
100 times

0:49:22.420,0:49:25.290
So here are a hundred cities

0:49:25.960,0:49:31.889
that could be in China right where this is assuming that there is no relationship between

0:49:32.260,0:49:35.010
temperature and R right there just random numbers

0:49:36.490,0:49:37.690
and

0:49:37.690,0:49:43.290
So each time I recalculate that so if I hit control equals it will just recalculate it

0:49:44.290,0:49:46.290
Right I get different numbers

0:49:46.839,0:49:49.979
Okay, because they're random and so you can see at the top here

0:49:50.740,0:49:52.740
I've then got the average of

0:49:54.579,0:49:59.339
All of the temperatures and the average of all of the hours and the average of all the temperatures

0:50:02.290,0:50:07.290
Varies and the average of all of the odds varies as well. So then I

0:50:09.040,0:50:12.119
What I did was I copied those random numbers

0:50:14.530,0:50:16.530
Over here

0:50:17.230,0:50:23.490
Let's actually do it so I'll go copy these 100 random numbers and paste them

0:50:25.960,0:50:27.760
Here

0:50:27.760,0:50:33.389
Here here here. And so now I've got

0:50:34.839,0:50:40.979
1 2 3 4 5 6 I've got 6 kind of groups of 100 cities

0:50:41.289,0:50:43.559
all right, and so let's

0:50:45.220,0:50:51.359
Stop those from randomly changing any more by just fixing them in stone there

0:50:57.789,0:51:00.329
Okay, so now that I've pasted them in I've got

0:51:01.030,0:51:08.489
6 examples of what a hundred cities might look like if there was no relationship at all between temperature and R and

0:51:09.579,0:51:13.589
I've got their mean temperature and R in each of those six examples

0:51:13.630,0:51:19.079
And what I've done is you can see here at least for the first one is I've plotted it

0:51:19.630,0:51:22.710
Right and you can see in this case. There's actually a

0:51:23.680,0:51:25.680
slight positive slope and

0:51:26.500,0:51:28.500
I've actually calculated

0:51:30.549,0:51:32.549
The slope

0:51:33.010,0:51:38.010
For each just by using the slope function in Microsoft Excel and you can see that actually

0:51:39.430,0:51:41.430
In this particular case is just random

0:51:42.460,0:51:44.490
Five times it's been

0:51:45.369,0:51:47.818
Negative and it's even more negative than their

0:51:48.460,0:51:50.460
0.023

0:51:50.559,0:51:55.109
And so you can like it's kind of matching our intuition here

0:51:55.109,0:51:58.499
which is that this the slope of the line that we have here is

0:51:59.829,0:52:07.079
Something that absolutely can often happen totally by chance. It doesn't seem to be indicating any kind of real relationship at all

0:52:07.990,0:52:11.819
if we wanted that slope to be like

0:52:12.490,0:52:18.689
More confident we would need to look at more cities. So like here I've got

0:52:21.549,0:52:27.149
3,000 randomly generated numbers and you can see here. The slope is

0:52:28.930,0:52:35.760
0.0002 right. It's almost exactly zero, which is what we'd expect right when there's actually no relationship between C and R

0:52:35.760,0:52:37.949
And in this case there isn't they're all random

0:52:38.740,0:52:44.219
Then if we look at lots and lots of randomly generated cities, then we can say oh, yeah, there's there's no slope

0:52:44.590,0:52:49.379
But when you only look at a hundred as we did here you're going to see relationships

0:52:51.010,0:52:58.139
Totally coincidentally very very often. All right, so that's something that we need to be able to measure and

0:52:58.690,0:53:01.649
So one way to measure that is we use something called a p-value

0:53:02.590,0:53:05.159
So a p-value here's our p-value works

0:53:05.470,0:53:12.600
we start out with something called a null hypothesis and the null hypothesis is basically what's what's our

0:53:13.270,0:53:16.800
starting point assumption so our starting point assumption might be oh

0:53:16.800,0:53:24.179
There's no relationship between temperature and R. And then we gather some data and have you explained what R is I have. Yes

0:53:24.390,0:53:26.390
How is the transmissibility of the virus?

0:53:28.180,0:53:35.310
So then we gather data of independent and dependent variables, so in this case the independent variable is the thing that we think

0:53:36.130,0:53:43.290
Might cause the dependent variable. So here the independent variable would be temperature the dependent variable would be R. So here we've gathered data

0:53:43.990,0:53:46.379
There's the data that was gathered in this example

0:53:46.380,0:53:51.960
And then we say what does centage of the time would we see this amount of relationship?

0:53:51.960,0:53:55.830
which is a slope of 0.02 3 by chance and

0:53:56.260,0:54:00.090
as we've seen one way to do that is by what we would call a

0:54:00.190,0:54:07.260
simulation which is by generating random numbers 100 set pairs of random numbers a bunch of times and seeing how often

0:54:07.510,0:54:09.600
You see this this relationship?

0:54:11.140,0:54:13.830
We don't actually have to do it that though, there's actually a

0:54:14.710,0:54:22.290
Simple equation we can use to jump straight to this number which is what percent of the time would we see that relationship by chance?

0:54:25.150,0:54:27.150
And

0:54:27.580,0:54:31.110
This is basically what that looks like we have the

0:54:31.780,0:54:37.439
Most likely observation which in this case would be if there is no relationship between temperature

0:54:37.590,0:54:40.410
then the most likely slope would be zero and

0:54:41.410,0:54:47.039
sometimes you get positive slopes by chance and sometimes you get

0:54:48.160,0:54:50.219
pretty small slopes and sometimes you get

0:54:51.010,0:54:54.180
large negative slopes by chance and so

0:54:54.520,0:54:55.410
the you know

0:54:55.410,0:55:00.569
the larger the number the less likely it is to happen whether it be on the positive side or the negative side and

0:55:00.910,0:55:02.969
so in our case our question was

0:55:04.390,0:55:10.529
How often are we going to get less than negative 0.02 3 so it would actually be somewhere down here

0:55:10.930,0:55:14.430
And I actually copy this from Wikipedia where they were looking for positive numbers

0:55:14.560,0:55:18.090
And so they've colored in this area above the number

0:55:18.370,0:55:25.380
So this is the p-value and so you can we don't care about the math, but there's a simple little equation you can use to

0:55:25.960,0:55:27.960
directly figure out

0:55:28.780,0:55:32.460
This number the p-value from the data

0:55:34.360,0:55:37.589
So this is kind of how nearly all

0:55:38.710,0:55:41.879
kind of medical research results tend to be shown and

0:55:42.850,0:55:47.519
Folks really focus on this idea of p-values and indeed in this particular study

0:55:48.220,0:55:51.090
As well see in a moment. They reported p-values

0:55:52.120,0:55:57.539
So probably a lot of you have seen p-values in your previous lives. They come up in a lot of different domains

0:55:59.110,0:56:01.110
Here's the thing

0:56:01.180,0:56:05.039
They are terrible you almost always shouldn't be using them

0:56:05.950,0:56:09.119
Don't just trust me trust the American Statistical Association

0:56:10.330,0:56:12.870
They point out six things about p-values

0:56:13.570,0:56:19.529
And those include p-values do not measure the probability that the hypothesis is true

0:56:20.080,0:56:23.610
Or the probability that the data were produced by random choice alone

0:56:24.040,0:56:31.110
now we know this because we just saw that if we use more data, right so if we sample

0:56:32.020,0:56:37.350
Three thousand random cities rather than 100 we get a much

0:56:38.080,0:56:43.080
Smaller value, right? So p-values don't just tell you about how big a relationship is

0:56:43.090,0:56:48.539
But they actually tell you about a combination of that and how much data did you collect? All right, so

0:56:49.060,0:56:53.039
So they don't measure the probability. The hypothesis is true. So therefore

0:56:54.430,0:56:59.610
Conclusions and policy decisions should not be based on whether a p-value passes some threshold

0:57:02.500,0:57:04.500
P-value does not measure

0:57:04.780,0:57:06.780
the importance of a result

0:57:07.150,0:57:10.349
Right because again, it could just tell you that you collected lots of data

0:57:10.810,0:57:18.269
Which doesn't tell you that the results actually of any practical import and so by itself. It does not provide a good measure of evidence

0:57:20.500,0:57:27.840
So Frank Harrell who is somebody who I read his book and it's a really important part of my learning

0:57:27.840,0:57:31.590
He's a professor of biostatistics has a number of great articles about this

0:57:33.370,0:57:34.810
He says

0:57:34.810,0:57:39.029
Null hypothesis, testing and p-values have done significant harm to science

0:57:39.700,0:57:44.520
And he wrote another piece called and the whole hypothesis significance testing never worked

0:57:45.550,0:57:47.230
so

0:57:47.230,0:57:48.730
I've shown you what?

0:57:48.730,0:57:56.070
p-values are so that you know why they don't work not so that you can use them right, but they're a super important part of

0:57:57.010,0:58:03.570
machine learning because they come up all the time in making just you know, when people saying this is how we decide whether

0:58:04.270,0:58:06.630
your drug worked or whether there is a

0:58:07.330,0:58:11.580
Epidemiological relationship or whatever and indeed

0:58:12.220,0:58:14.220
p-values appear in this paper

0:58:14.710,0:58:18.149
so in the paper, they show the results of a

0:58:19.030,0:58:21.030
multiple linear regression

0:58:21.310,0:58:24.840
And they put three stars next to any

0:58:25.480,0:58:29.070
Relationship which has a p-value of 0.01 or less?

0:58:30.880,0:58:32.880
So

0:58:34.420,0:58:39.240
There is something useful to say about a small p-value like 0.01 or less

0:58:40.150,0:58:43.890
Which is the thing that we're looking at did not probably did not happen by chance

0:58:44.440,0:58:46.390
right the biggest

0:58:46.390,0:58:52.259
statistical error people make all the time is that they see that a p-value is not less than

0:58:52.660,0:58:55.080
0.05 and then they make the erroneous

0:58:55.660,0:58:57.660
conclusion that no

0:58:58.090,0:59:00.120
relationship exists, right

0:59:01.060,0:59:05.399
Which doesn't make any sense because like let's say you only had like three data points

0:59:06.039,0:59:11.999
Then you almost certainly won't have enough data to have a p-value of less than 0.05 for any

0:59:13.299,0:59:19.619
Hypothesis, so like the way to check is to go back and say what if I picked the exact opposite null hypothesis

0:59:20.140,0:59:24.210
What if my null hypothesis was there is a relationship between temperature and R

0:59:24.789,0:59:27.059
Then do I have enough data to?

0:59:28.029,0:59:32.129
Reject that null hypothesis. All right, and if the answer is no

0:59:32.950,0:59:34.359
then

0:59:34.359,0:59:38.338
You just don't have enough data to make any conclusions at all. All right

0:59:38.890,0:59:45.569
so in this case, they do have enough data to be confident that there is a relationship between

0:59:46.329,0:59:48.329
temperature and ah

0:59:48.490,0:59:49.779
Now, that's weird

0:59:49.779,0:59:54.989
because we just looked at the graph and we did a little back of a bit of a back-of-the-envelope in Excel and we thought

0:59:55.509,0:59:58.019
This is could it could well be random so

1:00:00.039,1:00:02.039
Here's where the issue is

1:00:02.619,1:00:08.518
the graph shows what we call a univariate relationship a univariate relationship shows the relationship between

1:00:08.890,1:00:13.319
One independent variable and one dependent variable and that's what you can normally show on a graph

1:00:14.019,1:00:16.169
but in this case they did a

1:00:16.990,1:00:19.259
multivariate model in which they looked at temperature and

1:00:20.259,1:00:21.609
humidity and

1:00:21.609,1:00:23.559
GDP per capita and

1:00:23.559,1:00:25.559
population density

1:00:25.599,1:00:33.179
And when you put all of those things into the model, then you end up with statistically significant results for temperature and humidity

1:00:33.730,1:00:37.079
Why does that happen? Well, the reason that happens is because

1:00:38.950,1:00:43.169
All these variation in the blue dots is not random

1:00:43.420,1:00:47.159
There's a reason they're different right and the reasons include

1:00:48.069,1:00:51.538
denser cities are going to have higher transmission for instance and

1:00:52.150,1:00:57.180
Probably more humid who have less transmission. So when you do a

1:00:59.109,1:01:06.689
Multivariate model, it actually allows you to be more confident of your results, right?

1:01:09.309,1:01:16.169
But the p-value as noted by the American Statistical Association does not tell us whether this is a practical importance

1:01:17.109,1:01:21.119
The thing that tells us is this is practical as importance is the actual slope

1:01:21.910,1:01:23.940
That's found and so in this case

1:01:25.930,1:01:33.719
The equation they come up with is that R equals three point nine six eight minus three point

1:01:33.720,1:01:38.130
Oh point O three eight by temperature minus point zero two four by relative humidity

1:01:38.170,1:01:44.909
This is this equation is this practically important? Well, we can again do a little back of the envelope here

1:01:47.710,1:01:49.710
By just putting that into

1:01:50.800,1:01:58.680
Excel let's say there was one place. It had a temperature of ten centigrade and a humidity of forty then if this equation is

1:01:58.680,1:02:00.960
Correct, R would be about two point seven

1:02:01.840,1:02:05.340
Somewhere with the temperature of 35 centigrade and a humidity of eighty

1:02:06.070,1:02:08.070
Will be about point eight

1:02:08.380,1:02:12.630
So is this practically important? Oh my god. Yes, right

1:02:13.360,1:02:16.259
two different cities with different climates

1:02:16.960,1:02:20.369
Can be if they're the same in every other way and this model is correct

1:02:21.010,1:02:28.290
Then one city would have no spread of disease because R is less than 1 1 would have massive exponential explosion

1:02:29.650,1:02:31.000
so

1:02:31.000,1:02:38.400
We can see from this model that if the modeling is correct, then this is a highly practically significant result

1:02:38.400,1:02:43.799
So this is how you determine practical significance of your models is not with p-values

1:02:44.380,1:02:47.579
But with looking at kind of actual outcomes

1:02:48.910,1:02:51.180
So, how do you think about?

1:02:52.360,1:02:54.539
the practical importance

1:02:55.150,1:03:00.420
Of a model and how do you turn a predictive model into something useful in production?

1:03:01.000,1:03:05.880
so I spent many many years thinking about this and I actually

1:03:06.490,1:03:08.230
created a

1:03:08.230,1:03:11.190
With some other great folks actually created a paper about it

1:03:13.330,1:03:15.330
Designing great data products

1:03:17.320,1:03:19.180
And

1:03:19.180,1:03:21.180
This is largely based on

1:03:22.270,1:03:24.840
Ten years of work I did at a company

1:03:24.840,1:03:28.920
I founded called optimal decisions group and optimal decisions group was

1:03:29.230,1:03:33.849
focused on a question of helping insurance companies what prices to set

1:03:34.520,1:03:35.960
and

1:03:35.960,1:03:41.919
Insurance companies up until that point had focused on predictive modeling actuaries in particular

1:03:42.619,1:03:44.619
Spent their time trying to figure out

1:03:46.190,1:03:50.649
How likely is it that you're going to crash your car and if you do how much damage might you have?

1:03:51.440,1:03:55.779
And then based on that try to figure out what price they should set for your policy

1:03:56.809,1:04:01.449
So for this company what we did was we decided to use a different approach

1:04:01.700,1:04:05.950
Which I ended up calling the drivetrain approach which is described here

1:04:06.829,1:04:08.359
to

1:04:08.359,1:04:13.749
Just set insurance prices and indeed to do all kinds of other things and so for the Insurance example

1:04:14.720,1:04:17.709
the objective would be if an insurance company would be

1:04:18.559,1:04:20.559
how do I maximize my

1:04:21.049,1:04:23.049
let's say five-year profit and

1:04:23.539,1:04:28.419
Then what inputs can we control? Can we control which what I call levers?

1:04:29.029,1:04:32.229
so in this case, it would be what price can I set and

1:04:32.779,1:04:39.339
Then data is data, which can tell you as you change your levers. How does that change your objective?

1:04:39.470,1:04:46.299
So if I start increasing my price to people who are likely to crash their car, then will get less of them

1:04:46.299,1:04:52.059
Which means we have less costs, but at the same time we'll also have less revenue coming in. For example

1:04:52.609,1:05:00.369
So to link up there kind of the levers to the objective via the data, we collect we build models that described how the levers

1:05:01.160,1:05:03.160
influence objective and this is all like

1:05:03.740,1:05:08.469
It seems pretty obvious. What do you say it like this, but when we started work with

1:05:09.200,1:05:16.839
Optimal decisions in 1999. Nobody was doing this an insurance everybody in insurance was simply doing a predictive model

1:05:18.020,1:05:23.709
To guess how likely people were to crash their car and then pricing was set by like adding

1:05:24.500,1:05:28.149
20% or whatever. It was just done in a very kind of naive way

1:05:29.329,1:05:31.250
so

1:05:31.250,1:05:32.559
what I did is I a

1:05:32.559,1:05:39.339
You know over many years took this basic process and tried to help lots of companies cigarette how to use it to turn

1:05:40.010,1:05:42.010
predictive models into actions

1:05:43.160,1:05:45.160
so the starting point

1:05:45.170,1:05:51.300
in like actually getting value in a particular model is thinking about what is it you're trying to do and you know

1:05:51.300,1:05:53.519
What are the sources of value in that thing you're trying to do?

1:05:54.190,1:06:00.810
The levers what are the things you can change? Like? What's the point of a predictive model if you can't do anything about it, right?

1:06:02.170,1:06:06.270
Figuring out ways to find what data you you don't have which ones suitable what's available?

1:06:06.460,1:06:09.480
then thinking about what approaches to analytics you can then take

1:06:10.180,1:06:12.180
and then super important like

1:06:12.730,1:06:13.750
well

1:06:13.750,1:06:18.869
Can you actually implement, you know those changes and super super important?

1:06:18.910,1:06:22.500
how do you actually change things as the environment changes and

1:06:22.750,1:06:29.489
You know interestingly a lot of these things areas where there's not very much academic research. There's a little bit and

1:06:31.359,1:06:37.289
Some of the papers that have been particularly around maintenance of like how do you decide when your machine learning model is kind of?

1:06:38.109,1:06:43.919
Still okay. How do you update it over time? I've had like many many many many citations

1:06:44.650,1:06:49.889
But they don't pop up very often because a lot of folks are so focused on the math, you know

1:06:50.859,1:06:55.169
And then there's the whole question of like what constraints are in place across this whole thing

1:06:55.270,1:07:01.259
So what you'll find in the book is there is a whole appendix which actually goes through every one of these

1:07:01.599,1:07:05.068
six things and has a whole list of

1:07:05.770,1:07:09.540
Examples written so this is an example of how to like think about

1:07:11.050,1:07:18.149
Value and lots of questions that companies and organizations can use to try and think about

1:07:19.960,1:07:22.949
You know all of these different pieces of the actual

1:07:24.190,1:07:30.569
Puzzle of getting stuff into production and actually into an effective product. We have a question. Sure just a moment

1:07:30.569,1:07:36.959
so as I say so do check out this appendix because it actually originally appeared as a blog post and I think except for my

1:07:37.540,1:07:40.409
covered 19 posts that I did with Rachel

1:07:40.410,1:07:46.260
it's actually the most popular blog post I've ever written it said hundreds of thousands of views and it kind of represents like

1:07:47.319,1:07:49.180
20 years of hard one

1:07:49.180,1:07:53.700
um insights about like how you actually get value from

1:07:54.190,1:07:59.099
Machine learning and practice and what you actually have to ask. So, please check it out because hopefully you'll find it helpful

1:07:59.930,1:08:01.930
so when we think about like think

1:08:02.870,1:08:10.420
About this for the question of how should people think about the relationship between seasonality and transmissibility of Kovan 19

1:08:13.130,1:08:16.840
You kind of need to dig really deeply into the questions about like oh

1:08:18.200,1:08:22.510
Not just what what's that? What are those numbers in the data? But what does it really look like, right?

1:08:22.510,1:08:27.369
so one of the things in the paper that they show is actual maps right of

1:08:28.040,1:08:29.840
temperature and

1:08:29.840,1:08:32.829
humidity and ah, right and

1:08:33.650,1:08:37.659
you can see like not surprisingly that humidity and

1:08:38.330,1:08:40.330
temperature in China

1:08:40.520,1:08:46.419
What we would call um order correlated which is to say that places that are close to each other in this case

1:08:46.730,1:08:48.939
geographically have similar temperatures and

1:08:49.490,1:08:51.490
similar humidities and

1:08:51.530,1:08:53.270
so like

1:08:53.270,1:08:57.520
This actually puts into the question the a lot the p values

1:08:58.160,1:09:03.519
That they have right because you you can't really think of these as a hundred totally separate cities

1:09:03.920,1:09:07.420
Because the ones that are close to each other probably have very close behavior

1:09:07.420,1:09:10.180
so maybe you should think of them as like a small number of

1:09:10.940,1:09:14.200
Sets of cities, you know of kind of larger geographies

1:09:15.020,1:09:16.370
so

1:09:16.370,1:09:21.700
these are the kinds of things that when you look actually into a model you need to like think about what are the

1:09:22.310,1:09:27.700
What are the limitations but then to decide like well, what does that mean? What do I what do I do about that?

1:09:29.720,1:09:36.340
You you need to think of it from this kind of utility point of view this kind of end to end

1:09:37.070,1:09:40.299
what are the actions I can take order the results point of view not just

1:09:41.210,1:09:44.500
Null hypothesis testing. So in this case, for example

1:09:46.520,1:09:48.760
There are basically four possible

1:09:49.430,1:09:55.570
Key ways. This could end up it could end up that there really is a relationship between

1:09:56.630,1:09:58.630
temperature and R or

1:09:59.720,1:10:05.050
So that's but the right hand side is or there is no real relationship between temperature and R and

1:10:06.350,1:10:09.610
We might act on the assumption that there is a relationship

1:10:10.100,1:10:15.220
Or we might act on the assumption that there isn't a relationship and so you kind of want to look at each of these four

1:10:16.040,1:10:18.040
possibilities and say like well, what would be the

1:10:19.250,1:10:21.250
economic and societal consequences

1:10:22.070,1:10:25.329
And you know, there's gonna be a huge difference in

1:10:26.000,1:10:30.220
Lives lost and you know economy is crashing and whatever else - you know

1:10:30.980,1:10:32.980
for each of these for

1:10:34.760,1:10:40.420
The the paper actually, you know has shown if their model is correct

1:10:40.550,1:10:48.219
What's the likely our value in March feel like every city in the world and the likely our value in July?

1:10:48.950,1:10:53.800
for every city in the world, and so for example, if you look at kind of New England and New York

1:10:54.530,1:11:00.009
The prediction here is and also West cut the other the very coast of the west coast. Is that in July

1:11:01.640,1:11:03.640
The disease will stop spreading

1:11:03.800,1:11:07.090
Now, you know in a that happens if they're right then

1:11:07.880,1:11:13.089
That's gonna be a disaster because I think it's very likely in America and also the UK

1:11:13.790,1:11:15.790
That people will say Oh

1:11:15.980,1:11:21.669
Turns out this disease is not a problem. You know, it didn't really take off at all. The scientists were wrong

1:11:22.160,1:11:25.240
people will go back to their previous day-to-day life and

1:11:25.820,1:11:32.679
We could see what happened in 1918 flu virus of like the second go around when winter hits

1:11:33.740,1:11:35.740
could be much worse than

1:11:36.200,1:11:41.260
Than the start right? So like there's these kind of like huge potential

1:11:42.320,1:11:48.489
policy impacts depending on whether this is true or false and so - think about it -

1:11:49.280,1:11:55.179
Yes. I also just wanted to say that it would be it would be very irresponsible to think. Oh

1:11:55.730,1:11:57.999
Summers gonna solve it. We don't need to act now

1:11:58.760,1:12:05.800
Just in that this is something growing exponentially and could do a huge huge amount of damage. Yeah. Yes. Okay, it already has done

1:12:06.650,1:12:07.450
by the way

1:12:07.450,1:12:11.530
if you assume that there will be seasonality and that

1:12:11.990,1:12:17.949
Summer will fix things then it couldn't beat you to be apathetic. Now if you assume there's no seasonality and

1:12:18.650,1:12:23.949
then there is then you could end up kind of creating a larger level of

1:12:25.210,1:12:30.449
Expectation of destruction that actually happens and end up with your population being even more apathetic, you know

1:12:30.449,1:12:34.559
So that they're you know being wrong in any direction to be a problem. So

1:12:35.350,1:12:40.409
One of the ways we tend to deal with this with with this kind of modeling is we try to think about priors

1:12:41.230,1:12:45.750
So our priors are basically things where we you know, rather than just having a null hypothesis

1:12:46.150,1:12:52.230
We try and start with a guess as to like, well, what's what's more likely right? So in this case?

1:12:54.070,1:13:00.420
If memory serves correctly, I think we know that like flu viruses become inactive at 27 centigrade

1:13:00.730,1:13:03.209
We know that like cold the cold

1:13:04.120,1:13:09.509
Coronaviruses are seasonal the 1918 the 1918 flu

1:13:10.449,1:13:12.250
epidemic was seasonal

1:13:12.250,1:13:17.909
In every country and city that's been studied so far. There's been quite a few studies like this

1:13:17.909,1:13:21.059
They've always found climate relationships so far

1:13:21.159,1:13:25.469
so maybe we'd say well prior belief is that this thing is probably

1:13:25.780,1:13:29.190
Seasonal and so they would say well this particular paper adds

1:13:30.010,1:13:32.010
Some evidence to that

1:13:32.230,1:13:34.230
so like it shows like

1:13:34.300,1:13:38.250
how incredibly complex it is to use a

1:13:39.340,1:13:46.560
Model in practice for in this case policy discussions but also for like organizational decisions

1:13:47.230,1:13:49.230
because you know, there's always

1:13:49.719,1:13:53.759
Complexities there's always uncertainties. And so you actually have to think about the

1:13:54.429,1:13:59.729
The utility, you know and your best guesses and try to combine everything together as best as you can

1:14:00.400,1:14:02.400
okay, so

1:14:03.250,1:14:05.250
With all that said

1:14:07.659,1:14:08.890
It's still

1:14:08.890,1:14:10.890
nice to be able to get our

1:14:11.590,1:14:16.350
our models up and running even if you know even just a predictive model is

1:14:17.170,1:14:22.589
sometimes useful of its own sometimes it's useful to prototype something and sometimes it's just

1:14:23.230,1:14:28.919
It's got to be part of some bigger picture. So rather than try to create some huge into end model here

1:14:28.920,1:14:30.920
We thought we would just show you how to

1:14:31.780,1:14:33.780
get your

1:14:35.020,1:14:37.020
Your PI torch fast AI model

1:14:37.570,1:14:39.000
up and running

1:14:39.000,1:14:45.180
In his raw a form as possible so that from there you can kind of build on top of it as you like

1:14:45.610,1:14:47.610
So to do that

1:14:47.710,1:14:49.510
we are going to

1:14:49.510,1:14:53.699
Download and curate our own data set and you're going to do the same thing

1:14:54.340,1:14:58.139
you've got to train your own model on that data set and then you're going to

1:14:58.960,1:15:02.340
Create an application and then you're going to host it know

1:15:03.100,1:15:05.020
now

1:15:05.020,1:15:10.169
There's lots of ways to create curate an image data set you might have some photos on your own computer

1:15:11.110,1:15:13.110
There might be stuff at work you can use

1:15:15.160,1:15:20.789
One of the easiest though is just to download stuff of the internet there's lots of services for downloading stuff off the internet

1:15:21.640,1:15:26.459
We're going to be using Bing image search here because they're super easy to use

1:15:27.010,1:15:33.119
A lot of the other kind of easy to use things require breaking the Terms of Service of websites

1:15:33.610,1:15:36.869
so like we're not going to show you how to do that, but

1:15:37.420,1:15:40.410
There's lots of examples that do show you how to do that

1:15:40.870,1:15:43.140
So you can check them out as well if you if you want to

1:15:43.690,1:15:47.850
Being image search is actually pretty great at least at the moment these things change a lot

1:15:47.850,1:15:50.430
so keep an eye on our website to see

1:15:51.460,1:15:53.460
If we've changed our recommendation

1:15:53.710,1:16:00.000
The biggest problem with being image search is that the signup process is a nightmare?

1:16:00.730,1:16:06.390
At least at the moment. I feel like one of the hardest parts of this book is just signing up to their damn API

1:16:07.240,1:16:11.610
Which requires going through a sure it's called cognitive services as your cognitive services

1:16:11.710,1:16:17.220
So we'll make sure that all that information is on the website for you to follow through just how to sign up

1:16:17.830,1:16:19.830
So we're going to start from the assumption

1:16:20.410,1:16:22.410
That you've already signed up

1:16:25.060,1:16:27.060
But you can find it just go being

1:16:28.510,1:16:36.090
Being Image Search API and at the moment they give you seven days with a pretty high

1:16:38.140,1:16:41.010
Pretty high quota for free and then after that

1:16:42.430,1:16:44.430
You can keep using it

1:16:44.650,1:16:46.650
as long as you like

1:16:46.750,1:16:53.709
But they kind of limit it to like three transactions per second or something, which is plenty you can still do thousands for free

1:16:53.710,1:16:56.530
So it's at the moment. It's pretty great. Even for free

1:16:58.460,1:16:59.480
So

1:16:59.480,1:17:01.250
What will happen is?

1:17:01.250,1:17:06.220
When you sign up for being image search or any of these kind of services, they'll give you an API key

1:17:06.290,1:17:11.649
So just replace the xxx here with the API key that they give you

1:17:11.750,1:17:14.740
Okay, so that's now going to be called key

1:17:16.070,1:17:18.070
In fact, let's do it over here

1:17:20.060,1:17:24.399
Okay, so you'll put in your key and then there's a

1:17:25.400,1:17:29.530
function we've created called search images Bing, which is just a super tiny little

1:17:31.520,1:17:35.200
Function as you can see, it's just two lines of code. Just trying to save a little bit of time

1:17:37.340,1:17:39.340
Which will take some

1:17:39.470,1:17:44.919
Take your API key and some search term and return a list of URLs that match that search term

1:17:46.160,1:17:48.160
as you can see for

1:17:48.830,1:17:50.510
using

1:17:50.510,1:17:54.609
This particular service. You have to install a particular

1:17:55.730,1:17:59.530
Package, so we show you how to do that on the site as well

1:18:00.380,1:18:02.620
so once you've done so you'll be able to run this and

1:18:03.740,1:18:06.909
That will return by default. I think 150

1:18:07.610,1:18:09.050
URLs

1:18:09.050,1:18:12.969
Okay, so faster Y comes with a download URL function

1:18:12.970,1:18:17.439
So he let's just download one of those images just to check and open it up

1:18:17.440,1:18:22.240
And so what I did was I searched for grizzly bear and here I have a grizzly bear

1:18:24.110,1:18:26.230
So then what I did was I said, okay

1:18:26.230,1:18:32.589
Let's try and create a create a model that can recognize grizzly bears versus black bears versus teddy bears

1:18:33.590,1:18:36.520
So that way I can find out I could set up some video

1:18:38.270,1:18:41.919
Recognition system near our campsite when we're out camping that

1:18:42.530,1:18:43.960
Gives me bear warnings

1:18:43.960,1:18:48.520
But if it's a teddy bear coming then it doesn't warn me and wake me up because that would not be scary at all

1:18:49.460,1:18:52.450
So then I just go through each of those three bear types

1:18:53.030,1:18:54.890
Create a directory

1:18:54.890,1:19:00.669
With the name of grizzly or black or teddy bear searched being for that particular search term

1:19:03.110,1:19:05.000
Along with bear

1:19:05.000,1:19:09.370
And download and so download images is a first day I function as well

1:19:10.430,1:19:13.360
So after that I can call get image files

1:19:13.460,1:19:18.610
Which is a faster a function that will just return recursively all of the image files

1:19:19.040,1:19:24.640
Inside this path and you can see it's given me bears black and then lots of numbers

1:19:27.260,1:19:28.790
So

1:19:28.790,1:19:33.490
one of the things you have to be careful of is that a lot of the stuff you download will turn out to be like

1:19:33.490,1:19:37.990
not images at all and will break so you can call verify images to

1:19:38.450,1:19:41.559
Check that all of these file names are actual images

1:19:42.800,1:19:50.559
And in this case, I didn't have any failed. So there's it's empty. But if you did have some then you would call

1:19:51.920,1:19:58.149
path unlink unlink path to unlink is part of the Python standard library and it deletes a file and

1:19:59.240,1:20:02.169
Map is something that will call this function

1:20:02.990,1:20:06.189
for every element of this collection

1:20:06.830,1:20:08.830
this is part of a

1:20:08.870,1:20:10.730
special

1:20:10.730,1:20:13.450
FASTA a class called L. It's basically

1:20:14.180,1:20:16.570
It's kind of a mix between the Python standard library

1:20:17.330,1:20:23.169
List class and a numpy array class then we'll be learning more about it later in this course

1:20:23.600,1:20:29.769
But it basically tries to make it super easy to do kind of more functional style programming in Python

1:20:30.830,1:20:34.809
So in this case, it's going to unlink every thing that's in the failed

1:20:35.420,1:20:37.420
List, which is probably what we want

1:20:37.880,1:20:42.550
Now because there are all the images that fail to verify. Alright, so we've now got

1:20:43.550,1:20:49.570
a path that contains a whole bunch of images and they're classified according to

1:20:50.510,1:20:54.070
black grizzly or Teddy based on what folder they're in and

1:20:54.680,1:21:01.840
So to create so we're going to create a model and so to create a model. The first thing we need to do is to

1:21:02.360,1:21:04.280
Tell fast AI

1:21:04.280,1:21:07.419
What kind of data we have and how its structured?

1:21:08.690,1:21:15.550
Now in part in lesson 1 of the course, we did that by using what we call a factory method

1:21:15.620,1:21:22.359
Which is we just said image data loader start from name and it did it all for us

1:21:23.869,1:21:26.289
Those factory methods are fine for

1:21:27.020,1:21:30.159
Beginners, but now we're into lesson two. We're not quite beginners anymore

1:21:30.170,1:21:35.799
So we're going to show you the super super flexible way to use data in whatever format you like

1:21:35.960,1:21:40.119
And it's called the data block API. And so the data block API

1:21:45.050,1:21:47.800
Looks like this here's the data block API

1:21:49.340,1:21:55.600
You tell first AI what your independent variable is and what your dependent variable is

1:21:55.600,1:22:02.019
So what your labels are and what your input data is. So in this case our input data are images

1:22:03.170,1:22:08.619
And our labels are categories. So category is going to be either grizzly or black

1:22:09.260,1:22:11.179
or teddy

1:22:11.179,1:22:13.239
So that's the first thing you tell it now

1:22:13.239,1:22:19.178
That's the block's parameter and then you tell it how do you get a list of all of the in this case file names?

1:22:19.520,1:22:24.759
Right, and we just saw how to do that because Peters called the function ourselves if the function is called get image files

1:22:24.889,1:22:28.149
so we tell it what function to use to get that list of items and

1:22:29.090,1:22:34.989
Then you tell it how do you split the data into a validation set and a training set?

1:22:34.989,1:22:39.968
And so we're going to use something called a random spitter which just spits it randomly and we're going to point

1:22:40.130,1:22:42.130
30% of it into the validation set

1:22:42.469,1:22:48.669
we're also going to set the random seed which ensures that every time we run this the validation set will be the same and

1:22:49.760,1:22:55.389
Then you say okay. How do you label the data? And this is the name of a function called parent label?

1:22:55.389,1:22:57.789
And so that's going to look for each

1:22:59.300,1:23:05.560
Item at the name of the parent so this this particular one would become a black bear now

1:23:05.560,1:23:07.560
This is like the most common way for

1:23:09.469,1:23:16.178
Image datasets to be represented is that they get put the different images get the files get put into folder according to their label

1:23:17.119,1:23:18.679
and

1:23:18.679,1:23:21.579
Then finally here we've got something called item transforms

1:23:21.920,1:23:25.359
we're be learning a lot more about transforms in a moment that these are basically

1:23:25.580,1:23:33.070
functions that get applied to each image and so each image is going to be resized to 100 and twenty-eight by

1:23:33.560,1:23:35.560
128 square

1:23:35.960,1:23:38.950
So we're going to be learning more about datablock api soon

1:23:39.200,1:23:44.619
but basically the process is going to be it's going to call whatever is get items which is a list of image files and

1:23:45.650,1:23:50.320
Then I'm going to call get X get Y. So in this case, there's no get X but there is a get Y

1:23:50.320,1:23:55.659
So it's just parent label and then it's going to call the create method for each of these two things

1:23:55.660,1:23:57.789
It's going to create an image and it's going to create a category

1:23:58.520,1:24:01.990
Instead. I'm going to call the item transforms, which is resize

1:24:02.930,1:24:06.249
And then the next thing it does is it puts it into something called a data loader?

1:24:06.250,1:24:10.030
A data loader is something that grabs a few images at a time?

1:24:10.400,1:24:14.890
I think by default at 64 and puts them all into a single

1:24:14.890,1:24:19.090
It's got a batch. It just grabs 64 images and sticks them all together

1:24:19.760,1:24:23.499
And the reason it does that is it then puts them all onto the GPU at once

1:24:24.380,1:24:26.920
so it can pass them all to the

1:24:27.230,1:24:33.640
Model through the GPU in one go and that's going to let the GPU go much faster as we'll be learning about

1:24:34.730,1:24:36.879
And then finally, we don't use any here

1:24:37.190,1:24:43.960
We can have something called batch transforms, which we will talk about later and then somewhere in the mineral about here

1:24:44.480,1:24:51.250
Conceptually is the splitter which is the thing that splits into the training set and the validation set

1:24:51.410,1:24:55.209
So this is a super flexible way to tell fast AI

1:24:56.510,1:25:00.489
how to work with your data and so at the end of that

1:25:01.280,1:25:08.199
It returns an object of type data loaders. That's why we always call these things deals, right?

1:25:08.390,1:25:12.579
So data loaders has a validation and a training

1:25:12.860,1:25:14.150
data loader and

1:25:14.150,1:25:20.470
a data loader as I just mentioned is something that grabs a batch of a few items at a time and puts it on the

1:25:20.470,1:25:22.250
GPU for you

1:25:22.250,1:25:26.530
So this is basically the entire code of data loaders

1:25:27.470,1:25:31.269
So the details don't matter. I just wanted to point out that like a lot of these

1:25:31.850,1:25:34.629
Concepts in fast AI when you actually look at what they are

1:25:34.630,1:25:36.460
They're they're incredibly simple

1:25:36.460,1:25:42.100
but all things it's literally something that you just pass in a few data loaders to edit store some in an attribute and

1:25:42.410,1:25:47.359
Pass and gives you the first one backers train and second one backers valid

1:25:49.199,1:25:51.090
So

1:25:51.090,1:25:53.210
we can create our

1:25:55.230,1:26:02.359
Data loaders by first of all creating the data block and then we call the data loaders passing in our path to create deals and

1:26:02.940,1:26:05.029
Then you can call show batch on that

1:26:05.030,1:26:10.280
You can call show bachelor pretty much anything in fast AI to see your data and look we've got some Grizzlies

1:26:10.280,1:26:12.280
We've got a teddy we've got a grizzly

1:26:14.280,1:26:16.280
So you get the idea right

1:26:17.880,1:26:22.699
I'm going to look at these different. I'm going to look at data augmentation next week

1:26:22.699,1:26:27.469
So I'm going to skip over data-orientation and let's just jump straight into trading your model

1:26:31.710,1:26:35.089
So once we've got two yells we can just like in

1:26:35.760,1:26:39.199
Lesson one call CNN learner to create a

1:26:39.719,1:26:43.038
Resonate we're going to create a smaller resonant this time or isn't it 18?

1:26:43.590,1:26:49.519
Again asking for error rate we can then call dot fine-tune again. So you see it's all the same lines of code

1:26:49.519,1:26:50.999
we've already seen and

1:26:50.999,1:26:55.819
You can see our error rate goes down from nine to one. So we've got one percent error

1:26:56.519,1:27:02.719
And after training for about 25 seconds, so you can see you know, we've only got 450 images

1:27:03.690,1:27:10.789
we've trained for well less than a minute and we only have let's look at the confusion matrix so we can say I

1:27:11.400,1:27:18.559
Want to create a classification interpretation class. I want to look at the confusion matrix and the confusion matrix as you can see

1:27:18.659,1:27:21.768
It's something that says for things that are actually black bears

1:27:22.650,1:27:24.769
How many are predicted to be black bears?

1:27:25.559,1:27:26.610
versus

1:27:26.610,1:27:28.610
grizzly bears versus

1:27:28.679,1:27:33.859
Teddy bears. So the diagonal are the ones that are all correct. And so it looks like we've got two errors

1:27:33.860,1:27:38.150
We've got one grizzly that was predicted be black one black that was predictable grizzly

1:27:40.889,1:27:42.889
Super super useful

1:27:43.469,1:27:47.239
Method is plot top losses, and that'll actually show me

1:27:48.809,1:27:56.119
What my errors actually look like so this one here was predicted to be a grizzly bear but the label was black bear

1:27:56.770,1:28:00.450
This one was the one that's predictably a black bear and the label was grizzly bear

1:28:02.500,1:28:04.800
These ones here are not actually wrong there

1:28:04.830,1:28:11.160
this is predicted to be black and it's actually black but the reason they appear in this is because these are the ones that the

1:28:12.730,1:28:15.540
Model was the least confident about

1:28:16.780,1:28:19.829
Okay, so we're going to look at the image classifier cleaner next week

1:28:20.800,1:28:23.789
Let's focus on how we then get this into production

1:28:24.700,1:28:25.930
so

1:28:25.930,1:28:27.930
To get it into production

1:28:28.150,1:28:30.150
We need to

1:28:30.580,1:28:38.519
Export the model. So what exporting the model does is it creates a new file which by default is called export pickle?

1:28:39.280,1:28:41.170
Which contains?

1:28:41.170,1:28:43.920
The architecture and all of the parameters of the model

1:28:44.170,1:28:50.160
so that is now something that you can copy over to a server somewhere and

1:28:50.710,1:28:52.710
treat it as a

1:28:52.900,1:29:00.030
Predefined program right? So then so the the process of using your trained model

1:29:01.150,1:29:04.920
On new data kind of in production is called inference

1:29:05.410,1:29:10.019
So here I've created an inference learner by loading that learner back again

1:29:10.390,1:29:17.640
All right, and so obviously it doesn't make sense to do it right next to after I've saved it in a notebook

1:29:17.640,1:29:23.459
But I'm just showing you how it work. Right? So this is something that you would do on your server in inference and

1:29:24.730,1:29:31.470
Remember that once you have trained a model, you can just treat it as a program you can pass inputs to it

1:29:31.470,1:29:33.990
So this is now our our program

1:29:33.990,1:29:40.590
this is our Bayer predictor so I can now call predict on it and I can pass it an image and

1:29:41.200,1:29:43.200
It will tell me

1:29:43.210,1:29:45.210
Here is it is?

1:29:45.490,1:29:47.999
99.999% sure that this is a grizzly

1:29:49.240,1:29:53.280
So I think what we're going to do here is we're going to wrap it up here

1:29:54.160,1:29:57.180
And next week we'll finish off by

1:29:58.750,1:30:00.750
Creating an actual GUI

1:30:01.180,1:30:02.680
for our

1:30:02.680,1:30:04.510
bear classifier

1:30:04.510,1:30:06.370
We will

1:30:06.370,1:30:08.140
show how to

1:30:08.140,1:30:10.140
Run it for free on a service call

1:30:10.920,1:30:12.570
binder

1:30:12.570,1:30:14.570
and

1:30:16.530,1:30:23.150
Yeah, and then I think we'll be ready to dive into some of the some of the details of what's going on behind the scenes

1:30:23.849,1:30:27.379
Um any questions or anything else before we wrap up Rachel?

1:30:28.860,1:30:34.339
Now, okay, great. All right. Thanks everybody. So we

1:30:37.409,1:30:39.679
Hopefully, yeah, I think from here on

1:30:40.830,1:30:42.179
We've covered

1:30:42.179,1:30:49.279
You know, most of the key kind of underlying foundational stuff from a machine-learning point of view that we're going to need to cover

1:30:51.000,1:30:53.060
So we'll be able to ready to dive into

1:30:54.840,1:30:58.310
Lower-level details of how deep learning works behind the scenes

1:31:00.540,1:31:03.889
And I think that'll be starting from next week, so see you then
