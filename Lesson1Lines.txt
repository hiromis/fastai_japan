
　
























 




























　





















その方法とは、確率的勾配降下法あるいはSGDと呼ばれています。
今後，実際にSGDがどのように動作するのかを確認し、スクラッチで作成しますが、今のところは心配する必要はありません。
これだけは言っておきますが、SGD もニューラルネットも数学的には全く複雑ではありません。
ほとんど足し算と掛け算だけです。
トリックは、足し算と掛け算を、数十億，いやそれより多く、私たちが直感的に把握できるより多く行っていることです。
彼らは極めて強力ですが，ロケットのように複雑ではありませんし，それらがどのように動くのかをしっかり確認します。
ここまでがアーサー・サミュエル版です。
今日では、用語は異なりますが、全く同じ考えを使っています。
真ん中にあるのがアーキテクチャで重みを調整することで何かしらのタスクを適切に処理するようになる関数です。
それがアーキテクチャであり、モデルの関数形です。
時々、モデルはアーキテクチャを意味すると言われることがありますが、あまり混乱させないようにしてください。
しかし、本当に正しい言葉はアーキテクチャーです。
私たちはそれらを「重み」とは呼ばず、「パラメータ」と呼んでいます。
重みには特定の意味があり、非常に特殊な種類のパラメータです。
モデルから出てくるもの、つまりパラメータを持つアーキテクチャの出力を、私たちは予測と呼んでいます。
予測は2種類の入力に基づいています：データである独立変数、猫や犬の写真のようなもの、そしてラベルとして知られている従属変数、これは「これは猫です」「これは犬です」といったもので、これが入力です。
つまり、結果は予測です。
アーサー・サミュエルの言葉を借りれば、パフォーマンスの尺度は、損失と呼ばれています。
損失は予測値のラベルから 計算されます そして、パラメータの更新があります  さて、これは先ほどの図と同じですが、今日使う言葉を入れてみました。
もし忘れてしまった場合は、この図を見て、これがモデルを作成するためにこのアーキテクチャで使用されているパラメータだと言ったら、もう一度戻って、パラメータの意味を思い出してみてください。
パラメータとは何か？予測値とは？損失とは何か？ パラメータを更新できるような方法でモデルのパフォーマンスを測定する関数の損失です。
ここで注意しなければならないのは、ディープラーニングや機械学習は魔法ではないということですね。
モデルは、学習しようとしていることの例を示すデータがあるところでしか作成できません。
それは訓練に使われた入力の中で見たパターンでしか動作を学習できないんですよね？ だから、猫や犬の画像がないと、アーキテクチャを作るパラメータの更新がないので、アーキテクチャとパラメータを合わせたものがモデルなんです。
つまり、モデルと言えば、モデルが猫や犬の線画を予測するのが上手になるということです。
また、この学習アプローチは予測値を作成するだけであることにも注意してください。
それに対して何をすべきかは教えてくれません。
これは、「誰かにどの製品を勧めるか」というようなレコメンデーションシステムのようなものを考えるときに非常に重要になるでしょう。
私たちはそんなことしませんよね？ 私たちが見せた製品について誰かが何を言うか予測することはできますが、私たちは行動を生み出しているわけではありません。
予測をしているのです。
これは非常に重要な違いです。
犬や猫の絵などの入力データの例があるだけでは不十分なのです。
ラベルなしでは何もできません。
企業がよく言うのは 「データが足りない」と言うことがよくあります。
ほとんどの場合は、「ラベル付きのデータが足りない」という意味です。
ほとんどの場合、「ラベル付けされたデータが十分ではない」という意味です。
なぜなら、企業がディープラーニングを使って何かをしようとしている場合、多くの場合、すでに行っていることを自動化したり、改善したりしようとしているからです。
つまり、理論的には、そのデータを持っているか、またはそのデータを取得する方法を持っていることを意味します。
彼らはそれをやっているからだ。
しかし、しばしば厄介なのは、それにラベルを付けることです。
例えば医療における放射線学のモデルを作成するとします。
思いつく限りのあらゆる医療画像を入手できることはほぼ間違いないでしょう。
しかし、それらを腫瘍の悪性度や髄膜腫の有無などでラベル付けするのは非常に難しいかもしれません。
このような違いは、戦略に大きな影響を与える重要なものです。
モデルとは、PDPの本で見たように、モデルは環境の中で動作します。
あなたはそれを展開して、何かをするのです。
そして、PDPのフレームワークのこのような部分が非常に重要なのです。
そうですよね？実際に何かをしているモデルがあります。
例えば、どこで逮捕されるかを予測する（行動を推奨するわけではありません）警察のような予測モデルを構築したとします。
これはアメリカの多くの管轄区域で使われているものです。
今では、データに基づいて、ラベル付けされたデータに基づいて、それを予測しています。
この場合、実際には（アメリカの）データを使っています。
黒人か白人かにもよりますが、アメリカの黒人は白人に比べてマリファナ所持で逮捕される頻度が７倍くらい高いと思います。
実際のマリファナ使用量は白人と黒人でほぼ同じですが、偏ったデータを元に取り締まり予測モデルを構築すると、その予測はこのような偏ったデータに基づいて 「ここに逮捕できる人がいるだろう」と 予測します。
そうすると、法執行機関の職員は、モデルが予測した地域に警察活動を集中させることに決めるかもしれません。
その結果、より多くの人を逮捕することになる。
そして、それを使ってモデルに戻す。
するとモデルは黒人居住区ではもっと多くの人を逮捕すべきだと予測するようになります。
これは、モデルが環境と相互作用する中で、どのように正のフィードバックループと呼ばれるものが作られるかの一例です。
モデルの利用増加に伴い、データはより偏ったものになり、モデルはさらに偏ったものになります。
ですから、機械学習で注意しなければならないことの1つは、モデルが実際にどのように使用されているかを認識し、その結果として起こり得るかを認識することです。
これはプロキシの例であるということを伝えたいです。
なぜなら、ここでは逮捕者が犯罪の代わりとして使われているからです。
そして、往々にして，プロキシと実際の値の差は大きいのです。
ありがとう、レイチェル。
本当に重要なポイントですね。
それでは最後に、このコードで何が起こっているのかを見てみましょう。
私たちが実行したコードは、基本的には、1,2,3,4,5,6行のコードです。
コードの最初の行はインポート行です。
Pythonでは、外部ライブラリをインポートしない限り、外部ライブラリを使うことはできません。
通常はライブラリから必要な関数やクラスだけをインポートします。
しかし、Pythonにはモジュールからすべてをインポートできる便利な機能があります。
ほとんどの場合、これは悪い考えです。
なぜなら、デフォルトでは、Pythonの動作方法は、import starを実行した場合、ライブラリの中であなたが使いたい重要なものだけをインポートするのではないからです。
実際には、使用した全てのライブラリや、使用した全てのライブラリがimportしているものもimportするため、結果的に名前空間を爆発させてしまい、多くのバグを引き起こしてしまいます。
fastaiはREPL環境で素早くプロトタイピングができるように設計されているので、私たちはこの問題を避けるために多くの時間を費やしました。
ですから、これをするかしないかはあなた次第です。
しかし、fastaiライブラリからstarをインポートする場合は、実際に必要な部分だけを取得できるように明示的に設計されているので安心してください。
一つだけ言及しておきたいのは、ビデオの中で "fastai2"と呼ばれていることです。
収録時はプレリリース版を使っているからです。
オンライン版、MOOC版を見ている時には、2は取れているでしょう。
他にも言及しておきたいことがあります。
fastaiには4つの定義済みアプリケーション、vision、text、テーブル、協調フィルタリングがあります。
それぞれについて、詳しく学んでいく予定です。
アプリケーションはそれぞれ、例えばここにビジョンがあるとすると、.allからインポートすることができます。
これで、一般的なvisionアプリケーションに必要なほぼ全てがimportされます。
Jupyter NotebookのようなREPLシステムを使っていれば、必要なものはすべてそこにあるので、戻って調べたりする必要はありません。
Pythonユーザーがそれを行っているというのは
問題点です。
untar_dataのようなものを見たとしても、それがどこから来たのかはインポート行を見ればわかるでしょう。
しかしstart import (import *)すると、わからなくなってしまうのです。
REPLでは戻って調べる必要がないということです。
調べたい記号を入力して [SHIFT - ENTER] を押すだけで、それがどこから来たのかを正確に教えてくれます。
この通りです。
大変便利なのです。
この場合、例えば、実際にデータセットを構築するために、ImageDataLoaders.from_name_func()を呼び出しました。
doc関数を使えば簡単にドキュメントを確認できます。
ご覧のように、デフォルト値を含めて全ての引数が確認できます。
そして最も重要なことは、[SHOW IN DOCS]は、exampleを含む完全なドキュメントへのリンクになっています。
fastAIのドキュメントにはすべて例がついています。
さらに、それらは全てJupyter Notebookで書かれているので、自分でコードを実行して、実際の動作や出力など確認できます。
また、ドキュメントにはたくさんのチュートリアルがあります。
例えば、visionのチュートリアルには多くのことが書かれていますが、その1つは、レッスン１の内容をほぼカバーしています。
fastAIにはたくさんのドキュメントがあります。
ドキュメントは完全に検索可能ですし、おそらく最も重要なことは、これらのドキュメントのすべてが完全にインタラクティブなJupyter Notebookになっていることです。
そこで、このコードをさらに見てみると、importの後の最初の行は、untar_dataを呼んでいます。
これはデータセットをダウンロードして、解凍します。
すでにダウンロードされている場合は、再度ダウンロードすることはありませんし，解凍済みなら解凍しません。
ご覧のように、fastAIはすでに多くの有用なデータセットを定義しています。
データセットは深層学習で想像できるように超重要です。
これからたくさんのデータセットを見ていくでしょう。
そして、これらのデータセットは多くのヒーロー（とヒロイン）によって作成され、基本的には数ヶ月から数年かけてデータを照合し、これらのモデルを構築するために使用することができます。
次のステップは、このデータが何であるかをfastAIに伝えることであり、データについて私たちは多くのことを学んでいきます。
しかし、今回は、基本的には「これには画像が含まれている」と言っています。
このパスにある画像が含まれています。
untar_dataは解凍済みの
データのパスを返します。
あるいはすでに解凍されている場合は、以前に解凍された場所を教えてくれます。
そのパスの中に実際にどんな画像があるのかを教えてくれます。
本当に興味深いものに label_func があります。
ファイルごとに、それが猫なのか犬なのかをどうやって判別するのでしょうか。
実際にオリジナルのデータセットのReadMEを見てみると、少し風変わりなものが使われています。
しかし彼らが決めたことです。
そこで、ここにis_catという小さな関数を作って、最初の文字が大文字かどうかを返すようにしました。
これで猫かどうかを判断する方法をfastaiに伝えました。
この2つについては後で説明します。
次に、データが何であるかを伝えます  次にlearnerと呼ばれるものを作らなければなりません。
learnerとは学習するものです。
訓練をします。
だから、どんなデータを使うか教えなければなりません。
そして、どのようなアーキテクチャを使うかを教えなければなりません。
このコースではアーキテクチャについてたくさんお話します。
しかし、基本的には、事前に定義されたニューラルネットワークのアーキテクチャがたくさんあり、それぞれ長所と短所があります。
コンピュータビジョン用のアーキテクチャはResNetと呼ばれています。
これは非常に素晴らしい出発点なので、ここでは小さ目なものを使うことにします。
これらはすべて事前に定義されていて、すぐに使えます。
そして、訓練中にプリントアウトしたいものをfastaiに伝えることができます。
ここでは、「ああ、誤差を教えてください、トレーニング中に」と言っています。
そして、fine_tuneと呼ばれるとても重要なメソッドを呼び出すことができます。
次のレッスンで説明しますが、fine_tuneが訓練を行います。
valid_pctは非常に重要なことをします。
この場合、データの20%は、モデルの学習には使用しません。
その代わりに、モデルの誤差率を知るために使います。
したがって、常にfastaiでは、このメトリック、error_rateは常に訓練されていないデータの一部で計算されます。
この考え方については、今後のレッスンで詳しく説明します。
しかし、ここでの基本的な考え方は、過学習していないことを確認したいということです。
説明しましょう。
過学習は次のようなものです。
例えば、これらの点にフィットする関数を作ろうとしているとしましょう。
素敵な関数はこのようになりますよね？しかし、この関数をはより正確にフィットしています。
見てください、これはこちらよりもはるかにすべての点に近づいています。
ですから、明らかにこちらの方が優れた関数です。
ただし、点がある場所の外に出るとすぐに、特に端から外れてしまうと、明らかに意味がありません。
これが過学習と呼ばれるものです  過学習は様々な理由で起こります。
大きすぎるモデルを使ったり、十分なデータを使っていなかったり...この話はこれからですね。
しかし、ディープラーニングの技術は、適切なフィットを持つモデルを作成することがすべてです。
モデルが適切にフィットしているかどうかを知る唯一の方法は、それを訓練するために使用されていないデータでうまく機能するかどうかを見ることです。
そのため、私たちは常にデータの一部を脇に置いて、バリデーション・セットと呼ばれるものを作成します。
バリデーション・セットとは、モデルを訓練しているときには全く触らないようにしているデータですが、モデルが実際に機能しているかどうかを把握するためだけに使用しています。
シルヴァンが本の中で言っていたことですが、fastaiを勉強していて面白いことの一つは、多くの面白いプログラミングを学ぶことができるということです。
私は子供の頃からプログラミングをしていたので、かれこれ40年くらい経ちます。
シルヴァンと私は二人ともpythonで多くのことを行うために本当に一生懸命働いていて、プログラミングのプラクティスを知っています。
私たちのコードの中では、今まで見たことのないようなことをやっていることがよくあります。
以前のコースを受講した多くの学生は、このコースでコーディングやPythonコーディング、ソフトウェアエンジニアリングについて多くを学んだと言っています。
だから、何か新しいものを見たときはチェックして、なぜそのような方法で行われたのか気になったらフォーラムで気軽に質問してください。
一つ言及しておきたいのは、先ほども述べたように、ほとんどのPythonプログラマーがimport *を行うことはありません。
私たちはそのようなことをたくさんやっています。
私たちは伝統的なPythonプログラミングのアプローチに従わないことをたくさんやっています。
私は長年にわたって多くの言語を使ってきたので、特にPython的な方法ではなく、他の多くの言語や他の多くの表記法からのアイデアを取り入れて、データサイエンスに適したものをベースにPythonプログラミングへのアプローチを大幅にカスタマイズしています。
つまり、あなたがfastaiで見たコードは、あなたの職場でPythonを使っている場合、おそらく、あなたの職場のスタイルガイドや通常のアプローチには合わないでしょう。
ですから、私たちのやり方に従うのではなく、あなたの組織のプログラミングのやり方に合うようにする必要があります。
しかし、おそらくあなた自身の趣味の仕事では、私たちのものに従ってみて、それが面白くて参考になるかどうかを見てみてもいいでしょうし、もしあなたが管理職で、そうすることに興味があるのであれば、あなたの会社でそれを試してみるのもいいでしょう。
さて、最後に、かなり面白いものをお見せしましょう、それは、このコードを見てみてくださいuntar_data、ImageDataLoaders.from_name_funcから、learner、fine_tuneします。
untar_data、SegmentationDataLoaders.from_label_func、label_func、learner、fine_tune。
ほとんど同じコードですが、これは何かをするモデルを構築しました。
これは画像を撮影したものです。
左側がラベル付きのデータです。
色は、それぞれ、車、木、建物、空、線、道路などを示しています。
右側は私たちのモデルですが、私たちのモデルは各ピクセルごとに、車なのか、ラインマークなのか、道路なのかを判別することに成功しました。
たった20秒以内でこれを実現しました。
ですから、非常に小さなモデルです。
いくつかのミスをしています -- この線のマークを見落としていたり、いくつかの車が家だと思っているような？しかし、数分間これを訓練すれば、ほぼ完璧になることがわかります。
基本的な考え方は、ほぼ同じコードを使って、猫や犬を分類するのではなく、セグメンテーションと呼ばれるものを非常に速く作ることができるということです。
見てください、同じものがあります。
Import *、TextDataLoaders.from_folder，learner、fine_tune
基本的なコードは同じです。
これは、文章が表している感情が肯定的か否定的かを理解するできるようになりました。
同じ3行のコードで15分の訓練による93%という正確度は、1000から3000語の何千もの映画レビューからなるIMDBデータセットの分類においては2015年時点で世界最高のものだったと思います。
私たちは基本的に同じコードを使って、私たちのブラウザで、世界クラスのモデルを作成しています。
ここでも同じ基本的な手順です。
from import *、untar_data、TabularDataLoaders.from_csv、Learner、fit
これは今、これらの列を含むcsvに基づいて給与を予測するモデルを構築しています。
これは表形式のデータですね。
ここでは基本的な手順は同じです。
Import *、untar_data、CollabDataLoaders.from_csv、learner，fine_tune
これはユーザーと映画の組み合わせごとに、視聴履歴をもとに，ユーザが映画をどう評価するかを予測するものを構築しています。
これは協調フィルタリングと呼ばれ、レコメンデーションシステムで使用されています。
ここでは、fastaiの4つのアプリケーションの例を見てきました。
このコースを通してお分かりになると思いますが、基本的に同じコードと、同じ数学とソフトウェア工学の基本的な概念によって、同じようなアプローチを使って、大きく異なることができるのです。
なぜかというと、アーサー・サミュエルのおかげです。
モデルをパラメータ化する方法と、重みを更新して損失関数を改善する更新手順があれば、何ができるのかという基本的な説明があるからです。
この場合、ニューラルネットワークを使うことができます。
他のレッスンよりも少し短くなりますが、その理由は、先ほど述べたように、私たちは世界的なパンデミックの始まりの時期にあるからです（少なくとも欧米では（他の国ではもっと進んでいます）。
このことについては、コースの最初の方でお話しましたが、そのビデオは別の場所で見ることができます。
今後のレッスンでは、より深層学習について学ぶことになるでしょう。
次のレッスンに取り組む前に、次の週までに以下に取り組むことをお勧めします。
GPUサーバをスピンアップして、終了したらシャットダウンして、ここにあるすべてのコードを実行できることを確認して、自分が認識できる方法でPythonを使っているかどうかを確認してください。
自分のやり方を知ることができれば、快適に過ごせるようになるでしょう。
この学習スタイル、トップダウン学習で最も重要なことは、実験を行うことであり、コードを実行できるようになる必要があります。
ですから、私のお勧めは、コードを実行できるようになるまでは先に進まないで、本の章を読んで、アンケートに答えてください。
検証セットやテストセット、転送学習については、まだまだやるべきことがあります。
ですから、まだすべてのことができるわけではありませんが、これまでのコースを見てきたことに基づいて、できる部分はすべてやってみてください。
レイチェル、何か追加したいことがあれば教えてください。
それでは、レッスン１に参加してくれてありがとうございました。
次回は転移学習について学び、その後、実際にインターネット上で公開できるアプリの制作に移ります。
