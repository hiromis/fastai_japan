0:00:03.020,0:00:05.020
みなさんこんにちは

0:00:05.210,0:00:07.210
レッスン4です。

0:00:07.850,0:00:10.989
今回からようやく、実際にニューラルネットの

0:00:11.780,0:00:15.400
訓練を深掘りしていきます。

0:00:15.920,0:00:19.180
前回からこのテーマに入り、

0:00:19.180,0:00:22.569
確率的勾配降下法（SGD）を勉強しました。

0:00:22.939,0:00:27.219
復習ですが、アーサー・サミュエルのアイデア、つまり、

0:00:28.710,0:00:35.849
実際のパフォーマンスからその時点での重みの有効性を自動で評価する方法と、

0:00:35.950,0:00:39.330


0:00:39.820,0:00:44.520
パフォーマンスを最大化するための重み更新メカニズムを用意できれば、

0:00:45.550,0:00:53.400
完全自動で、自らの経験から学習するプログラムができるのです。

0:00:54.340,0:01:00.659
前回のMNISTへのアプローチは実際にはこの方法に基づいていませんでした。

0:01:01.390,0:01:04.229
パラメータがなかったからです。

0:01:04.960,0:01:07.800
そこで、最後に、

0:01:08.770,0:01:11.729
どのようにパラメタライズするか、つまり、パラメータを持つ関数をどう作るか考えました。

0:01:12.880,0:01:19.049
そこで考えたのは、ある数字の画像である確率というものは、

0:01:19.869,0:01:27.389
その画像のピクセルと重みで表現できて、数式としては、それらを要素毎に掛けてから

0:01:28.090,0:01:30.090
和をとるというものです。

0:01:31.600,0:01:33.600
そしてさらに、

0:01:34.270,0:01:41.759
確率的勾配降下法が動く様子も見ました。基本的な考えとしては、ランダムに初期化した

0:01:43.210,0:01:45.960
パラメータから始め、まずそれらを使って

0:01:46.690,0:01:51.840
何かしらの関数に従って予測を計算し、

0:01:54.000,0:01:58.919
その予測を損失関数を使って評価します。そのあと、

0:01:59.830,0:02:07.080
あるパラメータの微小変化による損失の変化である勾配を計算します。

0:02:08.020,0:02:13.619
最後に、その勾配を使ってパラメータを更新するのですが、更新する時は

0:02:14.770,0:02:16.799
パラメータから勾配に学習率をかけた値を引きます。

0:02:17.350,0:02:21.299
そしてその更新されたパラメータで予測を計算し、、、という流れを繰り返します。

0:02:22.450,0:02:28.530
これが基本の7ステップで、

0:02:29.920,0:02:35.220
前回は簡単な二次関数にこれを使いました。

0:02:38.430,0:02:44.010
パラメータの推移はこんな感じで、最終的にはこのようにどんどん近づいていき

0:02:47.610,0:02:49.830
良い曲線になりました。

0:02:55.290,0:02:57.180


0:02:57.180,0:03:03.109
“Summarizing gradient descent”は簡潔なまとめで、各章で

0:03:03.840,0:03:07.849
勉強したことの要約ですから復習に使ってください。

0:03:10.220,0:03:14.259
今からこの方法を使って、MNISTの3と7を

0:03:14.930,0:03:17.349
識別するモデルを作りましょう。

0:03:18.049,0:03:24.309
まず、関数の入力を作る必要があります。

0:03:26.569,0:03:28.569


0:03:29.060,0:03:31.600
このような関数です。

0:03:32.270,0:03:37.929
ですので、1行に並べられたピクセルとそれと同様のパラメータが必要です。

0:03:41.060,0:03:43.060
ですから、xは

0:03:43.670,0:03:50.079
ピクセルです。今回はピクセルとパラメータを要素毎に掛け合わせてから和をとるので、

0:03:50.300,0:03:52.840
グリッドであること自体はさほど

0:03:53.660,0:03:55.660
重要ではないですから

0:03:56.300,0:03:58.250


0:03:58.250,0:04:00.250
ベクトルに変形してしまいましょう。

0:04:00.770,0:04:08.770
PyTorchではviewメソッドで変形します。引数は、変形後のshape、つまり

0:04:09.230,0:04:11.230
そもそも何次元にするか、そして

0:04:11.930,0:04:15.670
各次元の大きさです。ここでは、列の数は

0:04:17.239,0:04:21.429
1枚の画像に含まれるピクセル数、つまり

0:04:22.220,0:04:29.320
28x28で、行の数はデータセットに含まれる画像の枚数になります。

0:04:29.320,0:04:32.050
この-1は

0:04:32.690,0:04:34.690


0:04:35.420,0:04:37.479
データセットの大きさと等価です。

0:04:37.480,0:04:42.219


0:04:43.580,0:04:45.580
We can grab all our threes
ここで3と7全て用意して

0:04:45.650,0:04:49.480
cat関数で結合しました。そして最後にviewメソッドで

0:04:50.180,0:04:53.230
行列に変形しています。

0:04:53.960,0:04:56.679
その行列は各行が

0:04:57.380,0:05:02.080
1枚の画像に対応しています。

0:05:04.560,0:05:09.060
次にラベルを用意します。ここではラベル1が3、

0:05:09.669,0:05:14.249
ラベル0が7に対応しています。

0:05:14.770,0:05:17.160


0:05:20.130,0:05:26.809
この部分はベクトルを作っていますが、ラベルも行列で表したいので、

0:05:27.780,0:05:32.059
PyTorchのunsqueezeメソッドを使って

0:05:33.210,0:05:35.210


0:05:35.310,0:05:38.569
欲しい場所に次元を追加します。

0:05:38.790,0:05:42.170
つまり、ここでの処理は12396個の要素があるベクトルを

0:05:42.720,0:05:46.160
列が1、行が12396の行列に変えています。

0:05:46.890,0:05:51.199


0:05:52.500,0:05:55.429


0:05:57.150,0:06:03.650
このxとyをデータセットに整形します。

0:06:04.500,0:06:09.919
データセットは、[]を使って要素にアクセスができて、

0:06:11.040,0:06:15.890
各要素はタプルです。

0:06:17.760,0:06:20.460


0:06:23.020,0:06:25.020
ここでデータセットを作り、

0:06:25.570,0:06:30.029
要素を取得すると、タプルで、それは、インデックスに対応する列の

0:06:30.670,0:06:32.670
独立変数と従属変数を持っています。

0:06:33.730,0:06:35.530


0:06:35.530,0:06:36.970


0:06:36.970,0:06:44.580
これを作るにはPythonのzip関数を使います。入力それぞれの対応する位置の要素を

0:06:46.420,0:06:51.839
タプルにしていく関数です。

0:06:52.150,0:06:55.709
zip関数の戻り値をlistにすればデータセットになります。

0:06:56.410,0:07:03.869
dsetはリストになっていて、各要素は

0:07:04.510,0:07:11.159
1枚の画像とそれに対応するラベルです。

0:07:12.250,0:07:14.250


0:07:14.710,0:07:21.840
これは重要なコンセプトです。データセットはインデクシングできて、各要素がタプルです。

0:07:23.280,0:07:26.220
これはタプルの展開です。

0:07:26.220,0:07:28.350
タプルを0番目と1番目以降の2つに分けて、

0:07:28.510,0:07:34.200
各変数に代入しています。

0:07:34.200,0:07:36.779


0:07:38.610,0:07:45.770
ここまでの3ステップをバリデーションセットに対しても行います。これでデータセットの準備ができました。

0:07:47.550,0:07:49.760
では次にパラメータを初期化します。

0:07:50.820,0:07:52.820


0:07:53.610,0:07:58.880
ただランダムに初期化すれば良いのです。これがその関数です。

0:07:59.550,0:08:00.870


0:08:00.870,0:08:02.870
パラメータのshapeを与えれば

0:08:03.360,0:08:04.500


0:08:04.500,0:08:06.500
標準分布からのサンプリングで

0:08:07.950,0:08:15.710
パラメータを初期化します。shit+tabでrandnのドキュメントも見れます。

0:08:17.340,0:08:19.340


0:08:23.790,0:08:26.029
これは分散が1と言っています。

0:08:26.670,0:08:29.329
ここはstd（標準偏差）ではなくvar（分散）にするべきでしたね。

0:08:29.330,0:08:31.640


0:08:32.039,0:08:38.598
分散を掛け合わせることで調整します。

0:08:39.900,0:08:41.900
そして、このパラメータに関する

0:08:42.659,0:08:45.769
勾配を計算する必要があるので

0:08:46.380,0:08:51.499
requires_grad_を呼びます。

0:08:51.870,0:08:56.150
このアンダースコアは

0:08:56.400,0:09:02.299
呼び出し元に対してさ要することを意味します。

0:09:03.480,0:09:05.480


0:09:06.450,0:09:07.650


0:09:07.650,0:09:09.650
つまりこのTensorは勾配が必要だということです。

0:09:10.260,0:09:12.289
ここで重みを初期化しています。

0:09:12.870,0:09:14.820


0:09:14.820,0:09:21.770
shapeは(28*28, 1)です。28*28なのは各ピクセルに対応する重みが必要だからで

0:09:22.500,0:09:26.929
1は、出力の大きさを1にしたいからです。

0:09:28.740,0:09:30.740


0:09:32.070,0:09:34.070


0:09:35.529,0:09:37.529


0:09:38.740,0:09:44.939
重みとピクセルの積では不十分かもしれません。なぜならもしピクセルが全て0なら

0:09:45.430,0:09:48.449
出力が常に0になってしまうからです。

0:09:48.459,0:09:55.289
そこでwx+bという式にするためにバイアスbを用意します。

0:09:55.990,0:10:01.019
これはただの数字です。これも乱数で初期化します。

0:10:02.379,0:10:03.519


0:10:03.519,0:10:09.568
パラメータと重みは違うと話しました。ここでは重みがwで、

0:10:10.540,0:10:12.540


0:10:12.910,0:10:18.089
バイアスはb、そしてこの2つともが関数のパラメータで

0:10:18.519,0:10:22.739
訓練によって調整されるものです。

0:10:22.740,0:10:25.259
つまり、勾配を必要とし、値が更新されるものです。

0:10:26.589,0:10:31.679
モデルの重みとバイアスがパラメータです。

0:10:34.279,0:10:36.378


0:10:39.120,0:10:43.160
（受講生からの質問）勾配降下法と確率的勾配降下法の違いはなんですか？

0:10:45.570,0:10:50.030
ここまでやってきたのは勾配降下法で、確率的勾配降下法はこのあとやります。

0:10:52.339,0:10:57.469
さて、まずデータセットの最初の画像のラベルを予測を計算しましょう。

0:10:57.899,0:11:05.179
ここではweightsを転置することで要素積にし、それからsumで和を計算しています。最後に

0:11:05.519,0:11:07.729
バイアスを加えれば

0:11:08.490,0:11:10.490
予測です。

0:11:11.069,0:11:17.748
全ての画像の予測計算はfor-loopでもできますが、それだとGPUを有効活用できず、

0:11:18.779,0:11:24.948
また、最適化もされないので大変遅いです。

0:11:25.259,0:11:29.449
画像やピクセルに繰り返し処理を適用したい場合は

0:11:30.180,0:11:33.739
なるべくPythonのfor-loopを使わないようにしてください。

0:11:34.889,0:11:36.480


0:11:36.480,0:11:41.269
この予測の計算は行列積と呼ばれるものです。

0:11:41.910,0:11:43.969


0:11:44.790,0:11:46.790


0:11:47.399,0:11:51.529
行列積を知らなかった、もしくは忘れてしまったという人はKhan Academyか何かで

0:11:53.009,0:11:57.229
復習することをおすすめします。

0:11:57.779,0:12:05.779
簡単に説明します。このイラストはwikipdeiaから持ってきたものです。行列AとBがあります。

0:12:06.959,0:12:12.258
計算結果の(1, 2)に入るのは、行列Aのこの部分と

0:12:12.959,0:12:19.518
行列Bのこの部分の要素積の和です。

0:12:19.519,0:12:22.698
つまり、a_{1, 1} * b_{1, 2}

0:12:23.009,0:12:28.188
+ a_{1, 2} * b_{2, 2}です。

0:12:29.890,0:12:31.890
ここも同じようにして、

0:12:31.890,0:12:38.849
b_{1, 3} * a_{3, 1} + b_{2, 3} * a_{3, 2}となります。

0:12:49.440,0:12:51.440


0:12:51.440,0:12:52.529


0:12:52.529,0:12:59.119
matrixmultiplication.xyzも行列積の良い説明です。右側の行列を反時計回りに90度回し左の行列の上に書きます。

0:13:00.149,0:13:02.149
そして、それぞれを掛けて足し合わせるのです。

0:13:03.480,0:13:08.489
2つ目のものがこの位置に入り、2番目の場所で終わります。

0:13:08.490,0:13:10.490


0:13:14.490,0:13:16.950
これが行列積です。

0:13:18.279,0:13:20.849


0:13:23.010,0:13:26.939
予測は行列積で計算できます。

0:13:27.730,0:13:33.270
PyTorchでは@で行列積を表します。

0:13:33.280,0:13:36.449


0:13:37.630,0:13:39.630
1つ目のデータに対する予測は

0:13:40.959,0:13:44.549
20.2336でした。行列積を用いて

0:13:46.110,0:13:47.830


0:13:47.830,0:13:51.660
データセット全体に対する予測を計算すると、

0:13:52.180,0:13:57.420
ここに20.2336があり、その他データに対する予測結果も計算されています。

0:13:57.940,0:14:04.109
行列積を用いることで簡単な線形の関数を高速に計算できることは覚えておいてください。

0:14:04.930,0:14:09.030


0:14:11.300,0:14:15.039
この式はニューラルネットワークで登場する基本的な2つの式の1つです。

0:14:17.809,0:14:21.309
いくつかのデータを表す行列に重みを掛けて、バイアスを加えるという式です。

0:14:21.309,0:14:26.499
もう1つの基本的な式は活性化関数です。

0:14:29.600,0:14:35.920
ランダムに初期化したモデルの予測を計算したので評価しましょう。

0:14:36.410,0:14:40.270
評価のために予測が0以上なら

0:14:40.790,0:14:42.790
3の画像、そうでなければ7

0:14:43.370,0:14:49.989
というふうにしましょう。

0:14:51.329,0:14:55.469
preds > 0は予測が3ならばTrue、そうでなければFalseを返し、それを

0:14:56.889,0:14:58.589
floatに変換します。

0:14:58.589,0:15:02.488
こうすることで先ほど用意したラベルと比較できます。

0:15:03.309,0:15:05.199


0:15:05.199,0:15:06.639


0:15:06.639,0:15:08.290


0:15:08.290,0:15:13.139
(preds>0.0).float() == train_yは正解ならTrue、間違いならFalse

0:15:13.629,0:15:17.279
を返します。

0:15:18.309,0:15:23.999
So if we take all those trues and falses and turn them into floats, so that'll be ones and zeroes and then take their mean
さらにこれをfloatに変換し、平均を求めると、

0:15:25.520,0:15:32.629
0.49、つまり現時点でモデルは50%正しく識別できるということです。

0:15:33.300,0:15:35.330
ここでitemメソッドを使っています。

0:15:36.360,0:15:38.360
これなしだと

0:15:38.910,0:15:40.910
tensorが返されます。

0:15:40.950,0:15:45.109
ランク0のtensorです。

0:15:45.110,0:15:48.649



0:15:49.980,0:15:54.800
ここでは小数点以下を見やすくするためにPythonのスカラーを使いたかったので、itemを使いました。

0:15:55.230,0:16:02.420
そして、こうすることでパラメータの微小変化によるaccuracyの変化を

0:16:03.209,0:16:05.599
見やすくしたかったのです。

0:16:06.480,0:16:10.670
さて、ここでweights[0]に1.0001掛けて少し大きくしましょう。

0:16:11.730,0:16:14.570


0:16:15.810,0:16:18.150


0:16:18.910,0:16:20.910
そして、この新たな

0:16:23.500,0:16:29.580
重みで計算した予測のaccuracyと先ほどのaccuracyの差が

0:16:30.279,0:16:35.249
weights[0]に関するaccuracyの勾配です。予測は先ほどと同様に

0:16:36.190,0:16:41.429
計算し、閾値で0/1にし、ラベルと比較し、平均を取れば新たなパラメータによる予測です。

0:16:41.800,0:16:43.480


0:16:43.480,0:16:45.220


0:16:45.220,0:16:47.580
そしてaccuracyは全く同じでした。

0:16:48.279,0:16:50.279


0:16:52.150,0:16:53.590
勾配は

0:16:53.590,0:16:55.570


0:16:55.570,0:17:01.710
入力の変化に対する出力の変化です。忘れていたらKhan Academyなどで復習してください。

0:17:02.710,0:17:04.710


0:17:04.839,0:17:09.938
yの変化、つまりy_new - y_oldは

0:17:11.300,0:17:17.019
0.4912… - 0.4912… で0です。

0:17:19.430,0:17:21.199
weights[0]の変化は

0:17:21.199,0:17:26.139
全くaccuracyを変えませんでした。

0:17:27.380,0:17:29.380
つまり勾配が0です。

0:17:29.720,0:17:35.439
つまりパラメータも変わらないので予測も不変です。

0:17:38.760,0:17:40.020


0:17:40.020,0:17:46.430
勾配が0というのは問題です。

0:17:46.860,0:17:53.029
勾配が0だとパラメータは更新されず予測も改善されません。

0:17:53.430,0:17:54.140
直感的にいうと、

0:17:54.140,0:18:00.710
勾配が0な理由は、画像の1ピクセルだけを変えたところで、

0:18:01.140,0:18:09.110
その変化は予測が3だったものを7に変えることもまたその逆も起こさないだろうということです。

0:18:09.900,0:18:13.940
というのもこの閾値を使っているからです。

0:18:16.060,0:18:18.249
言い換えると

0:18:20.890,0:18:22.220
今使おうとしている

0:18:22.220,0:18:28.450
accuracy損失関数は凸凹です。それは平らな部分と段差でできてます。

0:18:29.060,0:18:30.170


0:18:30.170,0:18:35.379
多くのところで勾配が0です。この場合、私たちはaccuracyではない損失関数を

0:18:36.500,0:18:37.910


0:18:37.910,0:18:39.910
用意する必要があります。

0:18:41.150,0:18:47.920
では新しい関数を作りましょう。その関数はaccuracyが改善されたら

0:18:49.520,0:18:51.520
改善されるようなものです。

0:18:52.970,0:18:54.970


0:18:55.550,0:18:59.649
損失は小さいほど良いので、

0:18:59.809,0:19:05.319
損失が小さければaccuracyも良くなる必要があります。ですが、

0:19:06.260,0:19:07.880


0:19:07.880,0:19:09.880
勾配0は避ける必要があります。

0:19:10.269,0:19:15.429
予測が少しでもよければ、損失も良くなる必要があります。

0:19:17.149,0:19:19.149
では例を見ましょう。

0:19:19.369,0:19:21.049


0:19:21.049,0:19:24.338
ラベルをtargetsとしましょう。データは3つです。

0:19:24.950,0:19:29.349
3つのデータがあり、ラベルは1, 0, 1とします。

0:19:29.959,0:19:37.179
そして、あるニューラルネットで予測を計算し、その予測自体は(0.9, 0.4, 0.2)です。

0:19:38.479,0:19:40.479


0:19:40.700,0:19:46.779
そしてこの関数を損失関数とします。torch.dotを使います。これは基本的に

0:19:47.479,0:19:55.059
リスト内包で、if文です。ここではtargetが1なら1 - 予測を返します。

0:19:56.209,0:20:01.479
この要素はtargetが1なので損失は1 - 0.9です。

0:20:02.479,0:20:06.189
そうでなければ予測をそのまま返します。

0:20:07.640,0:20:09.640
ですのでこの例では、

0:20:09.870,0:20:12.410
1つめのターゲットは1なので

0:20:13.350,0:20:15.150
損失は1-0.9で

0:20:15.150,0:20:17.359
0.1

0:20:19.080,0:20:24.380
その次は0なので予測の0.4がそのまま損失、

0:20:24.380,0:20:28.070
3つ目はターゲットが1なので

0:20:28.070,0:20:34.579
1 - 0.2で0.8。

0:20:35.430,0:20:38.690
この関数だと、ターゲットが1なら予測が大きく、

0:20:38.690,0:20:44.539
ターゲットが0なら予測が小さくなります。

0:20:45.150,0:20:50.089
この例で最も外れた予測は0.2と予測しているものです。

0:20:51.390,0:20:54.619
つまり、ここでは0と予測しているけれどラベルは1という状態です。

0:20:54.620,0:21:00.530
そのため損失は0.8になっています。

0:21:01.080,0:21:03.080


0:21:03.210,0:21:04.560


0:21:04.560,0:21:10.729
こうして得た各予測に対する損失の平均を計算します。

0:21:11.160,0:21:13.519
この損失はが最小になるのは

0:21:14.280,0:21:16.639
予測が全て正しい時です。

0:21:17.550,0:21:21.529


0:21:23.450,0:21:25.450
つまり、

0:21:26.730,0:21:29.610
予測がラベルと全く同じ時

0:21:32.909,0:21:34.909
損失は

0:21:35.169,0:21:37.169
0, 0, 0です。

0:21:37.450,0:21:41.460
しかし、全く間違っている場合は、つまり、

0:21:42.549,0:21:44.549
1 - targetsのとき

0:21:45.830,0:21:47.830
1,1,1となります。

0:21:47.970,0:21:55.010
損失は予測がラベルに近い時に小さくなります。

0:21:57.900,0:22:03.719
ここで平均を取ると、その値は0.433ですが、

0:22:06.530,0:22:08.530
ここの最も間違っている

0:22:09.690,0:22:14.450
予測を0.2から0.8に変えると

0:22:14.970,0:22:16.560


0:22:16.560,0:22:18.120


0:22:18.120,0:22:25.910
損失は0.433から0.23になります。

0:22:26.580,0:22:33.799
この損失関数は実際のaccuracyに近い挙動を示します。

0:22:33.800,0:22:35.370
accuracyが上がれば損失は小さくなります。

0:22:35.370,0:22:41.450
しかも、この損失関数では勾配0が起きません。

0:22:41.700,0:22:46.309


0:22:47.340,0:22:52.849
一つ問題をあげるとすれば、予測が0から1の間でないとこの損失が使えないことです。

0:22:53.220,0:22:55.940
そうでないと、1 - predictionの結果はおかしなことになります。

0:22:57.610,0:23:01.780
ですから、常に予測が0-1に収まることを保証する必要があります。

0:23:02.480,0:23:07.209
そして、そうすることは直感的にも意味があると感じるでしょう。なぜなら、

0:23:07.210,0:23:09.340
予測を確率のようなもの、あるいは最低でも

0:23:09.890,0:23:12.009
そのスケールを表す数字にしたいからです。

0:23:13.220,0:23:16.329
そこで、予測結果をうけとり、

0:23:17.570,0:23:19.570


0:23:19.670,0:23:21.670


0:23:23.850,0:23:31.010
入力としてはこのような大きな数字にも対応し、0-1に変換する関数が必要です。

0:23:32.160,0:23:34.790
そのような関数は実際に存在し、

0:23:35.730,0:23:37.730
シグモイド関数と呼ばれています。

0:23:37.860,0:23:39.890
シグモイド関数はこのような形です。

0:23:40.320,0:23:44.180
小さい数字には0に近い数字を返し、

0:23:44.400,0:23:47.809
大きい数字を入力すると1に近い数字を返します。

0:23:48.940,0:23:50.940
it never gets past one and
そして、その結果は

0:23:51.499,0:23:57.249
0より小さくなることも1より大きくなることもなく、0-1の間では滑らかな曲線で

0:23:57.259,0:23:59.289
中心ではy=xのような動きをします。

0:24:00.440,0:24:03.580
シグモイド関数の定義は

0:24:03.919,0:24:08.589
1 / (1 + e(-x))です。

0:24:10.610,0:24:15.640
exp(x)というのは　　　

0:24:16.490,0:24:17.630
eのx乗です。

0:24:17.630,0:24:19.630
eというのは

0:24:23.740,0:24:30.030
円周率πのような値です。

0:24:32.060,0:24:34.629


0:24:39.370,0:24:42.579


0:24:45.590,0:24:47.590


0:24:48.860,0:24:55.849


0:24:59.450,0:25:04.999
私は、面白い関数を見た時にその定義は特に気にすることなく、

0:25:06.030,0:25:09.920
どんな形かを気にします。

0:25:10.140,0:25:10.640


0:25:10.640,0:25:14.150
ですので、みなさんもグラフを描画してみて、

0:25:14.400,0:25:18.499
その特定の式がどんな形になるか確認してみてください。

0:25:19.380,0:25:22.970
しかし私は関数の定義自体が

0:25:23.580,0:25:30.379
本当に重要だと考えたことは一度もありません。重要なのはこのシグモイド関数の形です。

0:25:31.380,0:25:34.249
全ての数字を0と1の間に押し込めるのです。

0:25:36.110,0:25:43.840
では、mnist_lossを元通りにする前に予測をシグモイド関数に渡しましょう。

0:25:44.810,0:25:51.340
それから、torch.where、今回はこれが損失関数の肝になります。

0:25:52.130,0:25:55.510
この関数を使えば勾配0を避けることができます。

0:25:56.390,0:26:01.150


0:26:02.150,0:26:04.150


0:26:05.490,0:26:07.490


0:26:07.690,0:26:12.429
ここでこうする理由は、accuracyが

0:26:13.940,0:26:15.860


0:26:15.860,0:26:23.110
本当に気にしているものですが、accuracyではパラメータ更新に十分な勾配が得られません。

0:26:23.750,0:26:25.750


0:26:29.840,0:26:31.730
そこで、accuracyのかわりに

0:26:31.730,0:26:36.759
その関数の値がよければaccuracyも良いことを意味し、

0:26:36.759,0:26:42.699
さらに勾配0をもたない関数を使うのでした。

0:26:42.700,0:26:46.419
これで私たちがメトリクスと損失を使う理由がわかったと思います。

0:26:46.549,0:26:53.648
メトリクスは本当に気にしているもので損失は、メトリクスに似ていて、なおかつ

0:26:54.200,0:26:55.940
勾配がちゃんと計算できるものです。

0:26:55.940,0:26:57.830


0:26:57.830,0:27:03.939
時には勾配をきちんと計算できるメトリクスもあるので損失として使えます。

0:27:04.309,0:27:06.339
例えば、平均二乗誤差です。

0:27:07.429,0:27:09.429
しかし分類問題はそうではありません。

0:27:10.129,0:27:12.129


0:27:12.680,0:27:15.579
ですから、この損失を使って

0:27:17.150,0:27:19.150
パラメータを更新します。

0:27:20.140,0:27:25.869
パラメータ更新の方法はいくつかあります。1つは、画像を1枚ずつ使う方法です。

0:27:27.260,0:27:29.260
1枚の画像のラベルを予測し、その予測に対する損失を計算し

0:27:30.050,0:27:32.050


0:27:32.480,0:27:35.320
パラメータを更新する。この処理を

0:27:35.660,0:27:41.019
繰り返すわけですが、これは本当に遅いプログラムになってしまします。

0:27:42.030,0:27:43.750


0:27:43.750,0:27:47.849
なぜなら、画像1枚につきパラメータ更新を1回行うのですから

0:27:48.850,0:27:53.130
1エポックがかなり長くなります。1回の更新でデータセット全体を使えば

0:27:53.890,0:27:57.839
速くなります。

0:27:58.720,0:28:00.510
巨大な行列積になります。

0:28:00.510,0:28:04.499
行列積はGPUで高速に実行されます。

0:28:04.870,0:28:09.390
予測と損失の計算をしたら勾配を計算してパラメータを更新します。

0:28:09.910,0:28:11.910


0:28:12.430,0:28:15.209
しかし、これも微妙です。

0:28:16.210,0:28:21.180
時にデータセットは何百万、何千万もの規模になるので、

0:28:21.850,0:28:25.560
1回のパラメータ更新で全データを使うのは非効率です。

0:28:26.230,0:28:27.790


0:28:27.790,0:28:31.739
妥協策として、一度にデータセットの一部だけを使って

0:28:32.620,0:28:35.280
パラメータを更新します。

0:28:35.560,0:28:40.649
この1回のパラメータ更新にもちいるいくつかのデータサンプルをミニバッチと呼びます。

0:28:40.840,0:28:42.840
ミニバッチというのは

0:28:43.390,0:28:45.390
データ数サンプルを表します。

0:28:45.820,0:28:51.360
ミニバッチに含まれるデータの数はバッチサイズと呼ばれます。

0:28:51.360,0:28:57.180
バッチサイズがデータセットのサイズに近ければ、1回のパラメータ更新にかかる時間は

0:28:57.280,0:28:59.280
長くなりますが、

0:28:59.440,0:29:01.620


0:29:02.290,0:29:06.269
バッチサイズが大きいと、その分、勾配は実際のデータセットの勾配に近づきます。

0:29:07.060,0:29:09.719


0:29:10.360,0:29:17.280
一方、バッチサイズが小さければ、各ステップは早くなりますが、

0:29:17.470,0:29:20.430
勾配は実際のデータセットから計算される勾配とはあまり

0:29:21.280,0:29:23.910
似てないかもしれません。

0:29:26.830,0:29:32.160
損失の平均の代わりに中央値を使うとどうなるのでしょうか？中央値は

0:29:32.350,0:29:35.550
平均より外れ値に対して頑健です。

0:29:38.279,0:29:42.898
先ほどの例で3つ目がひどく間違っていましたが、その場合は

0:29:42.899,0:29:48.598
勾配がパラメータを誤った方向に移してしまうのではないでしょうか？

0:29:52.260,0:29:54.260
中央値を試したことはありません。

0:29:55.529,0:30:01.879
中央値はミニバッチのたった1つの数字しか使いませんね？

0:30:03.210,0:30:05.210


0:30:05.220,0:30:07.220
つまり、

0:30:07.590,0:30:11.480
予測のほとんどを無視しています。

0:30:12.269,0:30:16.669
ですから、中央値を使って訓練したモデルは真ん中にあるものを予測するのに長けたものになると思います。

0:30:17.760,0:30:19.760


0:30:19.889,0:30:22.669
とはいえ試したことがないので気になりますね。

0:30:24.059,0:30:30.319
もう1つ起こりそうなことは勾配0だと思います。

0:30:30.320,0:30:33.439
（理解できず訳せませんでした）

0:30:34.350,0:30:36.350
（理解できず訳せませんでした）

0:30:36.750,0:30:41.750
（理解できず訳せませんでした）

0:30:42.720,0:30:48.589
あまりうまくいかないと思います。ただの推測なので試してみてください。

0:30:50.970,0:30:52.970
では、

0:30:53.200,0:30:55.799
ミニバッチはどう作れば良いのでしょうか。

0:30:56.679,0:31:03.419
PyTorchとfasitaiがその機能を提供しています。

0:31:04.150,0:31:08.099
どんなデータセットでもこのDataLoaderに渡せば、

0:31:08.920,0:31:14.699
ミニバッチを作ってくれます。バッチサイズは自分で指定できます。

0:31:16.090,0:31:18.540


0:31:18.540,0:31:23.459
そして、データセットのデータを全て使うまでミニバッチを作り続けてくれます。

0:31:23.500,0:31:27.569
ここでは、0から14までの数字からなるデータセットを用意し、

0:31:29.140,0:31:32.219
バッチサイズ5でDataLoaderにわたすと、

0:31:33.460,0:31:37.110
イテレータになります。

0:31:37.110,0:31:42.630
イテレータをlist関数に渡すと、イテレータから取得できる全ての要素を

0:31:42.630,0:31:46.919
含むリストになります。つまりここにはミニバッチが3つあり

0:31:47.470,0:31:50.640
0から14までの数字が含まれていることがわかります。

0:31:50.799,0:31:57.299
そして数字の出現する順番はランダムで1つのミニバッチに数字が5つ含まれます。

0:31:57.880,0:32:00.900
ランダムなのはshuffle=Trueとしているからで、学習データでは一般的です。

0:32:01.090,0:32:05.880
このshuffleによりさらに確率化されます。これは良いことでなぜなら

0:32:06.220,0:32:09.870
データセットがどんなものかがよりわかりにくくなるからです。

0:32:11.980,0:32:15.599
これがDataLoaderです。

0:32:17.590,0:32:19.590


0:32:19.630,0:32:23.549
ところで、私たちのデータセットはタプルを返します。

0:32:24.160,0:32:28.589
この例ではただ整数を返しているので、タプルを返すようにしましょう。

0:32:28.840,0:32:34.319
アルファベットを数え上げると、(0, ‘a’), (1, ‘b’), (2, ‘c’)というふうになります。

0:32:34.840,0:32:40.770
これをデータセットにしてみましょう。これをDataLoaderにわたし、

0:32:41.350,0:32:46.500
バッチサイズを6とすると、このdata loaderはミニバッチとしてタプルを返します。

0:32:47.590,0:32:48.970


0:32:48.970,0:32:50.650


0:32:50.650,0:32:57.689
このタプルは6個の数字とそれに対応するアルファベットから構成されています。

0:32:58.600,0:33:01.050
左側は私たちにとっては独立変数、そして右側は

0:33:01.570,0:33:08.790
従属変数のようですね。そして、最後のミニバッチですが、バッチサイズが

0:33:10.000,0:33:13.800
データセットのサイズの約数とは限らないので最後のバッチだけ小さいです。

0:33:14.500,0:33:16.500


0:33:19.170,0:33:23.659
私たちはすでにデータセットを用意しています。

0:33:24.540,0:33:28.999
それをDataLoaderに渡せば、Pythonのイテレータと同等のものになり、

0:33:29.280,0:33:32.119
そのイテレータを使ってloopすることができます。

0:33:32.280,0:33:37.430
DataLoaderはデータセット同様にタプルを返すので、

0:33:37.950,0:33:42.649
1つ目の要素とそれ以降の要素に展開できます。

0:33:43.470,0:33:46.339
ここではそれがxとyです。xがあれば予測を計算できます。

0:33:47.100,0:33:54.079
予測とyから損失を計算し、それからPyTorchに勾配を計算してもらいます。

0:33:55.020,0:34:02.060
二次関数を使ったSGDの例と同様にパラメータを更新します。

0:34:03.480,0:34:11.030
ではweightsとbiasを作り直してから、MNISTのデータセットで

0:34:11.429,0:34:14.929
DataLoaderを作り、バッチサイズも大きくしましょう。

0:34:14.929,0:34:18.049
いつも多くのことをやっているのでまずは

0:34:18.419,0:34:25.909
1つ目のミニバッチをみましょう。firstはfastaiの関数でiteratorの1つ目の要素を撮ってきます。

0:34:26.040,0:34:29.870
任意のミニバッチを扱うのでそれを確認するのは大事です。

0:34:31.770,0:34:35.509
これが1つ目のミニバッチのshapeで

0:34:36.330,0:34:42.499
256行と28x28で784列です。つまり256枚のベクトルに変形された画像と

0:34:43.500,0:34:45.030


0:34:45.030,0:34:51.140
256個のラベルで、1が3、0が7に対応しています。

0:34:52.650,0:34:56.150
バリデーションセットのDataLoaderも作ります。

0:34:58.800,0:35:02.729


0:35:04.390,0:35:10.890
ここでミニバッチを作っているのは、、、なんででしたっけ

0:35:12.400,0:35:14.400


0:35:16.950,0:35:18.300


0:35:18.300,0:35:24.810
多分私が手で確認したかっただけですね。はじめの4つもってきて、ちゃんと動いているかを。

0:35:24.810,0:35:28.169
4つサンプルをもってきて、batchという変数にして、

0:35:29.470,0:35:35.970
linear1関数に渡しています。linear1関数は

0:35:39.640,0:35:46.099
xbatch @ weights の行列積の結果にbiasを加えます。

0:35:51.250,0:35:52.040


0:35:52.040,0:35:58.209
linear1関数は4つのサンプルの予測を計算します。

0:35:59.120,0:36:03.729
この予測に対する損失をこの損失関数で計算します。ここでbatchに対応するラベルを取得しています。

0:36:04.340,0:36:07.209
そしてこれが損失です。

0:36:08.180,0:36:14.020
Okay, and so now we can calculate the gradients and so the gradients are
ここまで来れば勾配を計算できます。gradのshapeは(784, 1)です。

0:36:15.950,0:36:16.900


0:36:16.900,0:36:17.770


0:36:17.770,0:36:19.730


0:36:19.730,0:36:27.429
各weightが少しだけ変化した時の損失の変化がgradientです。

0:36:28.370,0:36:33.849
biasもgradを持ちます。biasは要素が1つだけなのでgradも1つだけです。

0:36:37.240,0:36:39.310
この3つのステップは1つの関数にまとめられます。

0:36:39.980,0:36:43.300
calc_grad関数は勾配を求める関数で、

0:36:43.300,0:36:48.369
xとyとモデルを入力に受け取り、予測、損失、そしてbackwardステップを実行します。

0:36:49.010,0:36:51.399


0:36:52.460,0:36:55.990
ここでcalc_grad関数を使っていて、

0:36:56.330,0:37:00.279
weightの勾配の平均とbiasの勾配を確認しています。

0:37:00.920,0:37:02.920


0:37:03.420,0:37:06.690
もう一度calc_grad関数を実行してみましょう。

0:37:07.329,0:37:12.989
パラメータ更新はしていません。勾配の値が変わりましたね。

0:37:14.410,0:37:19.119
同じデータですから、同じ値になることを期待していたかもしれません。

0:37:20.030,0:37:22.030
なぜ勾配が変わったのでしょうか？

0:37:22.609,0:37:28.179
それはloss.backwardは勾配を計算するだけでなく、

0:37:28.490,0:37:35.439
すでにパラメータが持っている勾配、つまり.gradに今計算した勾配を加えるからです。

0:37:35.960,0:37:37.960


0:37:38.119,0:37:42.578
この挙動の理由はあとで説明しますが、今はそういうものであることは認識しておいてください。

0:37:43.160,0:37:46.149
そこで、私たちが行うべきは

0:37:46.819,0:37:52.749
grad.zero_()を実行することです。.zeroは0埋めされたtensorを返します。

0:37:53.299,0:37:56.319
アンダースコアがついているのでgradが0埋めされます。

0:37:57.049,0:38:01.209
つまりweights.gradもbias.gradも0になりました。

0:38:02.700,0:38:07.129
ここでcalc_gradを実行すると同じ結果になります。

0:38:08.490,0:38:10.110
そしてこれが

0:38:10.110,0:38:11.250


0:38:11.250,0:38:17.419
SGDを使った1エポックの学習コードです。DataLoaderをループし、xとyのミニバッチを取得し、

0:38:18.480,0:38:20.480
calc_grad関数で

0:38:20.780,0:38:22.780
prediction loss backward
予測、損失の計算とbackward

0:38:24.970,0:38:29.260
Each other parameters we're going to be passing those in so there's going to be the
関数に渡す各パラメータ、

0:38:30.260,0:38:33.879
768 weights and the one bias and then for each of those
つまり768個の重みと1つのバイアスですが、

0:38:34.790,0:38:36.790
Update the parameter
パラメータを更新します。

0:38:37.190,0:38:41.440
p.data - p.grad * lr。

0:38:42.349,0:38:47.649
これが確率的勾配降下法の更新で、次のイテレーションのために勾配を0にします。

0:38:49.010,0:38:53.080
ここでp -= ではなくp.data -=としている理由は

0:38:53.930,0:38:57.430
PyTorchは

0:38:57.830,0:39:03.789
勾配を計算するために全ての計算の履歴を残していますが、

0:39:04.700,0:39:07.270
勾配降下法の更新の勾配はいりません。

0:39:07.910,0:39:12.099
モデルの一部ではありません。

0:39:12.410,0:39:20.349
.dataはPyTorchに勾配計算が不要であることを伝えたい時に使うattributeです。

0:39:21.349,0:39:25.809


0:39:26.480,0:39:28.480


0:39:28.650,0:39:35.660
これが最もシンプルな確率的勾配降下法のループです。先ほど確率的勾配降下法と勾配降下法の違いを聞かれましたが、

0:39:36.090,0:39:40.549
勾配降下法はこのようなループがありません。

0:39:42.930,0:39:44.920
こんなふうに

0:39:44.920,0:39:51.720
勾配降下法のためのミニバッチなどはなく、データセット全体を使います。

0:39:52.059,0:39:56.939
勾配降下法では、ミニバッチではなく、データセット全体を使って

0:39:56.940,0:40:00.240
勾配を計算し、パラメータを更新します。

0:40:00.490,0:40:05.490
現実的にはほとんど使いません。

0:40:06.099,0:40:08.429
基本的にはいろんなサイズのミニバッチを使います。

0:40:16.070,0:40:18.429


0:40:19.130,0:40:22.179
以前使った関数をもう一度みていきましょう。

0:40:23.090,0:40:27.309
最初は予測結果が0以上かという条件でしたが、

0:40:27.530,0:40:33.159
今はシグモイド関数、与えられた値を0-1の範囲内に押し込める関数を使っているので、

0:40:33.470,0:40:38.800
予測を比べるべき対象は0.5です。

0:40:39.470,0:40:41.590
シグモイド関数のおさらいをしましょう。

0:40:46.030,0:40:51.040
0はシグモイド関数では0.5です。

0:40:51.710,0:40:55.689
ですからaccuracyの計算する関数を少しだけ変えました。

0:41:01.890,0:41:08.690
xとyのaccuracyを計算するには、このxは予測ですね

0:41:09.480,0:41:15.950
まず予測のシグモイドを計算して、それから0.5と比べて3かどうかを見ます。

0:41:16.170,0:41:19.519
この結果と実際のラベルを比較して、どこが正解かを見て、

0:41:20.280,0:41:22.280
boolからfloatに変換した後に

0:41:22.740,0:41:24.740
平均を計算します。

0:41:26.040,0:41:31.759
では、batchをlinear1関数に渡して、

0:41:32.430,0:41:36.680
対応するラベル4つと比較してaccuracyを計算しましょう。

0:41:38.400,0:41:44.929
これをバリデーションセット全体に対して行うときはリスト内包が使えます。

0:41:45.420,0:41:47.600
every batch in the validation set
バリデーションセットのミニバッチそれぞれの

0:41:50.640,0:41:52.980
accuracyを計算して、

0:41:54.460,0:42:00.869
それらをstackします。ここはリストです。tensorのリストをtensorにしたいときは

0:42:01.000,0:42:05.129
torch.stack関数を使います。

0:42:06.040,0:42:08.040


0:42:08.680,0:42:10.680
平均を計算して、

0:42:11.140,0:42:14.700
itemメソッドを使ってPythonのスカラーにします。

0:42:15.910,0:42:20.399
その結果を見やすさのために小数点以下4桁までに丸めます。

0:42:21.400,0:42:25.319
これがバリデーションセットに対するaccuracyで、モデルがランダムなのでほぼ50%ですね。

0:42:27.250,0:42:29.399
今から1エポックだけ訓練しましょう。

0:42:30.370,0:42:31.930


0:42:31.930,0:42:33.880
train_epoch関数の

0:42:33.880,0:42:34.900
引数は

0:42:34.900,0:42:40.740
parameters、ここではweightsとbias、

0:42:42.960,0:42:49.919
linear1モデルを使って、学習率は1として、訓練した後に

0:42:50.470,0:42:52.919
validate_epochでaccuracyを計算すると、

0:42:54.609,0:42:56.539
accuracyは

0:42:56.539,0:42:59.768
68.8%です。たった1エポックだけなので

0:43:00.650,0:43:02.920
これを何回も

0:43:03.799,0:43:05.359
繰り返します。

0:43:05.359,0:43:12.098
このようにaccuracyは上がり続け97%になりました。

0:43:13.960,0:43:15.960


0:43:16.140,0:43:18.140
素晴らしいですね。

0:43:18.910,0:43:21.839
私たちは簡単な線形関数の確率的勾配降下法をつくり

0:43:22.480,0:43:28.919
3と7だけのMNISTを97%識別できるようになりました。

0:43:31.140,0:43:35.879
処理がたくさんあるので、少しリファクタリングしましょう。

0:43:36.850,0:43:39.510
簡単なリファクタリングとして、

0:43:39.610,0:43:43.589
いくつかやることはありますが、基本的にはOptimizerと呼ばれるものを作ることです。

0:43:44.710,0:43:46.859
まず初めにlinear1関数を

0:43:48.370,0:43:50.370
使うのをやめます。

0:43:51.630,0:43:53.630
でもlinear1関数が

0:43:55.109,0:43:58.379
x@w + bというのは覚えておいてください。

0:44:00.009,0:44:04.559
実はPyTorchはこれと全く同じことを行うクラス

0:44:04.829,0:44:09.988
nn.Linearがあります。nn.Linearは主に2つのことをします。

0:44:10.509,0:44:11.650


0:44:11.650,0:44:17.460
linear1の計算とパラメータの初期化です。ですから

0:44:18.910,0:44:21.420
weightsとbiasを管理しなくて良くなります。

0:44:22.239,0:44:23.380


0:44:23.380,0:44:28.950
nn.Linearのインスタンス、これは(28x28, 1)の行列と

0:44:29.829,0:44:33.719
大きさ1のバイアスを作ります。

0:44:34.359,0:44:39.389
requires_gradもTrueにします。これらはすべてこのクラスに隠蔽されています。

0:44:39.390,0:44:42.509
そしてこれを関数として呼ぶと

0:44:43.480,0:44:45.339
x@w + bを実行します。

0:44:45.339,0:44:47.339


0:44:48.760,0:44:53.880
パラメータをチェックします。パラメータはそれぞれ

0:44:54.670,0:45:01.649
weightsが784、biasが1です。.parameters()を呼んでそれをw, bに展開します。

0:45:01.650,0:45:03.190
ほら、

0:45:03.190,0:45:04.600
784と

0:45:04.600,0:45:07.140
1がweightでbiasが1ですね。

0:45:08.410,0:45:10.410


0:45:10.490,0:45:14.979
このクラスを1から実装することは良い練習問題でしょう。

0:45:15.760,0:45:17.949
できるはずです。

0:45:18.890,0:45:24.339
自分でnn.Linearと全く同じように動作するものを自力で実装できることを確認してください。

0:45:26.019,0:45:31.109
So now that we've got this object which contains our parameters in a parameters
さて、nn.Linearのオブジェクトをつくり、それは.parameters()でどんなパラメータがあるか

0:45:32.229,0:45:40.139
わかります。ここまで来ればOptimizerを作れます。Optimizerには学習率とパラメータを渡します。

0:45:40.839,0:45:42.839


0:45:43.269,0:45:47.909
そして、step関数で、各パラメータを更新します。

0:45:47.910,0:45:52.649
この更新式は先ほどと同じで、p.data -= p.grad * lrです。

0:45:53.410,0:45:57.029
zero_grad関数で全てのパラメータの勾配（p.grad）を0にします。

0:45:57.549,0:45:59.669
もしくはNoneにしても良いです。

0:46:01.070,0:46:06.219
これをBasic Optimizerとしましょう。ここのコードはどれも

0:46:06.220,0:46:10.810
すでに使ったものでただクラスにまとめただけです。今からoptimizerのオブジェクトを

0:46:11.510,0:46:16.540
作ります。パラメータはlinearのもので、学習率は先ほどのものと同じです。

0:46:17.630,0:46:19.630
今度は学習のループですが、

0:46:19.940,0:46:22.029
DataLoaderの各ミニバッチに対して、

0:46:22.760,0:46:24.350
勾配を計算して、

0:46:24.350,0:46:26.350
opt step()

0:46:26.660,0:46:28.660
opt.zero_grad()

0:46:29.450,0:46:31.450
これだけです。

0:46:32.500,0:46:34.500
バリデーション用の関数は変更要らずです。

0:46:36.170,0:46:43.879
今度はこの学習ループを関数にまとめましょう。何エポックもtrain_epochを実行し、バリデーションaccuracyをプリントします。

0:46:45.410,0:46:48.319
実行するとさっきと同じです。

0:46:49.230,0:46:51.409
微妙に異なる結果ですが、

0:46:52.080,0:46:54.590
同じアイデアです。

0:46:55.470,0:46:57.470


0:46:57.990,0:47:00.079


0:47:01.800,0:47:04.190
リファクタリングしました。自分たちのOptimizerを作って

0:47:04.980,0:47:09.500
PyTorchのnn.Linearクラスを使うことで。

0:47:10.290,0:47:11.850
しかし、

0:47:11.850,0:47:18.799
実はBasicOptimを使う必要もありません。PyTorchがすでに実装しています。

0:47:19.740,0:47:21.740
SGDというクラスです。

0:47:22.470,0:47:28.970
このSGDはfastaiのものです。fastaiとpytorchの機能は一部重複しています。

0:47:29.970,0:47:34.220
そして同じように動きます。SGDには

0:47:34.590,0:47:39.049
パラメータと学習率をBasicOptimと同じように渡せます。

0:47:40.290,0:47:42.560
これで訓練すると同様の結果が得られます。

0:47:44.260,0:47:46.260
これらのfastaiとPyTorchの

0:47:46.630,0:47:49.559
クラスは摩訶不思議で複雑怪奇なものではなく

0:47:50.350,0:47:52.350
私たちが自分たちで

0:47:52.810,0:47:56.159
実装してきたものの

0:47:57.340,0:48:01.620
ラッパーのようなものです。ここまで非常に多くのステップがありました、

0:48:02.380,0:48:06.630
勾配降下法を知らなかった人は本当に多くの発見があったでしょう。

0:48:07.480,0:48:09.959
ですから、今日のレッスンは

0:48:11.830,0:48:15.759
非常に重要なレッスンで、一度立ち止まって、

0:48:15.760,0:48:22.420
理解を確かめるべきです。データセットは何か。

0:48:23.000,0:48:24.830
DataLoaderはとはどんなものか。

0:48:24.830,0:48:26.540
nn.Linearとは

0:48:26.540,0:48:33.580
SGDとは...もしまだはっきりわからないものがあれば、私たちがPythonで実装したところまで

0:48:33.950,0:48:35.950
戻りましょう。

0:48:36.500,0:48:42.550
DataLoaderは実装しませんでしたがその機能はそれほど面白くないです。

0:48:43.430,0:48:49.209
DataLoaderも自分で実装できるはずです。

0:48:52.069,0:48:54.069
もうすこしリファクタリングしましょう。

0:48:55.780,0:49:01.769
fastaiにはDataLoadersクラスがあります。これは簡単なクラスで

0:49:02.260,0:49:08.760
複数のDataLoaderを渡すことができ、.trainや.validでアクセスできます。

0:49:09.490,0:49:14.189
シンプルなクラスですが、便利です。

0:49:15.400,0:49:20.820
1つのオブジェクトで私たちのデータを全て把握していて

0:49:21.070,0:49:26.309
学習用データはシャッフルされて、バリデーションデータはシャッフルされてないこと、そして全てちゃんと動くことを確認します。

0:49:27.550,0:49:32.580
DataLoadersに学習用とバリデーションようのDataLoaderを渡したら、次は

0:49:33.460,0:49:37.080
fastaiのLearnerです。

0:49:37.390,0:49:40.230
LearnerクラスにはDataLoadersと

0:49:40.780,0:49:43.409
モデル、

0:49:44.470,0:49:46.470
そして、

0:49:46.990,0:49:53.070
最適化の関数、そして損失関数を渡します。さらに、メトリクスも渡します。

0:49:53.740,0:49:56.669
今まで自分たちでやってきたことです。

0:49:57.430,0:50:01.589
これからはlearnerがやってくれます。

0:50:01.589,0:50:06.659
つまり、train_modelやtrain_epochをやってくれます。

0:50:06.760,0:50:11.369
learnerに対して、

0:50:11.950,0:50:13.950
learn.fitを呼べば、

0:50:14.450,0:50:18.409
ほら、同じことを実行して、同じ結果が得られましたね。

0:50:19.200,0:50:20.960
しかもいくつか良い機能があります。

0:50:20.960,0:50:26.060
結果を綺麗なテーブルで出力し、さらに損失とaccuracyさらに所要時間もわかります。

0:50:27.000,0:50:31.010
何も特別なことはありません。全く同じことを自分の手で

0:50:31.680,0:50:33.680
PythonとPyTorchで

0:50:34.140,0:50:36.140
実装できます。

0:50:36.140,0:50:42.530
こういった抽象化はコード量を減らすこと、時間を節約すること、そして、理解するための

0:50:43.020,0:50:47.089
労力を節約します。特に特別なことはしていません。どれも自分でできることです。

0:50:48.839,0:50:53.818
もしライブラリなどがあなたが自分でできないことを行っている場合、

0:50:54.549,0:50:56.410
カスタマイズすることや

0:50:56.410,0:51:00.749
デバッグすることは不可能です。プロファイリングすることできません。

0:51:01.869,0:51:04.258
ですから、私たちは、

0:51:04.930,0:51:08.219
自分たちで使っているものは理解できることをしていることを確かにしたいのです。

0:51:10.960,0:51:15.159
これはただの線形関数です。これをニューラルネットにするには

0:51:16.310,0:51:18.310
どうすれば良いのでしょうか？

0:51:18.560,0:51:20.560


0:51:21.260,0:51:22.430
線形関数というのは

0:51:22.430,0:51:28.240
x @ w + bです。これをニューラルネットワークにします。

0:51:29.000,0:51:31.179
線形関数を2つ使います。

0:51:32.480,0:51:35.889
全く同じ関数ですが重みとバイアスが違います。この2行の間に

0:51:36.590,0:51:38.590
1行あります。

0:51:39.410,0:51:45.339
これは線形関数の結果と0の大きい方を返します。

0:51:46.220,0:51:53.709
つまりmax(res, 0)はresの負の値を全て0にする操作です。

0:51:54.230,0:51:56.230
線形関数を実行し、

0:51:56.690,0:52:02.169
負の値を全て0に置き換え、そしてその結果をまた線形関数に入力します。

0:52:02.990,0:52:05.619
信じられないかもしれませんがこれがニューラルネットです。

0:52:07.059,0:52:08.619


0:52:08.619,0:52:14.129
w1とw2は重みでb1とb2はバイアスです。

0:52:14.890,0:52:20.699
このモデルは先ほどと全く同じ学習コードで訓練できます。

0:52:21.729,0:52:23.729


0:52:25.290,0:52:28.620
res.max(0)は

0:52:31.240,0:52:34.990
正規化線形関数と呼ばれるものでふつう、

0:52:35.690,0:52:37.070
ReLUと書かれます。

0:52:37.070,0:52:40.600
PyTorchには実装されていて、

0:52:41.270,0:52:48.639
F.reluです。プロットするとこのようになり、

0:52:48.640,0:52:54.100
xが負なら0、それ以外はy=xになっています。

0:52:56.289,0:52:58.289


0:52:58.509,0:53:00.509
新しい専門用語ですが、

0:53:00.759,0:53:07.049
正規化線形関数というと複雑そうですが、その実態は

0:53:07.690,0:53:11.999
ほんの数行でかける非常に簡単な関数で、深層学習でよく使われます。

0:53:12.729,0:53:18.899
複雑であったり、洗練されていたり、印象的に聞こえるものは往々にして

0:53:19.690,0:53:23.339
それがなんであるか知ってしまえば非常にシンプルであることがわかります。

0:53:24.609,0:53:25.839
ところで

0:53:25.839,0:53:27.839
私たちは、

0:53:27.880,0:53:33.749
linear, ReLU, linearという計算をするのでしょうか。もし

0:53:38.980,0:53:40.980
真ん中のReLUを取り除くと

0:53:41.359,0:53:44.049
linear, linearとなり2つをまとめて

0:53:44.480,0:53:52.480
1つのlinearで表せます。

0:53:52.790,0:53:56.560
ただ重みを変えるだけで1つのlinearで表せるようになります。

0:53:56.930,0:54:01.570
linearだけをどんなに使っても、結局は1つのlinearより複雑になることはありません。

0:54:02.390,0:54:06.310


0:54:07.560,0:54:11.039
でも、非線形性をいれることで

0:54:12.380,0:54:14.750
全く違うものになります。

0:54:15.509,0:54:23.329
これはもう、重みとバイアスが十分に大きければどんな関数でも、例えば3と7を識別する
関数であっても

0:54:24.000,0:54:26.449


0:54:27.240,0:54:28.859
近似できるという

0:54:28.859,0:54:33.018
普遍性定理が成り立つものになります。

0:54:33.660,0:54:35.660


0:54:36.599,0:54:42.679
驚くべき事実です。本の数行のコードがどんな関数も近似できるのです。

0:54:43.589,0:54:46.758
もしw1, b1, w2, b2が

0:54:47.549,0:54:51.799
良い数字ならばという条件付きですが。そしてそれらの良い数字はSGDを使えばよいのです。

0:54:52.950,0:54:55.549
時間はかかるでしょう。メモリーはたくさん必要になるでしょう。

0:54:58.170,0:55:03.540
基本的には計算可能な問題の解は見つかるというものです。

0:55:04.690,0:55:06.690
多くの初心者は

0:55:08.559,0:55:15.819
深層学習のようなものは他にないのですかといった疑問や、

0:55:16.519,0:55:18.519
どうやってニューラルネットを作るのですか？と聞きますが

0:55:19.210,0:55:21.210
これがニューラルネットです。どうやって

0:55:22.720,0:55:24.730
SGDで学習するのですか？

0:55:25.520,0:55:27.520
学習を高速化する方法は

0:55:27.619,0:55:33.549
ありますか？またより少ないパラメータで行う方法はありますか？などなど。

0:55:34.400,0:55:36.400
しかしこれらはどれも

0:55:36.830,0:55:38.830


0:55:40.090,0:55:41.600
パフォーマンス改善です。

0:55:41.600,0:55:43.600


0:55:43.730,0:55:45.969
これが

0:55:47.180,0:55:50.680
ニューラルネットの学習についての理解です。

0:55:53.960,0:55:56.229
コードをもう少し簡潔にしましょう。

0:55:56.990,0:56:00.550
すでにnn.Linearを使って、weightとbiasを

0:56:01.400,0:56:03.400
置き換えられることは知っています。

0:56:04.730,0:56:11.200
両方とも置き換えましょう。そして、この関数では

0:56:13.619,0:56:18.199
関数の結果を次の関数に渡すということを繰り返し、

0:56:19.579,0:56:23.089
最後の関数の結果を返り値にしています。

0:56:23.459,0:56:26.929
これは関数合成というものです。

0:56:27.599,0:56:32.149


0:56:32.729,0:56:38.539
ニューラルネットは基本的には線形関数と

0:56:39.209,0:56:41.209
非線形関数の関数合成で

0:56:41.489,0:56:44.659
できています。

0:56:45.269,0:56:49.218
PyTorchは関数合成の機能を提供しています。

0:56:49.529,0:56:53.958
nn.Sequentialです。ここでは、最初のlinearレイヤーの

0:56:54.089,0:56:58.129
結果をReLUに渡し、その結果をまた次のlinearレイヤーに渡しています。

0:56:58.650,0:57:02.150
ここではF.reluではなくnn.ReLUを使っています。

0:57:02.150,0:57:05.719
これは機能としては全く同じですが、nn.ReLUはクラスです。

0:57:06.989,0:57:09.468
質問ですか、Rachel？

0:57:12.089,0:57:14.089
非線形関数を使うことで、

0:57:14.940,0:57:22.220
負の値が全て0になる結果、多くの勾配が0になり、学習を止めてしまうのではないでしょうか？

0:57:22.220,0:57:24.220


0:57:25.950,0:57:30.319
素晴らしい質問ですね、そして答えはYesです。

0:57:32.470,0:57:34.750
しかし、全ての画像に対して勾配を0にするわけではありません。

0:57:35.480,0:57:37.839
remember the mini-batches a shuffled so
ミニバッチはシャッフルされているので

0:57:38.359,0:57:43.029
あるミニバッチでは勾配がすべて0かもしれませんが次のミニバッチでは違います。

0:57:43.030,0:57:45.759
そして次のエポックではそもそもそういうバッチはないかもしれません。

0:57:47.359,0:57:50.889
確かに勾配0は起こりますが、

0:57:52.270,0:57:54.270
それはニューラルネットが多くの入力に対して

0:57:55.070,0:58:02.799
0を返すようなパラメータになってしまうときです。ミニバッチが全て0になってしまうこともあり得ますし、

0:58:05.600,0:58:10.929
いくつかのニューロンが不活性、つまり、

0:58:12.780,0:58:17.370
0になってしまうこともあり得ます。これは非常に大きな問題です。

0:58:18.160,0:58:20.160


0:58:21.090,0:58:23.579
計算資源の無駄遣いになっているわけです。

0:58:23.890,0:58:30.089
いくつかこれを避けるトリックがあり今後勉強しますが、1つは

0:58:31.600,0:58:35.969
この部分を平らにするのではなく、少し傾きをゆるやかにすることです。

0:58:36.490,0:58:42.150
Leaky ReLUというものです。

0:58:42.700,0:58:44.700


0:58:44.830,0:58:49.709
もう一つは、2つの重要な初期値をを大きすぎたり小さすぎたりする値に

0:58:50.770,0:58:56.159
しないことです。そして、パラメータ更新の際もその幅をあまり大きい値にしないことで

0:58:56.980,0:59:02.429
ほとんどの値が正になります。

0:59:02.800,0:59:05.640
しかし、私たちはこれから

0:59:05.950,0:59:11.909
ニューラルネットワークの内部でどれくらいのユニットが不活性かを分析する方法も勉強します。

0:59:11.980,0:59:16.590
不活性なユニットは仕事をしておらず、その状態は

0:59:18.240,0:59:23.639
十分多くの入力が0になれば続くので避けるべきです。

0:59:29.760,0:59:32.510
さて、ニューラルネットを用意しました。

0:59:33.300,0:59:37.910
先ほどと同じLearnerを使いますが、今回はsimple_netを渡します。

0:59:38.400,0:59:42.079
その他全ては同じで、

0:59:42.780,0:59:45.440
fitが使えます。

0:59:46.320,0:59:50.720
モデルが深くなるにつれて。1層から始めました。

0:59:51.290,0:59:57.590
私はパラメタライズされたレイヤーを数えますので、これは3層とも言えますが、私は2層と言います。

0:59:58.140,1:00:04.309
訓練する層が2つあります。1層のときから、学習率を小さくしました。1から0.1にしました。

1:00:04.890,1:00:06.890
なぜならモデルが深くなれば

1:00:07.080,1:00:12.769
損失は凸凹、つまり挙動がわかりにくくなるので、学習率を小さくします。

1:00:12.770,1:00:14.770
そしてこれをしばらく訓練します。

1:00:15.270,1:00:17.270


1:00:19.099,1:00:26.238
学習の経過はlearnerの内部を見ることでわかります。recorderアトリビュートです。

1:00:26.400,1:00:28.400
それは

1:00:28.799,1:00:30.799
この表に出てきた数字を全て記録しています。

1:00:31.349,1:00:36.469
基本的にはトレーニングロス、バリデーションロス、そしてaccuracyなどのメトリクスの3つを記録します。

1:00:37.499,1:00:39.529
記録された値は

1:00:40.349,1:00:45.709
結果の表のようなもので、各行の2番目は

1:00:46.410,1:00:49.309
accuracyです。

1:00:50.680,1:00:52.680
そして、

1:00:52.940,1:00:57.460
ここで使っているLクラスと

1:00:58.070,1:01:02.350
優れたメソッドであるitemgot(2)は

1:01:03.350,1:01:08.650
各行の2番目の要素を集めます。そしてこの

1:01:10.830,1:01:14.400
accuracyのグラフがかけて、最後のaccuracyは

1:01:16.660,1:01:20.020
テーブルの最終行の2番目の要素を見ればわかります。

1:01:20.900,1:01:26.410
98.3%ですね。悪くありません。

1:01:27.440,1:01:29.440


1:01:29.809,1:01:35.379
私たちはいま、どんな問題も任意の精度で解くことができる関数を手に入れました。

1:01:36.020,1:01:41.530
その条件は良いパラメータを見つけることができることです。そして、

1:01:42.200,1:01:46.389
どんな関数に対しても相当良いパラメータを見つける方法があります。

1:01:48.440,1:01:51.399
魔法みたいですね。

1:01:54.330,1:01:58.999
このネットワークが学習している内容を理解する方法はありますか？

1:01:59.310,1:02:01.699
ZeilerとFergusがやったように。

1:02:04.290,1:02:06.290
あとで見ます。

1:02:07.020,1:02:09.020
彼らの論文の詳細にはありませんが、

1:02:09.900,1:02:11.900
基本的には、

1:02:11.910,1:02:16.129
parameters()で実際の値を確認できるのです、

1:02:16.890,1:02:18.420
そして、

1:02:18.420,1:02:23.360
自分でやってみることをお勧めします。実際に学習済みのパラメータが

1:02:25.380,1:02:31.460
あります。もしモデルを使いたければlearn.modelで取れます。

1:02:34.410,1:02:42.170
モデルの中身はlearn.modelで実際に訓練したモデルを確認できます。

1:02:45.430,1:02:49.270
このモデルは3つの要素で構成されています。Linear, ReLU, Linearです。

1:02:49.970,1:02:53.889
私はこのモデルを扱いやすくするために変数に代入するのがすきです。

1:02:56.240,1:02:58.240


1:02:58.779,1:03:00.938
そしてインデクシングで1層だけ使うことができます。

1:03:03.440,1:03:05.440
parameters()でパラメータも確認できます。

1:03:07.589,1:03:08.589


1:03:08.589,1:03:10.619
generatorオブジェクトを返しますね。

1:03:10.619,1:03:16.318
これはパラメータのリストのようなものです。w, b = とすれば

1:03:17.079,1:03:20.008
展開できます。重みのshapeは

1:03:24.569,1:03:25.950


1:03:25.950,1:03:28.220
30 x 784ですね。

1:03:29.460,1:03:34.159
私がそうしたからです。ここで注意すべきは

1:03:35.069,1:03:37.050


1:03:37.050,1:03:39.050
2層以上の

1:03:39.599,1:03:42.139
ニューラルネットを作るために、

1:03:42.869,1:03:48.229
1つではなく30の出力を指定してより多くの特徴量を生成していることです。

1:03:48.230,1:03:50.000
もし特徴量の生成というのを

1:03:50.000,1:03:57.020
30個の線形モデルとみなせば、この30個のモデルを1つにまとめているのです。

1:03:58.230,1:04:00.230
みなさんは

1:04:00.930,1:04:04.099
それらの1つを

1:04:06.880,1:04:12.740
つまりここでは1行目を見れば、これを変形して、

1:04:17.920,1:04:25.470
画像の元々の形にしてしまえば、可視化できます。

1:04:31.040,1:04:38.029
これを見てわかることは、

1:04:43.720,1:04:48.359
このモデルは画像の上部中部下部で何かを見つけようと学習しようとしていることがわかります。

1:04:49.710,1:04:51.710
同様にして2番目についても確認できます。

1:04:53.570,1:04:55.570
何がなんだかわかりません。

1:04:57.450,1:05:02.760
これらの一部はそれほど明らかではありませんが、

1:05:03.339,1:05:06.539
お、これは先ほどのものに似ていますね。

1:05:09.349,1:05:11.349
なんだか真ん中を見ているようです。

1:05:12.779,1:05:14.779
これが基本的なアイデアです。

1:05:15.180,1:05:21.829
もし第二層以降を理解したいのならばこの方法はより洗練されるべきです。

1:05:23.130,1:05:26.450
しかし、最初の層はただプロットすればいけます。

1:05:31.040,1:05:34.810
今から、fastaiを

1:05:35.990,1:05:38.469
有効活用したコードを見ましょう。

1:05:38.470,1:05:45.399
DataLoaders.from_folderを使ってDataLoadersを作り、CNN LearnerをResNetで作り、

1:05:46.550,1:05:48.700
1エポックだけ学習します。

1:05:49.640,1:05:55.329
99.7！さっきは40エポックでも

1:05:56.240,1:06:00.909
98.3%でした。使えるトリックを全部使えば

1:06:01.640,1:06:06.549
高速になるし性能も上がります。このコースが終わる頃には、

1:06:08.420,1:06:16.180
遅くとも2つのコースを終える頃にはみなさんはスクラッチで1エポックでこの99.7%に

1:06:16.850,1:06:18.850
到達できると思います。

1:06:19.810,1:06:21.810
さげ、

1:06:23.279,1:06:25.279


1:06:26.970,1:06:31.519
専門用語の確認です。ReLUは負の値を0にする関数、

1:06:32.339,1:06:37.429
ミニバッチは入力とラベルをいくつかまとめたもので

1:06:38.279,1:06:39.989
ランダムに選ばれることもあります。

1:06:39.989,1:06:42.949
forward pass（順伝播）は予測を計算することです。

1:06:43.140,1:06:46.999
損失は、私たちが導関数を計算したい関数です。

1:06:47.309,1:06:51.919
勾配は損失のパラメータに関する勾配のことです。

1:06:52.769,1:06:55.728
backward pass（逆伝播）は勾配を計算することです。

1:06:57.059,1:07:03.738
勾配降下は勾配を使ってパラメータを更新することです。

1:07:03.930,1:07:07.519
学習率はパラメータ更新の幅です。


1:07:08.750,1:07:10.750


1:07:14.420,1:07:16.420
その他に覚えておくべきことは

1:07:16.980,1:07:22.639
最も大事な2つの専門用語ですが、ニューラルネット内部の数字で、

1:07:23.070,1:07:26.119
私たちが学習しているのはパラメータと呼ばれることと、

1:07:27.150,1:07:34.730
計算している数字は、例えばReLUであったり行列積で求められる数字ですが、

1:07:35.060,1:07:36.810
それらはactivationと呼ばれます。

1:07:36.810,1:07:39.139
activationとパラメータが

1:07:39.750,1:07:45.770
ニューラルネットででてくる数字の全てです。今後、私がこの2つの単語、

1:07:46.290,1:07:48.290
activationとパラメータ

1:07:48.420,1:07:55.550
を使う場合は何を意味しているかしっかり理解してください。なぜならこの2つがニューラルネット内部の

1:07:55.770,1:07:58.939
全ての数字です。activationは計算されるもの、

1:07:59.640,1:08:01.640
パラメータは学習されるものです。

1:08:02.910,1:08:08.240
これらの計算はtensorを使います。

1:08:08.970,1:08:14.629
ランク0 tensorはスカラー、ランク1 tensorはベクトル、ランク2 tensorは

1:08:14.630,1:08:20.989
行列とも言います。rank3からはrank3 tensor、rank4 tensorというふうに呼びます。

1:08:21.420,1:08:27.739
深層学習ではrank5 tensorがよく使われますので数字が大きくなってもビビらないでください。

1:08:29.430,1:08:33.470
では、休憩に入りましょうか？、質問ですか？

1:08:34.680,1:08:39.379
活性化関数はたくさんありますが、何を使うべきかの経験則はありますか？

1:08:40.560,1:08:47.779
はい、確かにたくさんありますが、それほど大きな差はありません。ReLUや

1:08:48.509,1:08:50.719
Leaky ReLU、

1:08:51.420,1:08:53.930
他にもいろいろありますが

1:08:54.509,1:08:59.719
どれを使ってもうまくいくはずです。細かい違いは今後見ていきますが、

1:09:00.720,1:09:04.699
基本的にはどれか1つ使うと問題が起きるというよりは

1:09:04.830,1:09:10.459
ものによっては時間がかかったり、少しだけより正確になったり、速くなったり、

1:09:10.460,1:09:12.460
という感じです。

1:09:13.039,1:09:19.789
素晴らしい質問でした。多くのコンセプトを学んだので次に進む前に

1:09:20.579,1:09:27.379
この章の質問に答えられるようにしてください。アンケートに答えて、

1:09:28.289,1:09:32.869
このnotebookを見返して、実行してみることで理解を確かめてください。

1:09:33.630,1:09:35.630
よし、それでは

1:09:35.759,1:09:39.348
7分の休憩です。

1:09:44.980,1:09:46.980 =======================================================================================================================================
Okay, welcome back

1:09:48.460,1:09:52.319
So now that we know how to create and train an ear on it

1:09:53.589,1:10:00.448
Let's cycle back and look deeper at some applications. And so we're going to try to kind of

1:10:01.900,1:10:07.440
Interpolate in from one end we've done they're kind of from scratch version at the other end

1:10:07.440,1:10:11.730
We've done the kind of four lines of code version and we're going to gradually nibble at each end

1:10:12.159,1:10:16.919
Until we find ourselves in the middle and we've we've we've touched on all of it

1:10:17.860,1:10:21.449
so let's go back up to the kind of the four lines of code version and and

1:10:22.210,1:10:24.210
Delve a little deeper

1:10:24.670,1:10:26.670
So

1:10:27.940,1:10:29.940
Let's go back to pets

1:10:30.530,1:10:33.260
And let's think though about like

1:10:34.110,1:10:35.820
How do you actually?

1:10:35.820,1:10:39.380
it'll start with a new data set and

1:10:40.350,1:10:42.350
Figure out how to use it

1:10:44.610,1:10:45.750
So it you know

1:10:45.750,1:10:51.899
The the data sets we provide it's easy enough to untie them you to say untie that or download it and hire it

1:10:53.619,1:10:58.979
If it's a data set that you're getting you can just use the terminal or I

1:10:59.770,1:11:01.679
Throw a Python or whatever

1:11:01.679,1:11:02.230
um

1:11:02.230,1:11:09.689
So let's assume we have a path that's pointing at something so initially you don't you don't know what that something is

1:11:11.409,1:11:15.689
So we can start by doing LS to have a look and see what's inside there

1:11:15.690,1:11:18.869
So the pets data set that we saw in Lesson one

1:11:20.170,1:11:23.580
contains three things annotations images and models and

1:11:24.130,1:11:26.130
You'll see we have this little trick here

1:11:26.290,1:11:29.219
where we say path based path equals and

1:11:29.350,1:11:34.589
Then the path to our data and that just does a little simple thing. Where when we print it out

1:11:34.630,1:11:40.020
It just doesn't show us. It just shows us relative to this path, which is a bit convenient

1:11:41.680,1:11:43.680
So

1:11:44.670,1:11:49.379
Go and have a look at the readme for the original pets data set

1:11:49.540,1:11:55.379
It tells you what these images and annotations folders are and not surprisingly the images path

1:11:55.660,1:12:02.399
So if we go path slash images, that's how we use path Lib to grab her sub directory and then LS

1:12:03.010,1:12:07.350
We can see here are the names that the paths through the images?

1:12:09.510,1:12:15.140
As it mentions here most functions and methods in Farsi I which returned a collection don't return a

1:12:15.930,1:12:20.449
Python list that they returned a capital L and a capital L

1:12:20.850,1:12:27.049
As we briefly mentioned is basically an enhanced list. One of the enhancements is the way it prints the

1:12:28.020,1:12:32.629
Representation of it starts by showing you. How many items there are in the list in the collection?

1:12:32.630,1:12:35.299
so there's seven thousand three hundred and ninety four images and

1:12:36.989,1:12:44.869
It it if there's more than ten things it truncates it and just says dot dot to avoid filling up your screen

1:12:47.070,1:12:49.130
So there's a couple of little conveniences there

1:12:50.370,1:12:55.339
And so we can see from this output that a file name

1:12:56.130,1:12:58.130
as we mentioned in

1:12:58.440,1:13:03.109
lesson 1 if the first letter is a capital it means it's a cat and

1:13:03.750,1:13:05.750
If the first letter is lowercase

1:13:06.150,1:13:07.860
It means it's a dog

1:13:07.860,1:13:11.270
But this time we've got to do something a bit more complex a lot more complex

1:13:11.270,1:13:14.779
Which is figure out what breed it is and so you can see the breed

1:13:15.570,1:13:19.250
Is kind of everything up to after the in the file name?

1:13:19.400,1:13:24.080
It's everything up to the the last underscore and before this number is the breed

1:13:25.369,1:13:27.369
so

1:13:27.530,1:13:32.559
We want to label everything with its breed, so we're going to take advantage of this structure

1:13:35.690,1:13:37.690
So

1:13:38.810,1:13:45.319
Their way I would do this is to use a regular expression a regular expression is something that looks at a string and

1:13:46.050,1:13:51.889
Basically lets you kind of pull it apart into its pieces in very flexible way. It is kind of simple little

1:13:52.470,1:13:57.470
Language for doing that. Um, if you haven't used regular expressions before um,

1:13:57.470,1:14:03.019
Please Google regular expression tutorial now and look it's going to be like one of the most useful tools

1:14:03.020,1:14:05.629
You'll come across in your life. I use them almost every day

1:14:06.570,1:14:08.220
I

1:14:08.220,1:14:13.550
Will go to details about how to use them since there's so many great tutorials. And there's also a lot of great like

1:14:14.130,1:14:21.200
Exercises, you know, there's regex regex is short for regular expression. There's regex crosswords. There's rich xqa is

1:14:21.870,1:14:26.059
All kinds of core regex things a lot of people like me love this tool

1:14:27.000,1:14:29.000
in order to

1:14:29.010,1:14:35.119
There's also a regex lesson in the fast AI and LP course, maybe even to regex lessons. Oh, yeah

1:14:35.730,1:14:38.600
I'm sorry for forgetting about the first day. I know because

1:14:39.930,1:14:41.930
What an excellent resource that is?

1:14:46.740,1:14:47.970
So

1:14:47.970,1:14:52.740
Regular expressions are how to get right the first time. So the best thing to do is to get a sample string

1:14:52.740,1:14:58.440
So good - good way to do that would be to just grab one of the file names. So let's pop it in F name

1:14:59.140,1:15:01.590
and then you can experiment with

1:15:04.150,1:15:09.869
Vector expressions. So re is the regular expression module in Python and

1:15:10.510,1:15:12.510
Find all we'll just grab

1:15:12.820,1:15:16.349
All the parts of a regular expression that have parentheses around them

1:15:16.660,1:15:20.639
So this regular expression and R is a special kind of string in Python

1:15:20.950,1:15:28.019
Which basically says don't treat backslash is special because normally in Python like backslash n means a newline

1:15:29.080,1:15:31.230
So here's us a string

1:15:32.140,1:15:34.140
Which I'm going to capture

1:15:35.100,1:15:36.910
Any letter

1:15:36.910,1:15:38.910
one or more times

1:15:38.950,1:15:40.950
followed by an underscore

1:15:41.050,1:15:43.320
followed by a digit one or more times

1:15:45.830,1:15:52.899
Followed by anything I published have use backslash dot for this phone followed by the letters jpg followed by the end of the string

1:15:54.320,1:16:00.340
and so if I call that vector expression against my bio names name Oh

1:16:01.670,1:16:03.790
Looks good, right so we kind of check it out

1:16:05.210,1:16:07.389
So now that seems to work we can create a data block

1:16:08.570,1:16:10.280
where the

1:16:10.280,1:16:17.020
Independent variables are images the dependent variables are categories just like before get items is going to be get image files

1:16:18.500,1:16:20.679
We're going to spit it randomly as per usual

1:16:22.880,1:16:29.049
And then we're going to get the label by calling regex labeler, which is a

1:16:30.950,1:16:32.950
Just a handy little

1:16:33.140,1:16:36.669
faster class which labels things with a regular expression

1:16:37.640,1:16:43.660
We can't call the regular expression this particular regular expression directly on the path lib path object

1:16:43.970,1:16:47.259
we actually want to call it on the name attribute and

1:16:47.660,1:16:54.639
fast AI has a nice little function chord using Etra using attribute which takes this function and

1:16:55.400,1:17:02.830
Changes it to a function which will be passed this attribute that's going to be using regex labeler on the name attribute

1:17:06.719,1:17:10.459
And then from that data block we can create the data loaders as usual

1:17:11.280,1:17:13.820
There's two interesting lines here

1:17:14.940,1:17:16.800
resize

1:17:16.800,1:17:23.599
and/or transforms log transforms we have seen before in notebook 2

1:17:26.880,1:17:28.949
In the section core data augmentation

1:17:30.300,1:17:33.559
and so ork transforms was the thing which

1:17:34.170,1:17:41.119
can zoom in and zoom out and warp and rotate and change contrast and change brightness and so forth and flip

1:17:41.789,1:17:48.589
To kind of give us almost. It's like giving us more data being generated synthetically from the data. We already have

1:17:54.430,1:17:56.650
And we also learned about random resize crop

1:17:57.710,1:18:00.490
Which is a kind of a really cool way of

1:18:01.160,1:18:02.810
getting

1:18:02.810,1:18:06.729
ensuring you get square images at the same time that you're

1:18:08.150,1:18:10.150
augmenting the data

1:18:11.030,1:18:12.200
Here

1:18:12.200,1:18:15.670
We have a resize to a really large image

1:18:15.950,1:18:20.260
but you know by deep learning standards for 60 by for 60 is a really large image and

1:18:21.080,1:18:22.910
Then we're using

1:18:22.910,1:18:28.329
Oak transforms with a size. So that's actually going to use random resize crop to a smaller size

1:18:29.270,1:18:31.270
Why are we doing that?

1:18:35.850,1:18:42.680
This particular combination of two steps does something which I think is unique to fast AI which we call pre sizing and

1:18:43.170,1:18:46.190
The best way is I will show you this beautiful

1:18:48.150,1:18:50.150
Example of

1:18:51.729,1:18:53.729
Wizardry that I'm so excited about

1:18:54.039,1:18:56.039
to show how pre sizing works

1:18:56.039,1:19:03.418
What pre sizing does is that first step where we say resize to 460 by 460 is it grabs a square?

1:19:04.730,1:19:10.970
And it grabs it randomly if it's a kind of landscape orientation photo, it'll grab it randomly

1:19:10.970,1:19:14.749
So it'll take the whole height and randomly grab somewhere from along the side

1:19:15.450,1:19:19.249
If it's a portrait orientation, then it or grab it

1:19:19.250,1:19:23.750
You know take the full wits and grab a grant and grab a random bit from top to bottom

1:19:25.050,1:19:27.050
So then we take this area here and here

1:19:27.390,1:19:28.730
It is right

1:19:28.730,1:19:35.569
And so that's what the first resize does and then the second org transforms bit will grab a random

1:19:36.540,1:19:37.920
wat

1:19:37.920,1:19:39.920
crop

1:19:40.000,1:19:45.669
Be rotated from in here and we'll turn that into a square and

1:19:47.060,1:19:48.620
so it does

1:19:48.620,1:19:52.329
So there's two steps its first of all resize to a square. That's big and

1:19:52.700,1:19:57.369
then the second step is to a kind of rotation and warping and

1:19:57.980,1:20:00.730
zooming stage to something smaller

1:20:01.430,1:20:03.430
in this case 224 by 224

1:20:04.940,1:20:07.330
Because his first step creates something that's Square

1:20:08.450,1:20:15.789
And always is the same size the second step can happen on the GPU and because normally things like rotating an image whopping

1:20:16.400,1:20:18.400
Actually pretty slow

1:20:18.890,1:20:24.519
Also normally doing a zoom and rotate and a warp

1:20:26.000,1:20:31.479
Actually is really destructive to the image because each one of those things requires an interpolation step

1:20:31.520,1:20:35.589
Which it's not just slow it actually makes the image really

1:20:36.200,1:20:38.200
quite low quality

1:20:38.270,1:20:41.589
So we do it in a very special way in faster. I think it's unique

1:20:42.440,1:20:49.960
Where we do all of the all of these kind of coordinate transforms like rotations and warps and zooms and so forth

1:20:52.490,1:20:56.349
Not on the actual pixels, but instead we kind of keep track of the

1:20:57.530,1:21:03.970
Changing coordinate values in a in a non lossy way so the full floating-point value and then once at the very end

1:21:04.190,1:21:06.190
We then do the interpolation

1:21:08.390,1:21:10.390
There is also quite striking

1:21:11.940,1:21:13.940
Here is what the difference looks like

1:21:15.360,1:21:18.079
Hopefully you can see this on on the video

1:21:18.600,1:21:22.220
on the left is our pre sizing approach and

1:21:22.590,1:21:30.529
On the right is the standard approach that other libraries use and you can see that the one on the right is a lot less

1:21:31.620,1:21:36.439
Nicely focused and it also has like weird things like this should be grass here

1:21:36.440,1:21:38.540
But it's actually got its kind of bum sticking way out

1:21:39.390,1:21:43.430
This has a little bit of weird distortions. This has got loads of weird distortions

1:21:44.190,1:21:48.740
So you can see the pre sized version really ends up. I'm way way better and

1:21:49.710,1:21:51.710
I think we have a question Rachel

1:21:53.370,1:21:55.880
Are the blocks in the data block an ordered list

1:21:56.550,1:22:03.050
They specify the input and output structures respectively. Are there always two blocks or can there be more than two?

1:22:03.420,1:22:08.389
For example, if you wanted a segmentation model with the second block be something about segmentation

1:22:12.050,1:22:14.470
So so, yeah, this is an ordered list

1:22:15.770,1:22:18.609
So the first item says I want to create an image

1:22:19.130,1:22:24.160
And then the second item says I want to create a category. So that's my independent and dependent variable

1:22:24.830,1:22:29.799
You can't have one thing here. You can have three things here. You can have any amount of things here you want

1:22:30.560,1:22:35.169
Obviously the vast majority of the time it'll be two only there's an independent variable and a dependent variable

1:22:35.930,1:22:37.989
We'll be seeing this in more detail later

1:22:38.270,1:22:44.379
Although if you go back to the earlier lesson when we introduced data blocks, I do have a picture kind of showing how these pieces

1:22:44.930,1:22:46.930
Get together

1:22:53.429,1:22:59.089
So after you've put together datablock created your data loaders you want to make sure it's working correctly

1:22:59.700,1:23:04.369
so the obvious thing to do for a computer vision data block is show batch and

1:23:05.730,1:23:07.910
Show batch will show you

1:23:10.829,1:23:12.829
The items and you can kind of just make sure they look

1:23:13.360,1:23:15.539
Sensible that looks like the labels are reasonable

1:23:15.539,1:23:21.959
If you add a unique equals true, then it's going to show you the same image with all the different augmentations

1:23:21.960,1:23:23.999
This is a good way to make sure your augmentations work

1:23:24.519,1:23:28.379
If you make a mistake in your data block in this example, there's no

1:23:28.900,1:23:30.989
resize so that different

1:23:31.510,1:23:35.219
Images are going to be different sizes or be impossible to collate them

1:23:36.039,1:23:40.139
Into a batch, so if you call dot summary

1:23:40.929,1:23:42.849
This is a really neat thing

1:23:42.849,1:23:49.469
Which we'll go through and tell you everything that's happening though. I collecting the items. How many did I find?

1:23:49.780,1:23:51.780
What happened when I split them?

1:23:51.909,1:23:53.909
What are the different?

1:23:54.039,1:23:58.649
Variables independent independent variables on creating let's try and create one of these

1:24:00.070,1:24:02.070
peers each step

1:24:02.289,1:24:04.289
Create my image

1:24:04.599,1:24:08.909
Categorize is what the first thing gave me an American Bulldog

1:24:09.400,1:24:14.309
is the final sample is this image this size this category and

1:24:15.699,1:24:23.428
Then eventually it says oh, oh it's not possible to collect your items. I tried to collect the zero index members of your tuples

1:24:23.429,1:24:31.109
So in other words, that's the independent variable and I got this was size 500 by 375. This was 375 by 500

1:24:31.110,1:24:34.380
Oh, I can't collate these into a tensor because they're different sizes

1:24:34.690,1:24:40.230
So this is a super great debugging tool for debugging your data blocks ever question

1:24:42.309,1:24:44.788
How does the item transforms

1:24:45.280,1:24:48.809
Precise work if the resize is smaller than the image is

1:24:48.940,1:24:54.359
A whole width or height still taken or is it just a random crop with the revised value?

1:24:55.329,1:24:57.389
so if you remember back to

1:24:59.739,1:25:01.739
To

1:25:03.150,1:25:05.299
We looked at the different ways of

1:25:08.420,1:25:10.420
Creating these things you can use squish

1:25:12.230,1:25:14.230
Can use pad

1:25:15.409,1:25:17.409
Or you can use crop

1:25:17.909,1:25:22.129
so if your image is smaller than the precise value then

1:25:22.949,1:25:28.819
Squish will really be zoom so it will just small stretch. It'll stretch it and then

1:25:30.329,1:25:35.449
Pattern crop will do much the same thing. And so you'll just end up with a you're not the same

1:25:36.119,1:25:42.708
This looks like these but it'll be a kind of lower more pixelated lower resolution because it's having to zoom in a little bit

1:25:48.010,1:25:53.159
Okay, so a lot of people say that you should do a hell of a lot of data cleaning before you model

1:25:53.920,1:25:56.850
We don't we say model as soon as you can

1:25:57.760,1:25:58.930
because

1:25:58.930,1:26:05.639
Remember what we found in in notebook - your your model can teach you about the problems in?

1:26:06.520,1:26:07.990
your data

1:26:07.990,1:26:10.649
so as soon as I've got to a point where I have a data block

1:26:11.560,1:26:14.370
That's working and I have data loaders. I'm going to build a model

1:26:15.450,1:26:21.809
And so here I'm you know, it also tells me how I'm going. So I'm getting 7% era. Wow, that's actually really good or

1:26:22.330,1:26:26.850
A pets model and so at this point now that I have a model I can do that stuff

1:26:26.850,1:26:32.850
We learned about earlier in our to the notebook or to where we trained our model and use it to clean the data

1:26:33.190,1:26:35.729
So we can look at the classification a confusion matrix

1:26:36.790,1:26:38.710
top losses

1:26:38.710,1:26:42.000
The image cleaner widget, you know so forth

1:26:47.180,1:26:53.769
Okay, now one thing interesting here is in book 4 we

1:26:55.190,1:26:59.919
Included a loss function when we created a loner and here we don't pass in our loss function

1:27:01.050,1:27:05.070
Why is that and that's because plus say I will try to

1:27:06.160,1:27:08.819
Automatically pick a somewhat sensible loss function for you

1:27:11.420,1:27:18.530
And so for a image classification task it knows what loss function is the normal one to pick and it's done it for you

1:27:19.170,1:27:21.800
but let's have a look and see what actually

1:27:22.560,1:27:24.060
did pick

1:27:24.060,1:27:26.060
So we could have a look

1:27:33.039,1:27:37.858
At learned loss funk and we will see it is cross-entropy loss

1:27:38.530,1:27:43.109
Why don't know this' cross-entropy loss i'm glad you asked let's find out

1:27:44.760,1:27:49.160
Cross entropy loss is really much the same as the

1:27:49.920,1:27:54.710
Amnesty lost we created with that with that sigmoid and the one minus

1:27:55.650,1:27:57.650
predictions and predictions

1:27:58.290,1:28:01.640
But it's it's a kind of extended version of that

1:28:02.670,1:28:05.239
now and the extended version of that is that

1:28:05.880,1:28:12.049
that torch dot where that we looked at in notebook for only works when you have um a

1:28:12.600,1:28:15.890
Binary outcome in that case it was is it a three or not?

1:28:16.800,1:28:18.800
but in this case we've got

1:28:19.410,1:28:21.649
Which of the thirty-seven pet breeds is it?

1:28:22.770,1:28:24.770
So we want to kind of create

1:28:25.800,1:28:32.330
something just like that sigmoid and torch don't wear that which also works nicely for

1:28:33.510,1:28:35.510
more than two

1:28:35.610,1:28:37.610
categories

1:28:37.920,1:28:43.790
So, let's see how we can do that, so first of all, let's grab a batch yes a question

1:28:48.860,1:28:54.850
Why do we want to build a model before cleaning the data I would think a clean dataset would help in training

1:28:57.050,1:29:04.210
Yeah, absolutely a current clean dataset helps in training but remember as we saw in notebook o2

1:29:05.690,1:29:11.529
An initial model helps you clean the data set. So remember how plot top losses helped us identify

1:29:12.320,1:29:14.680
mislabeled images and the

1:29:15.380,1:29:17.380
confusion matrix helps us recognize

1:29:17.810,1:29:21.310
which things we were getting confused and might need you know fixing and

1:29:21.830,1:29:24.219
the image classifier cleaner actually

1:29:24.219,1:29:29.709
Let us find things like an image that contain two bears rather than one bear and clean it up

1:29:30.020,1:29:33.129
so a model is just a fantastic way to

1:29:33.620,1:29:40.569
Help you zoom in on the data that matters which things into have the problems which things are most important

1:29:41.239,1:29:43.569
Stuff like that so you would go through and you clean it

1:29:44.210,1:29:49.029
With the model helping you and then you go back and train it again with the clean data

1:29:50.300,1:29:52.300
Thanks. Let go question

1:29:55.040,1:29:57.040
Okay, so

1:29:57.440,1:30:03.549
In order to understand cross-entropy loss. Let's grab a batch of data which we can use deals one batch

1:30:06.890,1:30:14.300
And that's going to grab a batch from the training set we could also go first DL strain

1:30:16.820,1:30:18.820
And that's going to do exactly the same thing

1:30:21.300,1:30:26.280
And so then we destructor that into the independent independent variable and so the dependent variable

1:30:27.100,1:30:31.379
Shows us we've got a batch size of 64. So it shows us the 64

1:30:32.380,1:30:34.380
categories

1:30:39.960,1:30:41.410
And

1:30:41.410,1:30:47.669
Remember those numbers simply refer to the index of into the vocab. So for example 16

1:30:49.879,1:30:51.629
Is a boxer and

1:30:51.629,1:30:56.569
So that all happens for you automatically when we say show batch it shows us those strings

1:30:57.749,1:31:00.409
So is a first mini batch

1:31:02.219,1:31:06.959
So now we can view the predictions that is the activations of the final layer of the network

1:31:08.110,1:31:10.110
by calling get prints and

1:31:10.329,1:31:13.799
You can pass in a data loader

1:31:14.650,1:31:17.429
And a DOTA loader can really be anything that's going to return

1:31:19.210,1:31:26.909
A sequence of mini batches so we can just pass in a list containing our mini batch as a data loader

1:31:26.909,1:31:30.239
And so that's going to get the predictions for one mini batch

1:31:30.820,1:31:32.820
So here's some predictions

1:31:33.880,1:31:37.480
Okay, so the actual predictions if we go

1:31:38.600,1:31:45.729
Preds zero-sum to grab the predictions for the first image and add them all up. They add up to one and

1:31:46.730,1:31:48.650
there are

1:31:48.650,1:31:55.389
37 of them so that makes sense. Right? It's like the very first thing is what is the probability that that is a

1:31:56.780,1:31:58.780
Else. Oh cab

1:32:00.380,1:32:03.139
The first thing is what's the probability it's an Abyssinian cat

1:32:03.929,1:32:05.929
It's ten to the negative six

1:32:06.179,1:32:08.419
You see and so forth

1:32:08.420,1:32:14.029
So it's basically like it's not this it's not this it's not this and you can look through and oh here this one here

1:32:14.150,1:32:16.150
You know don't see what other things it is

1:32:18.150,1:32:19.290
So

1:32:19.290,1:32:20.880
How did it?

1:32:20.880,1:32:26.120
you know, so we we obviously want the probabilities to sum to one because it would be pretty weird if

1:32:26.850,1:32:28.580
if they didn't it would say

1:32:28.580,1:32:34.999
You know that the the probability of being one of these things is more than 1 or less than 1 which would be extremely odd

1:32:37.590,1:32:39.090
So

1:32:39.090,1:32:41.480
How do we go about creating these?

1:32:42.179,1:32:46.219
Predictions where each one is between zero and one and they all add up to 1

1:32:47.159,1:32:49.159
To do that we use something called softmax

1:32:51.770,1:32:56.529
Softmax is basically an extension of sigmoid the handle more than

1:32:57.290,1:33:02.080
Two levels two categories. So remember the sigmoid function looked like this

1:33:04.719,1:33:06.908
Use that for our 3s vs. 7s model

1:33:08.489,1:33:10.210
So what if we want

1:33:10.210,1:33:11.560
37

1:33:11.560,1:33:16.859
Categories rather than two categories. We need one activation for every category

1:33:18.379,1:33:19.769
So actually

1:33:19.769,1:33:21.769
the threes and sevens model

1:33:22.649,1:33:24.889
Rather than thinking of that as an is-3

1:33:25.649,1:33:28.399
Model we could actually say oh that has two categories

1:33:28.800,1:33:36.709
so let's actually create two activations one representing how three like something is and one representing how seven like something is

1:33:38.070,1:33:40.999
so let's say you know, let's just

1:33:42.809,1:33:44.809
Say that we have

1:33:45.389,1:33:47.389
6m nest digits and

1:33:48.719,1:33:50.719
These were the

1:33:52.710,1:33:57.750
Can I do this and this first column were the activations of

1:33:59.470,1:34:01.390
My model

1:34:01.390,1:34:03.390
for for one

1:34:03.910,1:34:06.809
Activation and the second column was for a second activation

1:34:06.810,1:34:11.700
So my final layer actually has two activations now. So this is like how much like a3 is it?

1:34:11.700,1:34:15.209
And this is how much like a7 is it? But this one is not at all

1:34:15.790,1:34:18.479
Like a3 and it's slightly not like a seven

1:34:19.150,1:34:20.110
this is

1:34:20.110,1:34:23.610
Very much like a three and not much like a seven and so forth

1:34:23.680,1:34:26.459
So we can take that model and rather having rather than having one

1:34:26.710,1:34:32.939
Activation food-like is three we can have two activations for how much like a three how much like a seven?

1:34:34.000,1:34:36.899
So if we take the sigmoid of that

1:34:37.900,1:34:39.960
We get two numbers between Norton one

1:34:40.600,1:34:42.840
But they don't add up

1:34:43.840,1:34:50.789
To one so that doesn't make any sense. It can't be 0.66 chance. It's a three and point five six chance

1:34:50.790,1:34:54.330
It's a seven because every digit in that data set is only one or the other

1:34:55.360,1:34:57.360
So that's not going to work

1:34:58.180,1:35:03.990
but what we could do is we could take the difference between this value and this value and

1:35:04.570,1:35:07.409
Say that's how likely it is to be a three

1:35:08.110,1:35:13.169
So in other words this one here with a high number here and a low number here is very likely to be a three

1:35:15.620,1:35:18.220
So we could basically say in the binary case

1:35:19.070,1:35:25.659
These activations that what really matters is their relative confidence of being a three versus a seven

1:35:26.330,1:35:34.090
So we could create collect the difference between column one and column two and column index zero and column index one, right? And here's the

1:35:34.760,1:35:39.579
Difference between the two columns. There's that big difference and we could take the sigmoid of that

1:35:42.140,1:35:43.320
Okay, and

1:35:43.320,1:35:45.380
so this is now giving us a

1:35:45.990,1:35:48.920
single number between Norton one and

1:35:49.470,1:35:51.470
So then since we wanted two columns

1:35:52.200,1:35:53.670
We could make

1:35:53.670,1:36:00.889
column index zero the sigmoid and column index one could be one minus that and now look these all

1:36:02.190,1:36:04.120
Add up to one

1:36:04.120,1:36:11.370
So here's probability of three probability of seven, but the second one probably three properly seven and so forth

1:36:13.150,1:36:16.650
So like that's a way that we could go from

1:36:18.250,1:36:20.250
Having

1:36:20.430,1:36:23.330
Activations for every image

1:36:24.180,1:36:31.760
- creating two probabilities each of which is between Norton one and each pair of which adds to one

1:36:33.910,1:36:37.709
Great how do we extend that to more than two columns?

1:36:39.130,1:36:42.719
how to extend it to more than two columns we use this function

1:36:43.540,1:36:45.540
Which is called soft max

1:36:46.180,1:36:49.889
Those soft max is equal to e to the X

1:36:52.300,1:36:54.460
By some of a to the X

1:36:58.110,1:37:03.319
Just to show you if I go softmax on my activations I get

1:37:04.739,1:37:07.069
0.6 o2 five point three nine seventy five

1:37:07.979,1:37:11.119
Point six zero two five point three nine seven five I get exactly the same thing

1:37:12.360,1:37:16.489
Right. So softmax in the binary case is

1:37:17.519,1:37:18.780
identical to

1:37:18.780,1:37:20.840
The sigmoid that we just looked at

1:37:24.030,1:37:26.030
But in the multi category case

1:37:26.429,1:37:31.519
We basically end up with something like this. Let's say we were doing the teddy bear grizzly bear brown bear and

1:37:32.760,1:37:39.230
For that remember our neural net is going to have the final layer will have three activations. So let's say it was 0.02

1:37:39.870,1:37:41.730
negative 2.49

1:37:41.730,1:37:46.069
1.25. So to calculate softmax. I first go e to the power of

1:37:46.679,1:37:47.810
each of these three things

1:37:47.810,1:37:50.209
so here's E to the power of point O two e

1:37:50.370,1:37:52.819
to the power of negative two point four nine e

1:37:52.920,1:37:56.089
To the power of three point 4 e to the power of one point two five

1:37:56.760,1:37:58.380
Ok, then I add them up

1:37:58.380,1:38:05.659
so there's the sum of the X and then softmax will simply be one point O two divided by four point six and

1:38:06.179,1:38:11.749
Then this one will be 0.08 divided by four point six. And this one will be three point four nine divided by four point six

1:38:12.360,1:38:19.159
So since each one of these represents each number divided by the sum, that means that the total is one

1:38:20.659,1:38:22.340
okay, and

1:38:22.340,1:38:27.369
Because all of these are positive and each one is an item divided by the sum

1:38:27.369,1:38:29.499
It means all of these must be between naught and one

1:38:30.079,1:38:32.079
So this shows you that

1:38:32.300,1:38:33.800
softmax

1:38:33.800,1:38:37.540
Always gives you numbers between naught and 1 and they always add up to 1

1:38:40.380,1:38:43.290
That in practice you can just call torch dot softmax

1:38:45.039,1:38:47.039
We'll give you this result of this

1:38:47.989,1:38:49.989
dysfunction

1:38:50.289,1:38:52.289
So you should experiment with

1:38:52.479,1:38:57.929
This in your own time, you know write this out by hand and try putting in

1:38:58.719,1:39:00.070
these numbers

1:39:00.070,1:39:03.179
Right and and see how that you get back the numbers

1:39:03.179,1:39:05.609
I claim you're going to get back and make sure this makes sense to you

1:39:06.729,1:39:09.479
so one of the interesting points about softmax is

1:39:11.380,1:39:15.479
Remember I told you that X is e to the power of something and

1:39:15.999,1:39:20.159
Now what that means? Is that e to the power of something

1:39:20.800,1:39:25.439
Grows, very very fast. Right? So like

1:39:28.949,1:39:30.949
Exper for

1:39:31.550,1:39:33.550
Is 54

1:39:34.630,1:39:36.949
XP of eight

1:39:38.920,1:39:42.730
Is 29 2009 80 right it grows super fast and

1:39:43.640,1:39:48.999
What that means is that if you have one activation, it's just a bit bigger than the others

1:39:49.400,1:39:52.449
It's softmax will be a lot bigger than the others

1:39:53.120,1:39:55.419
So intuitively the softmax function

1:39:56.719,1:39:58.719
Really wants to pick one class

1:39:59.390,1:40:01.070
among the others

1:40:01.070,1:40:04.449
Which is generally what you want right when you're trying to train a classifier

1:40:05.510,1:40:11.949
To say which breed is it you kind of want it to to pick one and kind of go for it, right?

1:40:11.949,1:40:13.949
And so that's what softmax

1:40:14.960,1:40:16.190
does

1:40:16.190,1:40:18.190
That's not what you always want

1:40:18.190,1:40:21.429
So sometimes an inference time you want it to be a bit cautious

1:40:21.500,1:40:26.109
And so you kind of got to remember that softmax isn't always the perfect approach

1:40:26.810,1:40:31.959
But it's the default. It's what we use most of the time and it works well on a lot of situations

1:40:34.500,1:40:36.500
So that is

1:40:36.930,1:40:38.400
softmax

1:40:38.400,1:40:42.319
now in the binary case for the amnesty three versus sevens

1:40:42.420,1:40:50.210
This was how we calculated amnesty law suite oook the sigmoid and then we did either one minus that or that as our loss function

1:40:51.710,1:40:56.359
Um just fine as you saw it it worked, right?

1:40:59.720,1:41:01.720
Could do this

1:41:01.980,1:41:08.209
Exactly the same thing. We can't use torch dot wear anymore because targets aren't just zero or one targets could be any number from naught

1:41:08.210,1:41:09.780
to 36

1:41:09.780,1:41:11.780
So we could do that by

1:41:12.660,1:41:14.780
replacing the torch dot wear with

1:41:15.510,1:41:21.139
Indexing. So here's an example for the binary case. Let's say these are our targets

1:41:22.260,1:41:24.289
0 1 0 1 1 0 and

1:41:24.750,1:41:30.500
These are our softmax activation x' which we calculated before notice from some random numbers just for a toy example

1:41:32.430,1:41:35.119
So one way to do instead of doing torch dot where

1:41:36.540,1:41:38.540
We could instead

1:41:38.910,1:41:39.800
Have a look at this

1:41:39.800,1:41:46.790
I could say I could grab all the numbers from naught to 5 and if I index into here

1:41:47.190,1:41:51.739
With all the numbers from 0 to 5 and then my targets

1:41:54.579,1:41:59.939
0 1 0 1 0 1 1 0 then what that's going to do is it's going to pick a

1:42:00.489,1:42:02.669
row 0 it'll pick

1:42:03.579,1:42:05.669
0.6. And then for Row 1

1:42:06.309,1:42:08.309
It'll pick one

1:42:08.440,1:42:12.449
0.49 a row 2. It'll pick 0

1:42:13.119,1:42:15.269
point 1 3 0 4

1:42:16.030,1:42:17.530
It'll pick

1:42:17.530,1:42:22.559
1.00 3 and so forth. So this is a super nifty

1:42:23.500,1:42:25.360
indexing expression

1:42:25.360,1:42:27.159
which

1:42:27.159,1:42:33.539
You should definitely play with right and it's basically this trick of passing multiple things to the PI torch

1:42:34.360,1:42:40.679
Indexer, the first thing says which rows should you return and the second thing says for each of those rows?

1:42:41.079,1:42:44.819
Which column should you return? So this is returning all the rows and

1:42:46.420,1:42:51.540
These columns for each one and so this is actually identical

1:42:53.560,1:42:55.290
Don't wear

1:42:55.290,1:43:03.060
Or isn't that tricky? And so the nice thing is we can now use that for more than just two values

1:43:04.929,1:43:11.228
And so here's here's the fully worked out thing, so I've got my three is column I've got my sevens column

1:43:11.229,1:43:18.999
Here's that target is the indexes from naught one, two, three, four five. And so here's zero zero point six one one

1:43:19.610,1:43:22.029
five point nine zero two point

1:43:22.579,1:43:24.579
one three and so forth

1:43:27.670,1:43:30.489
So yeah this works just as well with more than two columns

1:43:31.010,1:43:33.010
So we can add

1:43:33.230,1:43:37.419
You know for doing a full amnesty, you know, so all the digits from naught to nine

1:43:37.429,1:43:41.319
We could have ten columns and we would just be indexing into the ten

1:43:45.800,1:43:48.650
So this thing we're doing where we're going -

1:43:49.710,1:43:51.710
our activations matrix

1:43:52.140,1:43:55.640
all of the numbers from naught to N and then our targets

1:43:56.580,1:44:03.440
Is exactly the same as something that already exists in PI torch called F dot ll us as you can see

1:44:03.870,1:44:09.050
Exactly the same at so again, we're kind of seeing that these things inside PI torch and fast AI

1:44:10.200,1:44:12.800
Are just little shortcuts for stuff we can write ourselves

1:44:14.430,1:44:15.970
And ll los

1:44:15.970,1:44:22.679
Stands for negative log likelihood again, it sounds complex, but actually it's just this indexing expression

1:44:25.000,1:44:30.600
Rather confusingly, there's no log in it. We'll see why in a moment

1:44:33.429,1:44:35.429
So let's talk about logs

1:44:36.380,1:44:37.880
So

1:44:37.880,1:44:39.880
This locks this loss function

1:44:40.100,1:44:43.870
works quite well as we as we saw in the notebook o4

1:44:43.870,1:44:49.240
It's basically this it is exactly the same as we learn no Paco for just a different way of expressing it

1:44:50.390,1:44:52.390
But we can actually make it better

1:44:52.520,1:44:56.290
because remember the probabilities we're looking at are between

1:44:56.960,1:45:00.340
Norton one so they can't be smaller than zero. They can't be greater than one

1:45:00.920,1:45:06.760
Which means that if our model is trying to decide whether to protect point nine nine zero point nine nine nine

1:45:07.430,1:45:11.889
It's going to think that those numbers are very very close together, but won't really care

1:45:13.299,1:45:15.169
But actually if you think about the error

1:45:15.169,1:45:18.909
You know if there's like a hundred thing a thousand things

1:45:19.519,1:45:22.869
Then this would like be ten things are wrong

1:45:22.869,1:45:28.568
And this would be like one thing is wrong. But this is really like ten times better than this

1:45:29.869,1:45:31.039
so

1:45:31.039,1:45:38.588
Really? What we'd like to do is to transform the numbers between zero and one two instead but three be between negative infinity and infinity

1:45:38.629,1:45:39.919
and

1:45:39.919,1:45:42.669
There's a function that does exactly that which is called

1:45:43.669,1:45:45.669
logarithm

1:45:46.930,1:45:52.769
Okay, so as the so the numbers we could have can be between zero and one

1:45:53.620,1:45:56.279
and as we get closer and closer to

1:45:58.690,1:45:59.740
Zero

1:45:59.740,1:46:03.809
It goes down to infinity and then at one

1:46:04.330,1:46:07.289
It's going to be zero and we can't go

1:46:07.870,1:46:09.870
above zero because

1:46:10.450,1:46:13.649
Our loss function we want to be negative

1:46:15.770,1:46:16.910
So

1:46:16.910,1:46:20.949
This logarithm in case you forgot hopefully use vaguely

1:46:20.950,1:46:22.730
Remember what logarithm is from high school?

1:46:22.730,1:46:30.489
but that basically the definition is is is this if you have some number that is why that is B to the power of a

1:46:30.980,1:46:33.310
Then logarithm is defined such that

1:46:33.980,1:46:35.390
a

1:46:35.390,1:46:40.629
Equals the logarithm of Y comma B in other words it tells you

1:46:42.770,1:46:46.029
B to the power of what equals y

1:46:48.410,1:46:50.410
Which is

1:46:51.130,1:46:55.900
Not that interesting of itself but one of the really interesting things about logarithms is this

1:46:56.840,1:47:00.190
Very cool relationship, which is that log of a times B

1:47:01.130,1:47:03.130
Equals log of a plus log of B

1:47:04.309,1:47:08.748
And we use that all the time in deep learning and machine learning

1:47:09.479,1:47:11.069
because

1:47:11.069,1:47:16.789
This number here a times B can get very very big or very very small if you multiply things

1:47:16.889,1:47:18.469
a lot of small things together

1:47:18.469,1:47:21.799
You'll get a tiny number if you multiply a lot of big things together

1:47:21.800,1:47:28.969
You'll get a huge number it can get so big or so small that the the kind of the precision in your computer's floating-point

1:47:30.030,1:47:32.030
Gets really bad

1:47:32.340,1:47:39.140
Where else this thing here adding is not going to get out of control. So we really love using logarithms

1:47:40.739,1:47:43.158
Like particularly in a deep neural net where there's lots of layers

1:47:43.350,1:47:48.890
We're kind of multiplying and adding many times though. This kind of tends to come out quite nicely

1:47:51.989,1:47:53.989
So

1:47:54.739,1:47:56.859
When we take thee

1:47:59.090,1:48:02.779
The probabilities that we saw before the

1:48:04.260,1:48:07.520
Deck the things that came out of this function

1:48:09.130,1:48:11.549
And retake their logs

1:48:14.010,1:48:17.909
And we take the main that is called negative log-likelihood

1:48:19.940,1:48:26.210
And so this ends up being kind of a really nicely behaved number because of this property of the log that we described

1:48:27.630,1:48:32.000
So if you take the softmax and then take the log

1:48:33.570,1:48:40.710
Then pass that to an ll loss because remember that didn't actually take the log at all despite the name that gives you

1:48:42.030,1:48:44.030
Russ entropy loss

1:48:45.450,1:48:47.899
So that leaves an obvious question of

1:48:48.900,1:48:50.160
Why?

1:48:50.160,1:48:52.519
doesn't nll loss actually take the log and

1:48:53.310,1:48:56.149
The reason for that is that it's more convenient

1:48:57.000,1:49:02.899
Computationally to actually take the log back at the softmax step. So pi torch has a function called

1:49:07.520,1:49:09.429
Softmax

1:49:09.429,1:49:12.969
So since it's actually easier to do the log at the softmax stage

1:49:12.969,1:49:19.868
It's just a faster and more accurate pay torch assumes that you use soft log max and then pass that to n ll us

1:49:20.749,1:49:26.499
So an L. Our loss does not do the log. It assumes that you've done the log beforehand

1:49:27.139,1:49:32.498
So log softmax followed by n ll loss is the definition of cross-entropy loss in pay torch

1:49:34.170,1:49:40.160
So that's our loss function and so you can pass that some activations and some targets and get back a number and

1:49:41.310,1:49:45.439
Pretty much everything in in pi torch every every one of these kinds of functions

1:49:45.990,1:49:53.059
You can either use the NN version as a class like this and then call that object as if it's a function

1:49:53.430,1:49:55.459
Or you can just use F dot

1:49:56.490,1:50:01.159
With the camelcase name as a function directly and as you can see, they're exactly the same number

1:50:04.530,1:50:06.530
People normally use the class version

1:50:08.640,1:50:14.929
In the documentation in pi torch, you'll see it normally uses a class version so will tend to use the class version as well

1:50:16.770,1:50:19.100
You'll see that it's returning a single number and

1:50:19.320,1:50:25.670
that's because it takes the mean because a loss needs to be as we've discussed the main but if you want to see the underlying

1:50:26.280,1:50:30.800
numbers before taking the mean you can just pass in reduction equals none and

1:50:31.500,1:50:34.699
That shows you the individual cross-entropy losses before

1:50:35.580,1:50:37.580
taking the mean

1:50:41.659,1:50:43.659
Okay

1:50:50.300,1:50:54.880
Great so this is a good place to stop with our

1:50:56.510,1:50:58.510
Discussion of

1:50:58.550,1:51:03.340
Loss functions and such things rich or were there any questions about this?

1:51:11.510,1:51:14.199
Why does the loss function need to be negative

1:51:18.639,1:51:21.029
Well, okay, I mean I guess it doesn't but it's

1:51:22.780,1:51:27.150
We want something that that the lower it is the better

1:51:30.730,1:51:32.730
And we kind of need it to cut off

1:51:33.410,1:51:34.550
somewhere

1:51:34.550,1:51:39.670
I have to think about this more more during the week because I'm it's a bit tough a bit tired

1:51:42.070,1:51:45.519
Yeah, so let me let me refresh my memory when I'm awake

1:51:46.969,1:51:48.969
Okay now

1:51:50.709,1:51:52.709
Next week

1:51:55.170,1:52:02.070
Well, nope not for the video next week actually happened last week so it's the thing I'm about to say is actually your

1:52:05.590,1:52:07.590
So next week we're going to be talking about

1:52:08.590,1:52:12.450
data ethics, and I wanted to kind of segue into that by talking about

1:52:13.450,1:52:15.220
My weeks gone

1:52:15.220,1:52:17.170
because

1:52:17.170,1:52:19.980
a week or two ago in a did a

1:52:20.710,1:52:22.710
as part of a lesson I

1:52:23.140,1:52:26.459
actually talked about the efficacy of

1:52:27.340,1:52:32.430
Masks, I mean specifically wearing masks in public and I pointed out that

1:52:33.430,1:52:38.999
The efficacy of masks seemed like it could be really high and maybe everybody should be wearing them

1:52:40.530,1:52:42.530
And

1:52:43.489,1:52:45.489
Somehow I

1:52:45.969,1:52:50.109
myself as the face of a global advocacy campaign

1:52:51.019,1:52:55.599
and so if you go to masks or

1:52:56.239,1:52:58.239
Osseo

1:52:58.310,1:53:00.310
Ah you will find

1:53:02.219,1:53:03.949
Website

1:53:03.949,1:53:05.949
um

1:53:06.380,1:53:07.639
Talking about

1:53:07.639,1:53:09.999
masks and I've been on

1:53:10.820,1:53:18.070
you know TV shows in South Africa and the US and England and Australia and on radio and

1:53:18.290,1:53:20.290
blah blah blah talking about

1:53:20.780,1:53:22.369
masks

1:53:22.369,1:53:25.569
Why is this? Well, it's because as

1:53:26.389,1:53:28.389
a data scientist

1:53:28.460,1:53:30.460
you know, I

1:53:30.530,1:53:32.530
noticed that the data around masks

1:53:33.619,1:53:39.189
Seem to be getting misunderstood and it seemed that that misunderstanding was costing possibly

1:53:40.330,1:53:42.330
hundreds of thousands of lives

1:53:42.680,1:53:50.530
you know literally in the places that were using masks it seemed to be associated with you know orders of magnitude fewer deaths and

1:53:53.460,1:53:59.850
To talk about next week is like, you know, what's your role is a data scientist and and you know

1:53:59.850,1:54:05.129
I strongly believe that it's to understand the data and then do something about it and

1:54:05.800,1:54:07.800
So nobody was talking about this

1:54:08.950,1:54:10.950
so I

1:54:11.170,1:54:14.069
Ended up writing an article that appeared in The Washington Post

1:54:15.400,1:54:19.290
That basically called on people to really consider

1:54:23.300,1:54:26.900
Asks which is this article and

1:54:29.950,1:54:34.149
You know, I was I was I managed to kind of get a huge team of brilliant

1:54:34.789,1:54:39.669
Not I'm not huge a pretty decent-sized team of brilliant volunteers who helped?

1:54:39.670,1:54:45.069
You know kind of build this website and kind of some PR folks and stuff like that

1:54:46.130,1:54:48.609
But what these came clear was?

1:54:49.980,1:54:57.270
and I was talking to politicians, you know, and it is laughs is what was becoming clear is that

1:54:58.610,1:55:02.929
people weren't convinced by the science, which is fair enough because it's it's hard to

1:55:04.110,1:55:06.739
you know when the whro and the CDC is saying

1:55:07.170,1:55:13.009
You don't need to wear a mask and some random data scientist is saying but doesn't seem to be what the data is showing

1:55:13.830,1:55:18.859
You know, you've got half a brain you would pick the whu-oh in the CDC not the random data scientist

1:55:19.470,1:55:23.720
So I really felt like I if I was going to be an effective advocate I needed

1:55:24.510,1:55:26.510
sort the science out and it

1:55:26.910,1:55:29.300
You know credentialism is strong

1:55:29.910,1:55:33.290
And so it wouldn't be enough for me to say it. I needed to find other people to say it

1:55:33.510,1:55:35.510
so I put together a team of

1:55:37.920,1:55:39.120
19

1:55:39.120,1:55:40.740
scientists

1:55:40.740,1:55:43.490
Including you know a professor of sociology

1:55:44.460,1:55:47.000
a professor of aerosol dynamics

1:55:48.660,1:55:54.619
The founder of an African movement that's that kind of studied preventive methods for methods for tuberculosis

1:55:57.480,1:56:00.020
a Stanford professor who studies

1:56:01.620,1:56:03.120
Mask

1:56:03.120,1:56:05.120
disposal and cleaning methods

1:56:05.460,1:56:08.270
see a bunch of Chinese scientists who study

1:56:09.450,1:56:11.280
epidemiology modeling

1:56:11.280,1:56:13.280
a UCLA professor

1:56:13.920,1:56:15.920
who is

1:56:15.990,1:56:17.990
one of the top

1:56:18.540,1:56:21.139
Infectious disease epidemiologist experts

1:56:21.930,1:56:26.510
and so forth so like this kind of all-star team of people from all around the world and

1:56:27.330,1:56:31.339
I had never met any of these people before so well, I don't know not quite sure

1:56:31.340,1:56:34.400
I knew Austin a little bit and I knew they nip a little bit

1:56:35.700,1:56:37.700
And helix a little bit

1:56:39.520,1:56:43.090
But on the whole you know, and well Reshma, we all know she's awesome

1:56:43.090,1:56:47.529
So it's great to actually have a pastor a community person there too. And

1:56:48.650,1:56:51.369
So but yeah, I kind of tried to pull together

1:56:52.640,1:56:58.780
people from you know, there's many geographies as possible and as many areas of expertise as possible and

1:57:00.800,1:57:04.480
You know the kind of the global community helped me find

1:57:06.740,1:57:14.650
Papers about about everything about you know how different materials work about how droplets form about

1:57:16.540,1:57:18.540
Epidemiology about

1:57:20.170,1:57:21.980
Case studies of

1:57:21.980,1:57:24.939
people infecting with and without masks blah blah blah and

1:57:25.760,1:57:29.770
We ended up in the last week. Basically we wrote this paper

1:57:30.610,1:57:32.610
It contains 84

1:57:33.199,1:57:34.900
citations

1:57:34.900,1:57:36.900
um and

1:57:37.910,1:57:41.500
You know, we basically worked around the clock on it as a team and

1:57:43.310,1:57:51.100
It's out and it's been sent to a number of some of the earlier versions three or four days ago we sent to some

1:57:52.460,1:57:56.560
Governments, so one of the things is in this team. I try to look for people who

1:57:57.560,1:58:04.120
Well what you know working closely with government leaders, not just that they're scientists. And so this this went out to a number of

1:58:04.790,1:58:06.790
government ministers and

1:58:06.860,1:58:10.509
in the last few days, I've heard that it was a

1:58:11.270,1:58:13.270
very significant part of

1:58:14.180,1:58:16.900
decisions by governments to change their

1:58:18.620,1:58:21.399
To change their guidelines around masks

1:58:22.610,1:58:28.929
And you know the fights not over by any means and in particular the UK is a bit of a holdout

1:58:29.720,1:58:31.720
But I'm going to be on

1:58:32.240,1:58:35.469
ITV tomorrow and then BBC the next day

1:58:37.070,1:58:41.920
You know, it's it's kind of required stepping out to be a lot more than just a data scientist so I've had to

1:58:42.620,1:58:44.620
Pull together, you know

1:58:44.720,1:58:47.980
Politicians and staffers I've had to you know

1:58:49.580,1:58:55.059
You know hassle with the media to try and get you know coverage and you know today I'm now

1:58:55.460,1:58:59.529
Starting to do a lot of work with unions to try to get unions to understand this, you know

1:58:59.530,1:59:02.530
It's really a case of like saying okay as a data scientist

1:59:03.380,1:59:05.380
and income in conjunction with

1:59:05.900,1:59:07.900
real scientists

1:59:08.390,1:59:11.079
We've built this really strong understanding that

1:59:12.650,1:59:16.120
Masks, you know this simple, but incredibly powerful tool

1:59:17.140,1:59:20.590
That doesn't do anything unless I can effectively communicate this to

1:59:21.320,1:59:24.789
Decision-makers. So today I was you know on the phone to

1:59:25.610,1:59:28.779
You know one of the top union leaders in the country

1:59:30.410,1:59:33.999
Explaining what this means basically it turns out that in buses

1:59:34.640,1:59:39.519
In America, the kind of the air conditioning is set up so that it blows from the back to the front

1:59:40.370,1:59:43.300
and there's actually case studies in the medical literature of how

1:59:44.210,1:59:48.220
People that are seated and of downwind of an air conditioning unit

1:59:48.920,1:59:56.649
In a restaurant ended up all getting sick with Co at 19 and so we can see why like bus drivers are dying

1:59:59.300,2:00:06.079
Because they're like, they're right in the wrong spot here and their passengers aren't wearing masks so I could have unexplained this science

2:00:08.490,2:00:10.490
To union leaders

2:00:10.680,2:00:13.309
So that they understand that to keep the workers safe

2:00:13.740,2:00:20.150
It's not enough just for the driver to wear a mask. But all the people on the bus needed to be wearing masks as well

2:00:21.419,2:00:23.419
so, you know all this is basically to say

2:00:26.480,2:00:31.120
You know as data scientists, I think we have a responsibility to

2:00:33.620,2:00:37.160
The study the data and then do something about it, it's not just a

2:00:38.460,2:00:42.260
Research, you know exercise it's not just a computation exercise

2:00:42.270,2:00:46.669
You know, what? What's the point of doing things if it doesn't lead to anything?

2:00:48.270,2:00:50.270
So

2:00:52.599,2:00:54.469
Um, yeah, so

2:00:54.469,2:00:58.509
Next week. We'll be talking about this a lot more but I think you know

2:00:58.510,2:01:02.409
this is a really to me kind of interesting example of how

2:01:05.419,2:01:11.349
Digging into the data can lead to really amazing things happening. And and in this case I

2:01:12.409,2:01:17.078
strongly believe and a lot of people are telling me they strongly believe that this kind of

2:01:17.479,2:01:25.269
Advocacy work that's come out of this data analysis is is already saving lives. And so I hope this might help inspire you

2:01:25.969,2:01:30.549
To to take your data analysis and to take it to places that it really makes a difference

2:01:31.429,2:01:33.969
So thank you very much, and I'll see you next week
