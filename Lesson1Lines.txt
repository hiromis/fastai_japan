
　
























 




























　
これは猫の線画ではなく 猫の写真を認識できるものです 
これからのコースでわかるように、この種のモデルは、あなたが与えた情報からのみ学習することができます。
これまでのところ、私たちは猫の写真しか与えていません。
アニメの猫でもなく、描かれた猫でもなく、抽象的な猫の表現でもなく、ただの写真です。
では次に実際に何が起きたのか？というのをみていきます。
ご覧の通り、今のところ、私はここにいい情報を得ていません。
もしこういう出力をノートに見たら、 File → Trust Notebook → Trustという操作をして下さい。
これはジュピターに表示に必要なコードの実行を許可すると伝えているのです。
セキュリティ上の問題がないと。
これで出力が表示されます。
時々、このような奇妙なコードを見ることがあります。
これは以下の図を作成するコードです。
このようなコードは基本的に隠します。
しかし時には見せるかもしれません。
しかし，一般にこのようなコードは無視して、出力されている図に集中してください。
ですので，毎回こういったコードを紹介するつもりはありません。
代わりにスライドを見てみましょう。
ここでやっていることは、機械学習です。
ディープラーニングは機械学習の一種です。
機械学習とは、プログラミングと同じで、コンピュータに何かを処理させる方法です。
しかし、写真に映っているのが猫か犬かを識別するプログラムを書くのは想像しにくいでしょう。
写真の中の犬と猫を認識するプログラムを作るために、ループや変数の代入、条件式をどう使えば良いのでしょうか？これは大変難しいです。
本当に難しいです。
ディープラーニングの時代までは、一見簡単そうな作業（犬猫の識別）を正確にこなすモデルは誰も持っていませんでした。
必要なステップを書き切れないからです。
通常は、いくつかの入力を処理し，いくつかの結果を返す関数を書きます。
ですので，人間がステップを書き下す方式のプログラムは写真の認識などが難しいのです。
1949年に、アーサー・サミュエルという人が、猫や犬の認識といった問題をプログラムに解かせる方法を見つけようとし始めました。
1962年にその方法を説明しました。
まず彼は問題に関して次のように述べました。
この種の計算のためのプログラムを書くのは困難な作業である。
なぜなら、処理に含まれる微細なステップをうんざりするほど詳細に綴らなければならないのだから。
コーダーなら誰もが知っているように，コンピュータは巨大なマヌケである。
そこで彼は、コンピュータに正確な手順を教えるのではなく、問題の例を与えて、その問題を解く方法をコンピュータに考えさせよう、と言いました。
そして、1961年までに、彼はチェッカープログラムを作成し、 コネチカット州のチャンピオンを倒しました。
「実際のパフォーマンスから重み（パラメータ）の有効性を自動でテストする手段とパフォーマンスを最大化するように重み（パラメータ）を変更するメカニズムを構築する」。
この文章が肝心です。
そして、かなりトリッキーな文章なので、時間をかけても大丈夫です。
プログラムへの入力があって、プログラムが何かを出力するという基本的な考え方は同じです。
ここではプログラムではなくモデルと呼びましょう。
モデルへの入力と結果。
そして、重みと呼ばれる2つ目のものがあります。
基本的な考え方としては、このモデルは、入力，例えばチェッカーボードの状態だけでなく、モデルがどのように動作するかを記述する重みあるいはパラメータに基づいて出力を作成するものです。
チェッカーの作戦を列挙し、パラメータを使って，サミュエルは重みと呼びますが，それぞれを説明することができて，現在の重みの割り当てが、実際のパフォーマンスの面でどれだけ効果的かを検証する方法があれば、言い換えれば、チェッカーをプレイするための特定の戦略が、ゲームの勝ち負けにつながるかどうか、そして、パフォーマンスを最大化するように重み
の割り当てを変更する方法があるとします。
そうすると、重みを一つずつ増やしたり減らしたりしてみて、少しでも良いチェッカーの戦略があるかどうかを調べるというのを何回も何回もやってみると、最終的にはそのような手順が完全に自動化されて、機械がその経験から学習するようにプログラムされます。
これが機械学習で、処理を与えられるのではなく、処理を学習するのです。
（図を指しながら）もしそのようなものがあったとしたら、我々は基本的に今、次のようなものを持っているでしょう：あなたは、入力と重みがモデルの入力となり結果，つまりチェッカーでの勝敗，を出力し，これらのパフォーマンスを評価します。
この評価が重要なステップです。
2つ目の重要なステップは、測定されたパフォーマンスに基づいて重みを更新する方法で、この一貫の処理を繰り返し行うこともあります。
このプロセスが機械学習モデルの訓練（学習）です。
これは抽象的なアイデアです。
しばらく実行した後、重みがかなり良い感じになりました。
ここで，どう訓練したかを無視すれば，機械学習モデルは従来のプログラムと同様なものになります。
ここでは「プログラム」ではなく「モデル」という単語が使われています。
訓練されたモデルは他のコンピュータプログラムと同じように使うことができます。
つまり、我々はコンピュータプログラムを，タスクを実行するための必要なステップを設書き下すのではなく、タスクを実行するよう学習させることによってプログラムを構築していて、訓練の後には別のプログラムになります。
このプログラムは推論と呼ばれます。
推論とは、この訓練済みのモデルを使ってチェッカーなどのタスクを解かせることです。
つまり，機械学習とは，処理を書き下すのではなく，コンピュータが経験から学習させるプログラムの訓練です。
これを画像認識のためにどうするかというと、モデルと重みは何かというと、それらを変化させていくと、猫と犬の認識がどんどん良くなっていきます。
相手の駒が自分の駒からどれくらい離れているか」に応じて、その状況で何をすべきかを列挙することは想像に難くありません。
どのように防御的な戦略と攻撃的な戦略を比較するべきか、などなど。
画像認識のためにどうするのか、全く明らかではありません。
私たちが本当に必要としているのはモデルに含まれる非常に柔軟な関数と、それを如何様にも機能させる重みです。
本物の...世界で最も柔軟性のある関数のような...そんなものがあることがわかりました それはニューラルネットワークです。
今後の授業で，その正確な数式を説明していきます。
しかし，ニューラルネットワークを使うためには、その数学的な関数が何であるかは、実際には重要ではありません。
これは、我々が言うところの「パラメータ化」された関数であり、重みを変えれば、異なるタスクを実行し、実際にはあらゆるタスクを実行するできます：普遍性定理と呼ばれるものは、この関数の形は、正しい重みを見つければ，任意の精度であらゆる問題を解けると数学的に証明しています。
これは以前にミンスキー（マービン・ミンスキー）問題の対処方法に関して述べたことを言い換えてるようなものです。
ニューラルネットワークは非常に柔軟性があるので，正しい重みを見つけることができれば 「これは猫か犬か」などの問題に答えられます。
つまり、（モデルの）訓練に力を注ぐ必要があります。
良い重み，これはサミュエルの言い方ですが，を見つけて、それらの適切な配分を見つけることです。
どうすれば良いのでしょうか？非常に汎用的な方法を求めています。
例えば、猫と犬の認識がどれくらい優れているかというような、パフォーマンスに基づいて重みを更新することです。
そして幸運なことに、そのような方法が存在することが判明しました！その方法とは、確率的勾配降下法あるいはSGDと呼ばれています。
今後，実際にSGDがどのように動作するのかを確認し、スクラッチで作成しますが、今のところは心配する必要はありません。
これだけは言っておきますが、SGD もニューラルネットも数学的には全く複雑ではありません。
ほとんど足し算と掛け算だけです。
トリックは、足し算と掛け算を、数十億，いやそれより多く、私たちが直感的に把握できるより多く行っていることです。
彼らは極めて強力ですが，ロケットのように複雑ではありませんし，それらがどのように動くのかをしっかり確認します。
ここまでがアーサー・サミュエル版です。
今日では、用語は異なりますが、全く同じ考えを使っています。
真ん中にあるのがアーキテクチャで重みを調整することで何かしらのタスクを適切に処理するようになる関数です。
それがアーキテクチャであり、モデルの関数形です。
時々、モデルはアーキテクチャを意味すると言われることがありますが、あまり混乱させないようにしてください。
しかし、本当に正しい言葉はアーキテクチャーです。
私たちはそれらを「重み」とは呼ばず、「パラメータ」と呼んでいます。
重みには特定の意味があり、非常に特殊な種類のパラメータです。
モデルから出てくるもの、つまりパラメータを持つアーキテクチャの出力を、私たちは予測と呼んでいます。
予測は2種類の入力に基づいています：データである独立変数、猫や犬の写真のようなもの、そしてラベルとして知られている従属変数、これは「これは猫です」「これは犬です」といったもので、これが入力です。
つまり、結果は予測です。
アーサー・サミュエルの言葉を借りれば、パフォーマンスの尺度は、損失と呼ばれています。
損失は予測値のラベルから 計算されます そして、パラメータの更新があります  さて、これは先ほどの図と同じですが、今日使う言葉を入れてみました。
もし忘れてしまった場合は、この図を見て、これがモデルを作成するためにこのアーキテクチャで使用されているパラメータだと言ったら、もう一度戻って、パラメータの意味を思い出してみてください。
パラメータとは何か？予測値とは？損失とは何か？ パラメータを更新できるような方法でモデルのパフォーマンスを測定する関数の損失です。
ここで注意しなければならないのは、ディープラーニングや機械学習は魔法ではないということですね。
モデルは、学習しようとしていることの例を示すデータがあるところでしか作成できません。
それは訓練に使われた入力の中で見たパターンでしか動作を学習できないんですよね？ だから、猫や犬の画像がないと、アーキテクチャを作るパラメータの更新がないので、アーキテクチャとパラメータを合わせたものがモデルなんです。
つまり、モデルと言えば、モデルが猫や犬の線画を予測するのが上手になるということです。
また、この学習アプローチは予測値を作成するだけであることにも注意してください。
それに対して何をすべきかは教えてくれません。
これは、「誰かにどの製品を勧めるか」というようなレコメンデーションシステムのようなものを考えるときに非常に重要になるでしょう。
私たちはそんなことしませんよね？ 私たちが見せた製品について誰かが何を言うか予測することはできますが、私たちは行動を生み出しているわけではありません。
予測をしているのです。
これは非常に重要な違いです。
犬や猫の絵などの入力データの例があるだけでは不十分なのです。
ラベルなしでは何もできません。
企業がよく言うのは 「データが足りない」と言うことがよくあります。
ほとんどの場合は、「ラベル付きのデータが足りない」という意味です。
ほとんどの場合、「ラベル付けされたデータが十分ではない」という意味です。
なぜなら、企業がディープラーニングを使って何かをしようとしている場合、多くの場合、すでに行っていることを自動化したり、改善したりしようとしているからです。
つまり、理論的には、そのデータを持っているか、またはそのデータを取得する方法を持っていることを意味します。
彼らはそれをやっているからだ。
しかし、しばしば厄介なのは、それにラベルを付けることです。
例えば医療における放射線学のモデルを作成するとします。
思いつく限りのあらゆる医療画像を入手できることはほぼ間違いないでしょう。
しかし、それらを腫瘍の悪性度や髄膜腫の有無などでラベル付けするのは非常に難しいかもしれません。
このような違いは、戦略に大きな影響を与える重要なものです。
モデルとは、PDPの本で見たように、モデルは環境の中で動作します。
あなたはそれを展開して、何かをするのです。
そして、PDPのフレームワークのこのような部分が非常に重要なのです。
そうですよね？実際に何かをしているモデルがあります。
例えば、どこで逮捕されるかを予測する（行動を推奨するわけではありません）警察のような予測モデルを構築したとします。
これはアメリカの多くの管轄区域で使われているものです。
今では、データに基づいて、ラベル付けされたデータに基づいて、それを予測しています。
この場合、実際には（アメリカの）データを使っています。
黒人か白人かにもよりますが、アメリカの黒人は白人に比べてマリファナ所持で逮捕される頻度が７倍くらい高いと思います。
実際のマリファナ使用量は白人と黒人でほぼ同じですが、偏ったデータを元に取り締まり予測モデルを構築すると、その予測はこのような偏ったデータに基づいて 「ここに逮捕できる人がいるだろう」と 予測します。
そうすると、法執行機関の職員は、モデルが予測した地域に警察活動を集中させることに決めるかもしれません。
その結果、より多くの人を逮捕することになる。
そして、それを使ってモデルに戻す。
するとモデルは黒人居住区ではもっと多くの人を逮捕すべきだと予測するようになります。
これは、モデルが環境と相互作用する中で、どのように正のフィードバックループと呼ばれるものが作られるかの一例です。
モデルの利用増加に伴い、データはより偏ったものになり、モデルはさらに偏ったものになります。
ですから、機械学習で注意しなければならないことの1つは、モデルが実際にどのように使用されているかを認識し、その結果として起こり得るかを認識することです。
これはプロキシの例であるということを伝えたいです。
なぜなら、ここでは逮捕者が犯罪の代わりとして使われているからです。
そして、往々にして，プロキシと実際の値の差は大きいのです。
ありがとう、レイチェル。
本当に重要なポイントですね。
それでは最後に、このコードで何が起こっているのかを見てみましょう。
私たちが実行したコードは、基本的には、1,2,3,4,5,6行のコードです。
コードの最初の行はインポート行です。
Pythonでは、外部ライブラリをインポートしない限り、外部ライブラリを使うことはできません。
通常はライブラリから必要な関数やクラスだけをインポートします。
しかし、Pythonにはモジュールからすべてをインポートできる便利な機能があります。
ほとんどの場合、これは悪い考えです。
なぜなら、デフォルトでは、Pythonの動作方法は、import starを実行した場合、ライブラリの中であなたが使いたい重要なものだけをインポートするのではないからです。
実際には、使用した全てのライブラリや、使用した全てのライブラリがimportしているものもimportするため、結果的に名前空間を爆発させてしまい、多くのバグを引き起こしてしまいます。
fastaiはREPL環境で素早くプロトタイピングができるように設計されているので、私たちはこの問題を避けるために多くの時間を費やしました。
ですから、これをするかしないかはあなた次第です。
しかし、fastaiライブラリからstarをインポートする場合は、実際に必要な部分だけを取得できるように明示的に設計されているので安心してください。
一つだけ言及しておきたいのは、ビデオの中で "fastai2"と呼ばれていることです。
収録時はプレリリース版を使っているからです。
オンライン版、MOOC版を見ている時には、2は取れているでしょう。
他にも言及しておきたいことがあります。
fastaiには4つの定義済みアプリケーション、vision、text、テーブル、協調フィルタリングがあります。
それぞれについて、詳しく学んでいく予定です。
アプリケーションはそれぞれ、例えばここにビジョンがあるとすると、.allからインポートすることができます。
これで、一般的なvisionアプリケーションに必要なほぼ全てがimportされます。
Jupyter NotebookのようなREPLシステムを使っていれば、必要なものはすべてそこにあるので、戻って調べたりする必要はありません。
Pythonユーザーがそれを行っているというのは
問題点です。
untar_dataのようなものを見たとしても、それがどこから来たのかはインポート行を見ればわかるでしょう。
しかしstart import (import *)すると、わからなくなってしまうのです。
REPLでは戻って調べる必要がないということです。
調べたい記号を入力して [SHIFT - ENTER] を押すだけで、それがどこから来たのかを正確に教えてくれます。
この通りです。
大変便利なのです。
この場合、例えば、実際にデータセットを構築するために、ImageDataLoaders.from_name_func()を呼び出しました。
doc関数を使えば簡単にドキュメントを確認できます。
ご覧のように、デフォルト値を含めて全ての引数が確認できます。
そして最も重要なことは、[SHOW IN DOCS]は、exampleを含む完全なドキュメントへのリンクになっています。
fastAIのドキュメントにはすべて例がついています。
さらに、それらは全てJupyter Notebookで書かれているので、自分でコードを実行して、実際の動作や出力など確認できます。
また、ドキュメントにはたくさんのチュートリアルがあります。
例えば、visionのチュートリアルには多くのことが書かれていますが、その1つは、レッスン１の内容をほぼカバーしています。
fastAIにはたくさんのドキュメントがあります。
ドキュメントは完全に検索可能ですし、おそらく最も重要なことは、これらのドキュメントのすべてが完全にインタラクティブなJupyter Notebookになっていることです。
そこで、このコードをさらに見てみると、importの後の最初の行は、untar_dataを呼んでいます。
これはデータセットをダウンロードして、解凍します。
すでにダウンロードされている場合は、再度ダウンロードすることはありませんし，解凍済みなら解凍しません。
ご覧のように、fastAIはすでに多くの有用なデータセットを定義しています。
データセットは深層学習で想像できるように超重要です。
これからたくさんのデータセットを見ていくでしょう。
そして、これらのデータセットは多くのヒーロー（とヒロイン）によって作成され、基本的には数ヶ月から数年かけてデータを照合し、これらのモデルを構築するために使用することができます。
次のステップは、このデータが何であるかをfastAIに伝えることであり、データについて私たちは多くのことを学んでいきます。
しかし、今回は、基本的には「これには画像が含まれている」と言っています。
このパスにある画像が含まれています。
untar_dataは解凍済みの
データのパスを返します。
あるいはすでに解凍されている場合は、以前に解凍された場所を教えてくれます。
そのパスの中に実際にどんな画像があるのかを教えてくれます。
本当に興味深いものに label_func があります。
ファイルごとに、それが猫なのか犬なのかをどうやって判別するのでしょうか。
実際にオリジナルのデータセットのReadMEを見てみると、少し風変わりなものが使われています。
しかし彼らが決めたことです。
そこで、ここにis_catという小さな関数を作って、最初の文字が大文字かどうかを返すようにしました。
これで猫かどうかを判断する方法をfastaiに伝えました。
この2つについては後で説明します。
次に、データが何であるかを伝えます  次にlearnerと呼ばれるものを作らなければなりません。
learnerとは学習するものです。
訓練をします。
だから、どんなデータを使うか教えなければなりません。
そして、どのようなアーキテクチャを使うかを教えなければなりません。
このコースではアーキテクチャについてたくさんお話します。
しかし、基本的には、事前に定義されたニューラルネットワークのアーキテクチャがたくさんあり、それぞれ長所と短所があります。
コンピュータビジョン用のアーキテクチャはResNetと呼ばれています。
これは非常に素晴らしい出発点なので、ここでは小さ目なものを使うことにします。
これらはすべて事前に定義されていて、すぐに使えます。
そして、訓練中にプリントアウトしたいものをfastaiに伝えることができます。
ここでは、「ああ、誤差を教えてください、トレーニング中に」と言っています。
そして、fine_tuneと呼ばれるとても重要なメソッドを呼び出すことができます。
次のレッスンで説明しますが、fine_tuneが訓練を行います。
valid_pctは非常に重要なことをします。
この場合、データの20%は、モデルの学習には使用しません。
その代わりに、モデルの誤差率を知るために使います。
したがって、常にfastaiでは、このメトリック、error_rateは常に訓練されていないデータの一部で計算されます。
この考え方については、今後のレッスンで詳しく説明します。
しかし、ここでの基本的な考え方は、過学習していないことを確認したいということです。
説明しましょう。
過学習は次のようなものです。
例えば、これらの点にフィットする関数を作ろうとしているとしましょう。
素敵な関数はこのようになりますよね？しかし、この関数をはより正確にフィットしています。
見てください、これはこちらよりもはるかにすべての点に近づいています。
ですから、明らかにこちらの方が優れた関数です。
ただし、点がある場所の外に出るとすぐに、特に端から外れてしまうと、明らかに意味がありません。
これが過学習と呼ばれるものです  過学習は様々な理由で起こります。
大きすぎるモデルを使ったり、十分なデータを使っていなかったり...この話はこれからですね。
しかし、ディープラーニングの技術は、適切なフィットを持つモデルを作成することがすべてです。
モデルが適切にフィットしているかどうかを知る唯一の方法は、それを訓練するために使用されていないデータでうまく機能するかどうかを見ることです。
そのため、私たちは常にデータの一部を脇に置いて、バリデーション・セットと呼ばれるものを作成します。
バリデーション・セットとは、モデルを訓練しているときには全く触らないようにしているデータですが、モデルが実際に機能しているかどうかを把握するためだけに使用しています。
シルヴァンが本の中で言っていたことですが、fastaiを勉強していて面白いことの一つは、多くの面白いプログラミングを学ぶことができるということです。
私は子供の頃からプログラミングをしていたので、かれこれ40年くらい経ちます。
シルヴァンと私は二人ともpythonで多くのことを行うために本当に一生懸命働いていて、プログラミングのプラクティスを知っています。
私たちのコードの中では、今まで見たことのないようなことをやっていることがよくあります。
以前のコースを受講した多くの学生は、このコースでコーディングやPythonコーディング、ソフトウェアエンジニアリングについて多くを学んだと言っています。
だから、何か新しいものを見たときはチェックして、なぜそのような方法で行われたのか気になったらフォーラムで気軽に質問してください。
一つ言及しておきたいのは、先ほども述べたように、ほとんどのPythonプログラマーがimport *を行うことはありません。
私たちはそのようなことをたくさんやっています。
私たちは伝統的なPythonプログラミングのアプローチに従わないことをたくさんやっています。
私は長年にわたって多くの言語を使ってきたので、特にPython的な方法ではなく、他の多くの言語や他の多くの表記法からのアイデアを取り入れて、データサイエンスに適したものをベースにPythonプログラミングへのアプローチを大幅にカスタマイズしています。
つまり、あなたがfastaiで見たコードは、あなたの職場でPythonを使っている場合、おそらく、あなたの職場のスタイルガイドや通常のアプローチには合わないでしょう。
ですから、私たちのやり方に従うのではなく、あなたの組織のプログラミングのやり方に合うようにする必要があります。
しかし、おそらくあなた自身の趣味の仕事では、私たちのものに従ってみて、それが面白くて参考になるかどうかを見てみてもいいでしょうし、もしあなたが管理職で、そうすることに興味があるのであれば、あなたの会社でそれを試してみるのもいいでしょう。
さて、最後に、かなり面白いものをお見せしましょう、それは、このコードを見てみてくださいuntar_data、ImageDataLoaders.from_name_funcから、learner、fine_tuneします。
untar_data、SegmentationDataLoaders.from_label_func、label_func、learner、fine_tune。
ほとんど同じコードですが、これは何かをするモデルを構築しました。
これは画像を撮影したものです。
左側がラベル付きのデータです。
色は、それぞれ、車、木、建物、空、線、道路などを示しています。
右側は私たちのモデルですが、私たちのモデルは各ピクセルごとに、車なのか、ラインマークなのか、道路なのかを判別することに成功しました。
たった20秒以内でこれを実現しました。
ですから、非常に小さなモデルです。
いくつかのミスをしています -- この線のマークを見落としていたり、いくつかの車が家だと思っているような？しかし、数分間これを訓練すれば、ほぼ完璧になることがわかります。
基本的な考え方は、ほぼ同じコードを使って、猫や犬を分類するのではなく、セグメンテーションと呼ばれるものを非常に速く作ることができるということです。
見てください、同じものがあります。
Import *、TextDataLoaders.from_folder，learner、fine_tune
基本的なコードは同じです。
これは、文章が表している感情が肯定的か否定的かを理解するできるようになりました。
同じ3行のコードで15分の訓練による93%という正確度は、1000から3000語の何千もの映画レビューからなるIMDBデータセットの分類においては2015年時点で世界最高のものだったと思います。
私たちは基本的に同じコードを使って、私たちのブラウザで、世界クラスのモデルを作成しています。
ここでも同じ基本的な手順です。
from import *、untar_data、TabularDataLoaders.from_csv、Learner、fit
これは今、これらの列を含むcsvに基づいて給与を予測するモデルを構築しています。
これは表形式のデータですね。
ここでは基本的な手順は同じです。
Import *、untar_data、CollabDataLoaders.from_csv、learner，fine_tune
これはユーザーと映画の組み合わせごとに、視聴履歴をもとに，ユーザが映画をどう評価するかを予測するものを構築しています。
これは協調フィルタリングと呼ばれ、レコメンデーションシステムで使用されています。
ここでは、fastaiの4つのアプリケーションの例を見てきました。
このコースを通してお分かりになると思いますが、基本的に同じコードと、同じ数学とソフトウェア工学の基本的な概念によって、同じようなアプローチを使って、大きく異なることができるのです。
なぜかというと、アーサー・サミュエルのおかげです。
モデルをパラメータ化する方法と、重みを更新して損失関数を改善する更新手順があれば、何ができるのかという基本的な説明があるからです。
この場合、ニューラルネットワークを使うことができます。
他のレッスンよりも少し短くなりますが、その理由は、先ほど述べたように、私たちは世界的なパンデミックの始まりの時期にあるからです（少なくとも欧米では（他の国ではもっと進んでいます）。
このことについては、コースの最初の方でお話しましたが、そのビデオは別の場所で見ることができます。
今後のレッスンでは、より深層学習について学ぶことになるでしょう。
次のレッスンに取り組む前に、次の週までに以下に取り組むことをお勧めします。
GPUサーバをスピンアップして、終了したらシャットダウンして、ここにあるすべてのコードを実行できることを確認して、自分が認識できる方法でPythonを使っているかどうかを確認してください。
自分のやり方を知ることができれば、快適に過ごせるようになるでしょう。
この学習スタイル、トップダウン学習で最も重要なことは、実験を行うことであり、コードを実行できるようになる必要があります。
ですから、私のお勧めは、コードを実行できるようになるまでは先に進まないで、本の章を読んで、アンケートに答えてください。
検証セットやテストセット、転送学習については、まだまだやるべきことがあります。
ですから、まだすべてのことができるわけではありませんが、これまでのコースを見てきたことに基づいて、できる部分はすべてやってみてください。
レイチェル、何か追加したいことがあれば教えてください。
それでは、レッスン１に参加してくれてありがとうございました。
次回は転移学習について学び、その後、実際にインターネット上で公開できるアプリの制作に移ります。
バイバイみんなさん。
