0:00:01.089,0:00:03.779
Hi everybody and welcome to lesson 7

0:00:05.589,0:00:09.718
We're going to start by having a look at a kind of regularization called weight decay and

0:00:10.330,0:00:14.580
the issue that we came to at the end of the last lesson is that

0:00:15.760,0:00:20.879
we were training our simple dot product not all with bias and

0:00:21.730,0:00:24.900
Our lost started going down and then instead of going up again

0:00:25.570,0:00:29.340
And so we have a problem that we are

0:00:30.189,0:00:31.599
overfitting and

0:00:31.599,0:00:34.289
Remember in this case. We're using mean squared error. So

0:00:34.960,0:00:38.430
Try to recall why it is that we don't need a metric here

0:00:39.730,0:00:41.690
because

0:00:41.690,0:00:45.349
Main square error is pretty much the thing we care to care about really or we could use

0:00:45.930,0:00:49.609
In absolute error if we like, but either of those works fine as a last function

0:00:50.250,0:00:54.829
They don't have the problem of big flat areas like accuracy does replication

0:00:56.090,0:01:00.790
so what we want to do is to make it less likely that we're going to

0:01:01.490,0:01:05.830
Overfit by doing something we call reducing the capacity of the model

0:01:05.830,0:01:09.399
The capacity of the model is basically how much space does it have?

0:01:09.560,0:01:16.150
to find answers and if it can kind of find any answer anywhere those answers can include

0:01:17.210,0:01:19.210
basically memorizing the data set

0:01:20.030,0:01:25.300
So one way to handle this would be to decrease the number of latent factors

0:01:26.810,0:01:28.810
But generally speaking

0:01:29.660,0:01:34.719
Reducing the number of parameters in a model and particularly as we look at more deep learning style models

0:01:36.500,0:01:41.500
Ends up biasing the models towards very simple kind of shapes

0:01:42.440,0:01:44.440
But there's a better way to do it

0:01:44.690,0:01:46.690
Rather than reducing the number of parameters

0:01:47.210,0:01:50.919
Instead we try to force the parameters to be

0:01:52.009,0:01:54.699
Smaller unless they're really required to be big

0:01:55.790,0:01:57.790
And the way we do that is with weight decay

0:01:58.700,0:02:03.970
Weight decay is also known as l2 regularization. They're very slightly different, but we can think of them as the same thing

0:02:04.780,0:02:07.390
and what we do is we change our loss function and

0:02:07.970,0:02:11.229
specifically we change the loss function by adding to it the

0:02:11.840,0:02:13.819
sum of all the weights

0:02:13.819,0:02:15.250
squared

0:02:15.250,0:02:18.339
In fact, all of the parameters squared really should stay

0:02:19.790,0:02:23.110
Why do we do that? Well, because if that's part of the loss function

0:02:23.630,0:02:27.759
Then one way to decrease the loss would be to decrease the weights

0:02:28.550,0:02:35.649
One particular weight or all of the weights or or something like that. And so when we decrease the weights

0:02:38.209,0:02:41.919
If you think about what that would do and think about for example

0:02:43.430,0:02:45.640
The different possible

0:02:47.120,0:02:48.769
values

0:02:48.769,0:02:51.579
Of a in y equals ax squared

0:02:52.250,0:02:57.880
The larger a is for example, a is 50 you get these very narrow Peaks

0:02:58.640,0:03:00.729
in general eeeek

0:03:01.340,0:03:03.110
coefficients are going to cause

0:03:03.110,0:03:10.059
big swings big changes in the in the loss a small changes in the parameters and

0:03:10.370,0:03:12.370
When you have these kind of sharp

0:03:13.070,0:03:17.199
Peaks or valleys it means that a small change

0:03:17.959,0:03:19.959
buthe a parameter

0:03:20.000,0:03:24.910
Can make a go a small change to the input and make a big change to the loss

0:03:24.910,0:03:27.729
And so if you have if you're in that situation

0:03:28.519,0:03:34.149
Then you can basically fit all the data points close to exactly with a really complex

0:03:34.579,0:03:38.709
Jagged function with sharp changes which exactly tries to sit on

0:03:39.500,0:03:44.169
Each data point rather than finding a nice smooth surface which connects them all together

0:03:45.530,0:03:49.699
now that all goes through them all so if we limit our weights

0:03:51.330,0:03:54.949
By adding in the loss function the sum of the weights squared

0:03:56.130,0:04:02.509
Then what is going to do is it's going to fit less well on the training set because we're giving it less room to try

0:04:02.670,0:04:06.649
Anything that it wants to but we're going to hope that it would result in a better

0:04:07.350,0:04:11.659
Loss on the validation set or the test set so that it will generalize better

0:04:13.270,0:04:18.899
One way to think about this is that the loss with weight decay is just the loss plus

0:04:20.440,0:04:22.869
The some of the parameters squared times

0:04:25.070,0:04:31.029
Number we pick a hyper parameter. Sometimes it's like point 1 0.01 or 0.001 and if region

0:04:33.440,0:04:37.360
This is basically what loss with weight decay looks like in his equation

0:04:37.520,0:04:44.349
But remember when it actually comes to what's how was the loss used in stochastic gradient descent? It's used by taking its gradient

0:04:45.500,0:04:49.000
so what's the gradient of this well, if you remember back to

0:04:50.000,0:04:52.000
when you first learn calculus

0:04:52.460,0:04:56.889
It's okay. If you don't the gradient of something squared is just

0:04:57.440,0:05:02.860
Two times that's something we change from parameters to weight, which is a bit confusing though

0:05:04.540,0:05:08.129
This used wait here to keep it consistent maybe parameters is better

0:05:10.599,0:05:15.959
So the derivative of weight squared is just two times weight so in other words to add in

0:05:16.840,0:05:21.629
This term to the gradient we can just add to the gradients

0:05:22.419,0:05:29.218
Weight decay times two times weight and since weight decay is just a hyper parameter

0:05:29.219,0:05:36.059
we can just replace it with weight decay times two, so that would just give us weight decay times weight though weight decay refers to

0:05:37.300,0:05:39.300
adding on the

0:05:40.330,0:05:42.330
to the gradients

0:05:42.490,0:05:44.490
the weights times

0:05:44.770,0:05:49.439
Some hyper parameter and so that is going to try to create these kind of more shallow

0:05:50.259,0:05:52.000
less bumpy

0:05:52.000,0:05:54.000
surfaces

0:05:54.500,0:05:56.500
Do that

0:05:57.270,0:06:03.349
Simply when we call fetch will fit one cycle or whatever we can pass in a wd

0:06:04.140,0:06:09.860
Parameter and that's just this number here. So if we pass in point one

0:06:10.770,0:06:13.909
Then the training loss goes from point two nine

0:06:15.780,0:06:22.679
Point four nine that's much worse, right because we can't over fit anymore. But the valid loss goes from point eight nine zero

0:06:23.350,0:06:25.529
point eight two much better

0:06:26.560,0:06:30.690
So this is an important thing to remember for those of you that have done a lot of more traditional

0:06:31.360,0:06:32.980
statistical models

0:06:32.980,0:06:35.730
Is in kind of more traditional statistical models

0:06:35.730,0:06:41.579
we try to avoid overfitting and we try to increase generalization by decreasing the number of parameters, but

0:06:42.220,0:06:46.679
In a lot of modern machine learning and certainly the morning

0:06:47.500,0:06:49.500
We tend to instead use

0:06:50.440,0:06:51.550
regularization

0:06:51.550,0:06:58.829
Such as weight decay because it gives us more flexibility it lets us use more non-linear functions and still a boy

0:06:58.830,0:07:01.650
You know still reduces the capacity of the model

0:07:02.800,0:07:07.320
Great, so we're down to point eight two three. This is a good model. This is really actually a

0:07:08.050,0:07:10.050
very good model

0:07:10.720,0:07:14.489
and so let's dig into actually what's going on here because in our

0:07:15.460,0:07:17.460
in our architecture

0:07:17.919,0:07:21.089
remember, we basically just had or

0:07:21.910,0:07:27.089
Embedding layers or what's an embedding layer? We've described it conceptually, but let's write our own

0:07:28.510,0:07:34.029
And remember we said that an embedding layer is just a computational shortcut for doing a matrix

0:07:34.250,0:07:39.100
multiplication by a one hot and coded matrix and that that is actually the same as just

0:07:39.620,0:07:41.620
indexing into an array

0:07:43.610,0:07:45.610
So it embedding is just a

0:07:45.870,0:07:47.729
indexing into an array

0:07:47.729,0:07:50.538
And so it's nice to be able to create our own

0:07:51.060,0:07:54.679
Versions or things that exist in pi torch and faster, so let's do that for everybody

0:07:56.340,0:08:00.259
So if we're going to create our own kind of layer, which is pretty cool

0:08:01.710,0:08:04.909
we need to be aware of something which is normally a

0:08:05.969,0:08:08.179
Layer is basically created by

0:08:08.849,0:08:11.839
Inheriting as we've discussed from module or NN module

0:08:12.539,0:08:18.049
so for example, this is an example here of a module where we've created a class called T that inherits from module and

0:08:18.870,0:08:20.310
when it's

0:08:20.310,0:08:25.639
Constructed remember that's what dunder init does. We're just going to sit. This is just a dummy little module here

0:08:25.639,0:08:30.709
We're gonna set self taught a through the number one repeated three times as a tensor

0:08:31.379,0:08:33.379
now if you remember back to

0:08:33.990,0:08:38.450
Notebook for we talked about how the optimizers in pi torch and faster. I

0:08:38.849,0:08:43.729
Rely on being able to grab the parameters attribute to find a list of all the parameters now

0:08:43.950,0:08:45.950
If you want to be able to optimize

0:08:46.140,0:08:53.359
Elf dot a you would need to it to appear in parameters. But actually there's nothing there. Why is that?

0:08:54.770,0:08:59.179
that's because pi torch does not assume that everything that's in a module is

0:08:59.520,0:09:03.079
Something that needs to be learnt the talat that it's something that needs to be learned

0:09:03.080,0:09:06.199
You have to wrap it with n n dot parameter

0:09:06.330,0:09:11.779
So here's exactly the same class, but torch dot ones, which is just a list of three three ones

0:09:12.209,0:09:16.099
This case is wrapped in an end up parameter. And now if I go

0:09:16.770,0:09:21.139
Parameters as see I have a parameter the three ones in it

0:09:22.370,0:09:26.210
And that's going to automatically call requires great underscore for us as well

0:09:28.080,0:09:32.970
We haven't had to do that for things like n n dot linear in the past because pi torch

0:09:33.310,0:09:39.989
Automatically uses an end parameter internally. So if we have a look at the parameters for something that uses an independent linear

0:09:40.660,0:09:45.810
With no bias layer, you'll see again. We have here parameter with three things in it

0:09:49.420,0:09:53.460
So we want to in general be able to create a

0:09:54.130,0:09:59.429
Parameter so something with a bio tensor with a bunch of things in and generally we want to randomly initialize them

0:09:59.860,0:10:02.789
So randomly initialize we can pass in the size

0:10:02.790,0:10:08.969
We want we can initialize a tensor of zeroes of that size and then randomly generates some normal

0:10:09.430,0:10:16.080
Normally distributed random numbers with a mean of 0 and a deviation of pointer one. No particular reason. I'm picking those numbers just

0:10:16.839,0:10:18.839
Know how this works

0:10:19.660,0:10:23.680
Things that will give us back her um at a parameters of any size we want and so

0:10:24.410,0:10:30.069
Now we're going to replace everywhere that used to say embedding. I've got to replace it with create params

0:10:32.340,0:10:37.819
Everything else here is the same in the inert under unit and then the forward is very very similar

0:10:38.550,0:10:43.520
But before as you can see, I'm grabbing the zero index column from X

0:10:43.520,0:10:46.699
That's my users and I just look it up

0:10:47.580,0:10:50.809
As you see in that user factors array

0:10:51.970,0:10:55.180
And the cool thing is I don't have to do anything with gradients myself for this

0:10:55.430,0:11:00.129
Manual embedding layer because pay torch can figure out the gradients automatically as we've discussed

0:11:00.319,0:11:06.099
but then I just got the dot product as before add on the bias as before to the sigmoid range as before and

0:11:06.259,0:11:08.259
So here's a dot product bias

0:11:08.540,0:11:11.589
without any special hi torch layers

0:11:12.649,0:11:15.369
And we fit and we get the same result

0:11:16.339,0:11:20.229
So I think that is pretty amazingly cool. We've really shown that

0:11:21.079,0:11:27.429
The embedding layer is nothing fancy is nothing magic, right? It's just indexing into an array

0:11:27.589,0:11:31.869
So hopefully that removes a bit of the mystery for you

0:11:33.410,0:11:36.219
So, let's have a look at this model that we've created and we've trained

0:11:36.829,0:11:42.399
And find out what it's learned that's already useful. We've got something we can make pretty accurate predictions with

0:11:43.579,0:11:45.579
but let's find out what those

0:11:45.709,0:11:47.709
What the model looks like?

0:11:48.760,0:11:50.760
Remember when we cut a question

0:11:51.610,0:11:54.000
Okay, let's take a question before you can look at this

0:11:55.240,0:11:59.940
What's the advantage of creating our own embedding layer over the stock PI torch one? Oh

0:12:01.779,0:12:02.490
Nothing at all

0:12:02.490,0:12:05.729
We're just showing that we can it's it's great to be able to dig under the surface

0:12:05.920,0:12:08.490
Because at some point you'll want to try doing new things

0:12:08.860,0:12:13.919
so a good way to learn to do new things is to be able to replicate things that already exist and

0:12:14.290,0:12:16.290
You can exit you understand how they work

0:12:16.420,0:12:22.829
It's also a great way to understand the foundations of what's going on is to actually create encode your own implementation

0:12:23.949,0:12:26.759
But I wouldn't expect you to use this implementation in practice

0:12:28.890,0:12:30.890
But basically it removes all the mystery

0:12:31.720,0:12:32.510
so

0:12:32.510,0:12:33.400
If you remember

0:12:33.400,0:12:41.049
we've created a learner called learn and to get to the model that's inside it you can always call learn model and

0:12:41.870,0:12:44.049
then inside that there's going to be

0:12:44.660,0:12:51.250
Automatically created for it. Well, sorry not automatically. We've created all these attributes movie factors movie bias bias and so forth

0:12:52.190,0:12:54.580
Where we can grab learn model drop movie bias

0:12:55.970,0:13:01.930
and now what I'm going to do is I'm going to sort that vector and

0:13:02.690,0:13:04.460
I'm going to print out

0:13:04.460,0:13:06.050
the first five

0:13:06.050,0:13:07.400
idols and

0:13:07.400,0:13:14.229
so what this is going to do is it's going to print out though the movies with the smallest bias and

0:13:14.779,0:13:21.789
Here they are. What does this mean? Well, it kind of means these are the five movies that people

0:13:22.430,0:13:24.230
really didn't like

0:13:24.230,0:13:26.089
But it's more than that

0:13:26.089,0:13:30.339
It's it's not only do people not like them. But if we take account of the

0:13:31.550,0:13:38.800
Genre, they're in the actors. They have you know, whatever. The latent factors are people liked them a lot less than they expected

0:13:39.020,0:13:40.460
so maybe

0:13:40.460,0:13:45.189
example people this is kind of I haven't seen any of these movies likely perhaps this is a

0:13:47.630,0:13:54.279
Sci-fi movie so people who kind of like these sci-fi movies down there so bad. They still didn't like it

0:13:55.529,0:13:59.008
Um so we can do the exact opposite which is to sort

0:13:59.709,0:14:03.268
sending and here are the top five movies and

0:14:03.910,0:14:07.469
Specifically, they're the top five my bias, right? So these are the movies

0:14:07.470,0:14:09.470
so even after you take account of the fact that

0:14:09.519,0:14:13.739
LA Confidential I have seen all of these ones. So a low confidential is a kind of a

0:14:14.319,0:14:22.019
Murder mystery cop movie I guess and people who don't necessarily like that genre or I think Guy Pearce was in it

0:14:22.019,0:14:26.729
So maybe they don't like Guy Pearce very much. Whatever people still like this movie more than they expect

0:14:28.620,0:14:33.390
So this is a kind of a nice thing that we can look inside our model and see what it's learned

0:14:34.480,0:14:37.109
now we can look out not only at the

0:14:38.830,0:14:40.000
Bias

0:14:40.000,0:14:41.140
vector

0:14:41.140,0:14:43.770
But we can also look at the factors now

0:14:43.770,0:14:50.549
There are 50 factors, which is too many to visualize so we could use a technique called pca principal components

0:14:50.550,0:14:51.900
Now this the details don't matter

0:14:51.900,0:14:59.040
But basically they're going to squish those 50 factors down to 3 and then we'll plot the top two

0:15:00.160,0:15:02.309
as you can see here and

0:15:03.520,0:15:05.549
What we see when we plot the top 2

0:15:06.700,0:15:14.040
Is we can kind of see that the movies have been and have spread out across a space of of?

0:15:14.500,0:15:17.520
Some kind of latent factors. And so if you look at the far, right?

0:15:18.190,0:15:20.190
There's a whole bunch of kind of

0:15:21.089,0:15:28.618
Big budget actually things and on the far left. There's more like cult kind of things Fargo Schindler's List

0:15:29.769,0:15:31.769
Monty Python

0:15:32.139,0:15:35.728
By the same token at the bottom. We've got some

0:15:37.059,0:15:39.059
English Patient

0:15:39.279,0:15:46.019
Perryman Harry Met Sally so kind of romance drama kind of stuff and at the top we've got

0:15:46.659,0:15:51.239
action and sci-fi kind of stuff so you can see even as though we haven't

0:15:51.909,0:15:54.718
Asked in any information about these movies

0:15:55.809,0:15:58.408
What we've seen is who likes what?

0:15:59.379,0:16:03.358
these latent factors have automatically kind of figured out a

0:16:03.819,0:16:10.139
Space or a way of thinking about these movies based on what kinds of movies people like and what other kinds of movies they like

0:16:10.209,0:16:11.829
along with those

0:16:11.829,0:16:14.339
That's really interesting to kind of try and visualize

0:16:15.279,0:16:17.279
What's going on inside?

0:16:17.649,0:16:19.649
your model

0:16:21.900,0:16:26.480
Now we don't have to do all this manually we can actually just say

0:16:27.210,0:16:28.920
Give me a collab learner

0:16:28.920,0:16:34.999
using this set of data loaders with this number of factors in this way range and it does everything we've just seen

0:16:35.880,0:16:41.150
Again about the same number. Okay. Well now you can see this is nice, right?

0:16:41.150,0:16:46.400
we've actually be able to see right underneath inside the collab liner part of the FASTA a

0:16:46.830,0:16:51.199
Application the collaborative filtering application and we can build it all ourselves from scratch

0:16:51.200,0:16:57.410
We know how to create the SGD know how to create the embedding layer. We know how to create the

0:16:58.950,0:17:06.830
Model the architecture. So now you can see, you know, we've really and build up from scratch our own version of this

0:17:08.320,0:17:12.809
So if we just type learned up model, you can see here. The names are a bit more generic

0:17:12.810,0:17:18.240
This is a user weight item weight is a bias item bias, but it's basically the same stuff we've seen before

0:17:19.929,0:17:27.769
And we can replicate the exact analyses we saw before by using this same idea. Yep

0:17:29.280,0:17:33.439
Slightly different order this time because is a bit random but

0:17:34.230,0:17:36.439
Pretty similar as well

0:17:37.470,0:17:42.890
Another interesting thing we can do is we can think about the distance between two movies

0:17:43.410,0:17:46.669
So let's grab all the movie factors

0:17:47.309,0:17:49.309
Or just pop them into a variable

0:17:50.039,0:17:53.719
And then let's pick a movie

0:17:55.480,0:17:57.819
And then let's find the

0:17:59.360,0:18:00.860
Distance

0:18:00.860,0:18:02.540
from

0:18:02.540,0:18:04.540
That movie ooh

0:18:04.700,0:18:09.250
every other movie and so one way of thinking about distance is you might recall the

0:18:09.890,0:18:11.890
Pythagorean formula or

0:18:12.110,0:18:16.959
The distance on on a on the hypotenuse of a triangle which is also the distance

0:18:17.390,0:18:23.709
We were a point on it in a Cartesian plane on a chart which is root x squared plus y squared

0:18:24.530,0:18:28.419
You might know doesn't matter if you don't but you can do exactly the same thing for

0:18:29.090,0:18:31.539
50 dimensions. It doesn't just work for two dimension

0:18:33.429,0:18:34.840
There's a

0:18:34.840,0:18:41.789
that tells you how far away a point is from another point if you if x and y are actually

0:18:42.340,0:18:44.879
differences between two movie

0:18:46.029,0:18:48.029
vectors

0:18:49.929,0:18:52.408
So then what gets interesting is

0:18:53.929,0:18:55.929
You can actually then

0:18:56.159,0:18:57.509
provide that

0:18:57.509,0:18:58.590
kind of by the

0:18:58.590,0:19:05.659
by the length to make all the lengths the same distance to find out how the angle between any two B movies and that actually

0:19:05.659,0:19:10.069
Turns out to be a really good way to compare the similarity of two things that's called cosine similarity

0:19:10.080,0:19:13.369
And so the details don't matter you can look them up if you're interested

0:19:13.619,0:19:19.339
But the basic idea here is to see that we can actually pick a movie and find the movie

0:19:19.340,0:19:21.409
That is the most similar to it

0:19:22.259,0:19:27.019
Based on these factors kind of interest ever questions. All right Oh,

0:19:28.320,0:19:35.450
What motivated learning at a 50 dimensional embedding and then using yay to reduce three versus just learning a few mention?

0:19:36.810,0:19:41.099
Oh because the purpose of this was actually to create a good model though. The the

0:19:41.740,0:19:48.420
Visualization part is normally kind of the exploration of what's going in on in your model and so with a 50

0:19:48.940,0:19:51.690
With 50 latent factors, you're going to get a more accurate

0:19:53.740,0:19:55.740
So that's one approach is this

0:19:56.630,0:19:58.100
dot product

0:19:58.100,0:20:02.890
Version there's another version we could use which is we could

0:20:03.800,0:20:05.600
create a

0:20:05.600,0:20:07.600
set of user factors and

0:20:08.090,0:20:10.390
a set of item factors and

0:20:11.360,0:20:13.360
The slight before we could look them up

0:20:14.690,0:20:18.379
But what we could then do instead of doing a dot product we could concatenate them together

0:20:19.789,0:20:21.739
into a

0:20:21.739,0:20:29.499
tensor that contains both the user and the movie factors next to each other and then we could pass them through a

0:20:30.830,0:20:37.659
simple little neural network lennier Lu linear and then sigmoid ranges before

0:20:39.110,0:20:41.329
So importantly here the first linear layer

0:20:41.340,0:20:47.360
The number of inputs is equal to a number of user factors plus the number of item factors

0:20:48.510,0:20:52.099
And the number of outputs is however many activations

0:20:52.740,0:20:54.740
we have

0:20:54.750,0:20:55.860
and

0:20:55.860,0:21:01.969
then which may be we just default to a hundred here and then the final layer will go from a hundred to one because we're

0:21:01.970,0:21:04.370
Just making one prediction and so we could create

0:21:05.340,0:21:12.110
Or call that collab and n we can instantiate that to create a model. We can create a learner and we can get

0:21:13.169,0:21:18.679
It's not going quite as well as before it's not terrible, but it's not quite as good as our dot product version

0:21:20.070,0:21:23.030
But the interesting thing here is it does give us some more flexibility

0:21:23.549,0:21:28.309
Which is that since we're not doing a dot product we can actually have a different

0:21:29.309,0:21:30.690
embedding size

0:21:30.690,0:21:34.880
For each of users versus items and actually faster

0:21:34.880,0:21:41.989
I has a simple heuristic if you call get embedding size and pass in your data loaders, it will suggest appropriate size

0:21:43.500,0:21:51.140
Embedding matrices or each of your categorical variables its each of your user and item

0:21:53.910,0:21:55.260
Enters

0:21:55.260,0:21:57.709
so that's so if we pass in

0:21:58.380,0:22:03.049
Star m's beddings that's going to pass in the user

0:22:04.520,0:22:06.520
Apple and the item

0:22:07.340,0:22:09.669
Apple which we can then pass to embedding

0:22:10.610,0:22:14.709
This is star prefix we learned about in the last class in case you forgot

0:22:16.730,0:22:18.880
So this is kind of interesting we can

0:22:20.640,0:22:22.640
You know we can see here that there's two different

0:22:22.780,0:22:27.149
Architectures we could pick from it wouldn't be necessarily obviously ahead of time, which one's going to work better?

0:22:27.310,0:22:34.259
I'm in this particular case the the simplest one the the dot product one actually turned out to work a bit better, which is interesting

0:22:35.380,0:22:38.640
this particular version here if you call collab donor and

0:22:39.280,0:22:41.519
pass use NN equals true

0:22:42.760,0:22:46.560
then what that's going to do is it's going to use this version the version with

0:22:48.130,0:22:51.030
Concatenation and the linear layers

0:22:54.400,0:22:57.689
So coli Bellona use an N equals true again

0:22:57.690,0:23:01.740
We get about the same result as you'd expect because it's just a rock-cut for this version

0:23:02.820,0:23:04.820
And it's interesting actually

0:23:06.669,0:23:11.189
We have a look at collab liner, it actually returns an object of type embedding an N

0:23:11.190,0:23:11.860
and

0:23:11.860,0:23:12.809
It's kind of cool

0:23:12.809,0:23:18.359
If you look inside the frosty air source code or use the double question mark trick to see the source code for embedding it in

0:23:18.429,0:23:20.429
You'll see it's three lines of code

0:23:21.730,0:23:25.559
How does that happen because we're using this thing called tabular model

0:23:26.200,0:23:28.149
Which we will learn about

0:23:28.149,0:23:30.508
in a moment, but basically this

0:23:32.289,0:23:34.679
Neural net version of collaborative filtering is

0:23:35.470,0:23:40.500
literally just a tabular model in which we pass no continuous variables and

0:23:42.850,0:23:44.850
I'm embedding sizes

0:23:45.660,0:23:47.660
So we'll see that in a moment

0:23:49.810,0:23:51.460
Okay, so that is

0:23:51.460,0:23:59.129
collaborative filtering and again take a look at the further research section in particular how if you finish the questionnaire and because there's some really

0:23:59.800,0:24:03.300
Important next steps you can take to push your knowledge and your skills

0:24:06.140,0:24:10.509
So let's now move to my book mine a Beulah and

0:24:11.990,0:24:15.189
we're going to look at tabular modeling and do a deep dive and

0:24:15.440,0:24:21.100
let's start by talking about this idea that we were starting to see here, which is

0:24:22.950,0:24:24.810
Embeddings and

0:24:24.810,0:24:32.749
Specifically, let's move beyond just having embeddings for users and items at embeddings for any kind of categorical variable

0:24:33.270,0:24:37.879
But really because we know an embedding is just a lookup into an array

0:24:38.610,0:24:40.610
It can handle

0:24:41.040,0:24:43.040
Any kind of discrete?

0:24:43.620,0:24:45.090
categorical data

0:24:45.090,0:24:51.320
So things like age are not discrete their continuous numerical data, but something like sex or postcode

0:24:52.050,0:24:59.449
Are categorical variables? Um, they have a certain number of discrete levels. The number of discrete levels. They have is called their

0:25:00.090,0:25:01.710
cardinality

0:25:01.710,0:25:08.300
So des haven't looked at an example of a data set that contains both categorical and continuous

0:25:08.730,0:25:15.620
Variables we're going to look at the Rossman sales competition that ran on Kegel a few years ago

0:25:15.620,0:25:18.559
and so basically what's going to happen is we're going to see a

0:25:19.080,0:25:21.650
table that contains information about

0:25:21.929,0:25:27.469
Various stores in Germany and the goal will be to try and predict how many sales there's going to be

0:25:28.050,0:25:31.699
for each day in a couple of weeks period for each store

0:25:34.410,0:25:39.300
One of the interesting things about this competition is that one of the gold medalists used

0:25:39.490,0:25:42.540
Deep learning and it was one of the earliest known

0:25:43.090,0:25:46.799
example of a state of the art deep learning a beulah model I

0:25:47.560,0:25:49.560
Mean this is not long ago

0:25:49.600,0:25:57.390
2015 or something but really this idea of creating state Athiya tabular models with deep learning has not been very common

0:25:57.820,0:25:59.820
and for not very long

0:26:00.890,0:26:04.359
You know interestingly compared to the other gold medalists in this competition

0:26:05.090,0:26:10.329
The folks that used deep learning used a lot less feature engineering and a lot less domain expertise

0:26:10.330,0:26:14.020
And so they wrote a paper called entity embeddings of categorical variables

0:26:14.660,0:26:16.660
in which they basically

0:26:17.059,0:26:24.579
described the exact thing that you saw in the in notebook eight the way you can think of one hot encodings as

0:26:25.010,0:26:30.550
Just being embeddings. You can catenate them together and you can put them through a couple of layers

0:26:31.670,0:26:36.670
They call them dense layers. We've got the Millennial layers and create a neural network out of there

0:26:37.889,0:26:39.869
so this is really

0:26:39.869,0:26:44.059
neat, you know it's kind of simple and obvious in hindsight trick and

0:26:44.700,0:26:47.269
They actually did exactly what we did in the paper

0:26:47.399,0:26:53.689
which is to look at the results of the trained embeddings, and so for example

0:26:53.690,0:26:56.059
they had an embedding matrix for

0:26:58.679,0:27:00.679
Regions in Germany

0:27:01.130,0:27:03.530
Because there was what there wasn't really metadata about this

0:27:03.530,0:27:06.709
These were just learnt in bedding was just like we learnt embeddings about movies

0:27:06.710,0:27:13.490
And so then they just created just like we did before a chart where they popped each region

0:27:14.010,0:27:16.010
According to I think probably a PCA

0:27:16.440,0:27:21.769
Of their embeddings and then if you circle the ones that are close to each other in blue

0:27:22.710,0:27:29.360
you'll see that they're actually close to each other in Germany and ditto for red and it over green and

0:27:29.790,0:27:31.790
Then here's the brown

0:27:32.250,0:27:34.250
so this is like

0:27:34.410,0:27:41.089
Pretty amazing is the way that we can see that it's kind of learnt something about what Germany looks like

0:27:41.700,0:27:45.080
Based entirely on the purchasing behavior of people in those states

0:27:46.380,0:27:49.280
something else they did was to look at every straw and

0:27:49.980,0:27:51.980
they looked at the

0:27:52.020,0:27:54.079
distance between stores in

0:27:54.810,0:27:56.989
practice like how many kilometers away they are and

0:27:57.690,0:28:03.290
Then they looked at the distance between stores in terms of their embedding distance

0:28:03.290,0:28:11.149
Just like we saw in the previous notebook and there was this very strong correlation that stores that were close to each other

0:28:12.270,0:28:13.590
physically

0:28:13.590,0:28:15.590
Ended up having close

0:28:16.320,0:28:18.530
embeddings as well even as though

0:28:20.370,0:28:23.690
Location of these stores in physical space was not part of the model

0:28:25.739,0:28:28.819
Ditto with days of the week so the days of the week for another

0:28:30.149,0:28:35.659
Embedding and the days of the week that were next to each other ended up next to each other in

0:28:36.269,0:28:41.929
Embedding space and ditto for months of the year. So pretty fascinating they're way

0:28:42.450,0:28:44.219
kind of

0:28:44.219,0:28:51.019
information about the world ends up captured just by looking at browning embeddings, which as we know are just

0:28:52.079,0:28:54.079
index lookups into an array

0:28:55.910,0:29:02.259
So the way we then combine these categorical variables with these embeddings with continuous variables

0:29:03.440,0:29:05.440
What was done in both the?

0:29:05.840,0:29:10.359
Entity embedding paper that we just looked at and then also described in more detail

0:29:11.780,0:29:15.790
By Google when they described how their recommendation system in Google Play works

0:29:16.730,0:29:24.730
this is from Google's paper is they have the categorical features that go through the embeddings and then there are continuous features and

0:29:25.010,0:29:29.080
Then all the embedding results and the continuous features are just concatenated together

0:29:29.660,0:29:36.190
Into this big concatenated table that then goes through in this case three layers of a neural net and interestingly

0:29:36.190,0:29:38.409
they also take the kind of

0:29:39.980,0:29:45.700
Collaborative filtering bit and do the dot product as well and combine the two so they use both of the

0:29:45.830,0:29:48.849
Bricks were used in the previous notebook and combine them together

0:29:51.120,0:29:52.800
So

0:29:52.800,0:29:55.489
That's the basic idea. We're going to be seeing or

0:29:56.430,0:29:59.389
proving beyond just collaborative filtering which is

0:30:00.060,0:30:05.659
Just two categorical variables to as many categorical and as many continuous variables as we like

0:30:07.070,0:30:09.070
But before we do that

0:30:09.420,0:30:15.139
let's take a step back and think about other approaches because as I mentioned the idea of

0:30:15.900,0:30:22.369
Deep learning is a kind of a best practice for tabular data. It is still pretty new and it's still kind of controversial

0:30:24.150,0:30:27.139
It's certainly not always the case but it's the best approach

0:30:27.960,0:30:31.579
So when we're not using deep learning, what would we be using?

0:30:32.490,0:30:40.349
Well, what would probably be using is something called an ensemble of decision trees and the two most popular are random forests

0:30:40.350,0:30:41.800
and

0:30:41.800,0:30:44.639
Gradient boosting machines or something them alone

0:30:45.640,0:30:53.579
so basically between multi-layered neural networks learnt with SGD and ensembles of decision trees that kind of covers the vast majority of

0:30:54.610,0:31:02.160
Approaches that you're likely to see for tabular data. And so we're going to make sure we cover them both us today. In fact

0:31:03.940,0:31:10.830
So although deep learning is nearly always clearly superior for stuff like images and audio and

0:31:11.050,0:31:17.039
Natural language texts, these two approaches tend to give somewhat similar results a lot of the time

0:31:17.770,0:31:18.940
for

0:31:18.940,0:31:24.359
Tabular data, so let's take a look some, you know, you really should generally try both and see which works best for you

0:31:25.180,0:31:27.180
for each problem you look at

0:31:30.250,0:31:35.530
Why does the range go from zero to 5.5 if the maximum I've

0:31:37.640,0:31:42.790
That's a great question. Um, the reason is if you think about it for sigmoid it's

0:31:43.340,0:31:49.959
Actually impossible for a sigmoid to get all the way to the top or all the way to the bottom. There's a asymptotes

0:31:50.240,0:31:53.170
So no matter how far how big your X is

0:31:53.420,0:31:57.039
Can never quite get to the top or no matter how small it is. It can never quite get

0:31:57.650,0:32:04.300
So if you want to be able to actually predict a rating of five, then you need to use something higher than five your maximum

0:32:06.500,0:32:14.469
Our embeddings used only for highly cardinal categorical variables or is this approach you general or low

0:32:14.810,0:32:17.560
cardinality can unuse one-hot encode a

0:32:21.140,0:32:26.109
Remind you cardinality is the number of great levels and variable and

0:32:29.110,0:32:30.049
That

0:32:30.049,0:32:35.019
An embedding is just a computation or shortcut or oh one hot encoding

0:32:35.510,0:32:40.119
so there's really no reason to use a one hot encoding because it's

0:32:41.000,0:32:42.710
As long as you have more than two levels

0:32:42.710,0:32:50.140
It's always going to be more memory and lower and give you exactly mathematically the same thing and if there's just two levels

0:32:50.480,0:32:55.660
Then it is basically identical so there isn't really any reason not to use

0:32:57.700,0:32:59.700
Thank you for those great questions

0:33:00.620,0:33:02.620
Um

0:33:03.700,0:33:05.700
Okay, so

0:33:07.080,0:33:14.000
One of the most important things about decision tree ensembles is that at the current state of the technology they do provide

0:33:14.640,0:33:17.089
faster and easier ways of interpreting the model

0:33:17.090,0:33:22.610
I think that's rapidly improving for deep learning models on tabular data, but that's where we are right now

0:33:23.330,0:33:31.040
They also require rest less hyper parameter tuning. So they're easier to kind of get right the first time. So my first approach

0:33:31.740,0:33:39.469
analyzing a dataset is always an ensemble of decision trees and specifically I pretty much always start with a random forest because it's just so

0:33:39.660,0:33:41.660
reliable

0:33:41.670,0:33:43.670
Yes

0:33:43.760,0:33:48.640
Your experience for highly imbalanced as such fraud or medical data

0:33:49.310,0:33:56.199
What usually best out of rainforest XG boost or neural network? Um, I'm not sure that

0:33:57.389,0:33:58.209
the

0:33:58.209,0:34:04.259
Weather the data is balanced or unbalanced is a key reason for choosing one of those about the others

0:34:04.260,0:34:06.260
I would try all of them and see which works best

0:34:09.110,0:34:15.819
So the exception to the guideline about start with decision tree ensembles as your first thing to try would be if there's some very high

0:34:15.980,0:34:22.869
Cardinality categorical variables then they can be a bit difficult to get to work really well in decision tree ensembles

0:34:24.680,0:34:30.669
Or if there's something like most importantly that's like plain text data or image data or audio data or something like that and

0:34:30.800,0:34:34.900
Then you'll definitely kind of need to use a neural net in there

0:34:34.900,0:34:37.839
But you could actually ensemble it with a random forest as we'll see

0:34:42.129,0:34:46.889
Okay, oh really we're going to need to understand how our decision tree ensembles work

0:34:48.940,0:34:56.129
So pay torch isn't a great choice for decision tree ensembles they're really designed for gradient based methods and

0:34:57.549,0:35:02.369
Random forests and decision tree growing are not really gradient based methods in the same way

0:35:03.549,0:35:05.969
So instead we're going to use a library called scikit-learn

0:35:08.359,0:35:10.969
Referred to as SK learn as a module

0:35:12.680,0:35:14.680
Scikit-learn does a lot of things

0:35:15.079,0:35:20.709
And we're only going to touch on a tiny piece of them if we need to do to train

0:35:21.589,0:35:23.589
decision trees and random forests

0:35:24.079,0:35:29.558
We've already mentioned before was McKinney's book. Also a great book for understanding more about circuit learn

0:35:31.500,0:35:33.500
So the data set for learning about

0:35:34.120,0:35:38.939
Decision tree ensembles is going to be another data set. It's going to it's called the

0:35:39.730,0:35:43.800
Blue Book for bulldozes data set and it's a cackle competition

0:35:44.830,0:35:48.299
So Kaggle competitions are fantastic. They are

0:35:50.290,0:35:56.999
Machine learning competitions where you get interesting data sets you get feedback on whether your approach is any good or not?

0:35:57.130,0:36:03.120
You can see on a leaderboard what approaches are working best and then you can read blog posts from the winning contestants

0:36:03.490,0:36:05.490
sharing tips and tricks

0:36:06.130,0:36:08.130
it's certainly not a

0:36:08.590,0:36:10.590
substitute for actual

0:36:13.660,0:36:15.470
Practice doing end-to-end

0:36:15.470,0:36:21.699
Data science projects but for becoming good at creating predictive models that are predictive

0:36:22.130,0:36:29.140
It's a really fantastic resource highly recommended and you can also submit to old most old competitions

0:36:29.390,0:36:32.829
To see how you would have gone without having to worry about you know

0:36:32.829,0:36:37.869
the kind of stress of like whether people will be looking at your results because they're not

0:36:39.200,0:36:41.349
Publicized or published if you do that

0:36:41.990,0:36:48.579
Is it yes, it's a question you comment on real time applications of random forests. I

0:36:49.250,0:36:54.489
Experience they tend to be too slow for real-time use cases like a recommender system

0:36:54.829,0:36:57.818
Neural network is much faster when run on the right hardware

0:36:58.400,0:37:01.059
Let's get to that once in go - shall we?

0:37:04.430,0:37:06.430
Now you can't just

0:37:08.280,0:37:09.610
Download an entire

0:37:09.610,0:37:14.909
cackled datasets using the entire data thing that we have in first AI so you actually have to sign up to Kegel and

0:37:15.010,0:37:16.630
Then follow these instructions

0:37:16.630,0:37:18.630
for how to download

0:37:19.780,0:37:24.179
Data from Kegel make sure you replace creds here with what it describes

0:37:24.180,0:37:30.450
You need to get a special API code and then run this one time to put that up on your server

0:37:30.450,0:37:32.450
and now you can

0:37:33.670,0:37:37.470
Use Kegel to download data using the API

0:37:38.830,0:37:40.000
so

0:37:40.000,0:37:44.580
After we do that, we're going to end up with a bunch of as you see CSV files

0:37:45.310,0:37:47.340
So let's take a look at this data

0:37:49.290,0:37:50.770
Well the main data

0:37:50.770,0:37:53.429
The main table is trained on CSV

0:37:53.460,0:37:55.460
remember that's comma separated values and

0:37:55.540,0:38:02.160
The training set contains information such as unique identify over say all the using unique identify over machine

0:38:02.320,0:38:07.229
Sale price say old date. So what's going on here is one row of the data

0:38:08.110,0:38:13.410
Represents a seeing a seal of a single piece of heavy machinery like a bulldozer

0:38:13.960,0:38:21.510
At an auction. So it happens at a date as a price. It's of some particular piece of equipment and so forth

0:38:22.540,0:38:26.219
So if we use pandas again to read in a CSV file

0:38:26.860,0:38:28.979
Let's combine training and valid together

0:38:29.620,0:38:31.620
We can then look at the columns to see

0:38:32.410,0:38:36.839
There's a lot of columns there and many things which I don't know. What the hell they mean like blade

0:38:37.000,0:38:39.030
extension and pad type and ride control

0:38:40.420,0:38:41.710
but the good news is

0:38:41.710,0:38:46.290
We're going to show you a way that you don't have to look at every single column and understand what they mean

0:38:46.780,0:38:49.410
And random forests are going to help us with that as well

0:38:50.560,0:38:56.580
So once again, we're going to be seeing this idea that models can actually help us with data understanding and data cleanup

0:38:58.150,0:39:04.709
One thing we can look at is ordinal columns and good place to look at that now if there's things there that you know are

0:39:05.590,0:39:08.999
discrete values that have some order like product size

0:39:09.520,0:39:14.400
It has medium and small and large medium and many these should not be in

0:39:15.880,0:39:19.979
You know alphabetical order or some random order they should be in this

0:39:20.560,0:39:23.729
Specific order right? They they have they have a specific ordering

0:39:24.280,0:39:26.170
so we can

0:39:26.170,0:39:28.030
use

0:39:28.030,0:39:34.870
But as type to turn it into a categorical variable and then we can say set categories ordered equals true

0:39:35.630,0:39:38.020
To basically say this is an ordinal column

0:39:38.020,0:39:43.389
So it's got discrete values, but we actually want to define what the order of the classes are

0:39:46.270,0:39:48.329
We need to choose which is the dependent variable

0:39:48.520,0:39:53.939
and we do that by looking on cattle and cowgirl will tell us that the thing we're meant to be predicting is sale price and

0:39:54.370,0:39:56.880
Actually specifically they'll tell us the thing

0:39:56.880,0:40:02.970
we're meant to be predicting is the log of sale price because root mean squared log error is them is the

0:40:04.060,0:40:07.350
What we're actually gonna be judged on in the competition where we take the log

0:40:08.530,0:40:13.199
So we're not going to replace sale price with its log and that's what we'll be using for now on

0:40:14.690,0:40:16.690
So a decision tree ensemble

0:40:17.150,0:40:22.420
Requires decision trees. Well, let's start by looking at decision trees. So a decision tree

0:40:23.180,0:40:28.479
In in in this case is a something that asks a series of binary

0:40:28.480,0:40:32.110
that is yes or no questions about data so such as is

0:40:33.050,0:40:37.840
Somebody less integrated at less than 30. Yes. They are. Are they eating? Healthily? Yes, they are

0:40:37.840,0:40:41.410
And so okay, then we're going to say they're fit or unfit

0:40:42.200,0:40:45.040
So like there's an example of some arbitrary

0:40:45.890,0:40:49.209
Decision tree that somebody might have come up with it's a series of binary

0:40:49.760,0:40:54.699
Yes, and no choices and at the bottom are leaf nodes that make some prediction

0:40:56.360,0:40:58.360
Now of course

0:41:00.010,0:41:01.580
Our

0:41:01.580,0:41:08.859
Bulldozers competition. We don't know what binary questions to ask about these things. And in what order?

0:41:09.680,0:41:12.280
in order to make a prediction about sale price, so

0:41:12.950,0:41:19.240
We're doing machine learning. So we're going to try and come up with some automated way to create the questions and

0:41:20.000,0:41:24.040
There's actually a really simple procedure for doing that. You have a think about it

0:41:24.040,0:41:27.159
so if you want to kind of stretch yourself here, however, think about

0:41:27.770,0:41:29.770
What's an automatic procedure?

0:41:29.869,0:41:36.969
that you can come up with that would automatically build a decision tree where the final answer would do a

0:41:37.250,0:41:42.520
you know the difficulty better than random job of estimating the sale price of

0:41:43.850,0:41:45.850
one of these options

0:41:48.660,0:41:51.720
All right, so here's here's the approach that we could use

0:41:52.510,0:41:57.300
Look through each column of the data set. We're going to go through each of

0:41:57.820,0:42:01.350
But obviously not sale prices the dependent variable sale ID machine ID

0:42:02.350,0:42:06.000
Auctioneer year made, etc. And so one of those will be for example

0:42:06.700,0:42:08.700
product size

0:42:08.740,0:42:12.159
And so then what we're going to do is we're going to loop through

0:42:13.310,0:42:14.900
each

0:42:14.900,0:42:16.900
possible value of

0:42:17.060,0:42:18.320
product size

0:42:18.320,0:42:20.469
Large large medium medium, etc

0:42:20.470,0:42:25.540
And then we're going to do a split basically like where this comma is and we're going to see okay

0:42:25.540,0:42:27.540
Let's get all of the options of large

0:42:28.880,0:42:35.470
Equipment and put that into one group and everything that's smaller than that and put that into another group

0:42:37.250,0:42:40.280
And so that's here split the data into two groups

0:42:41.940,0:42:44.099
Based on whether they're greater than or less than that value

0:42:45.200,0:42:50.710
If it's a categorical non ordinal value a variable, it'll be just whether it's equal or not equal to that level

0:42:51.940,0:42:56.109
And then we're going to find the average sale price for each of the two groups

0:42:56.110,0:43:03.460
So for the large group, what is the average sale price for the more than large group? What was the average sale price and

0:43:04.130,0:43:09.819
That will be our model. Our prediction will simply be the average sale price for that group

0:43:11.450,0:43:15.500
And so then you can say well how good is that model if our model was just to ask a single question

0:43:15.660,0:43:21.589
With a yes/no answer put things into two groups and take the average of the group as being our prediction

0:43:21.690,0:43:26.540
And we can see how good with that model be. What would be the root means great error from that model

0:43:27.559,0:43:30.319
And so we can then say all right. How could it would it be if we

0:43:31.170,0:43:36.709
Use large better and then let's try again. What if we did large medium is a split

0:43:37.019,0:43:39.319
What if we did medium is this better?

0:43:39.319,0:43:43.458
and so in each case we can find the root mean squared error of that incredibly simple model and

0:43:43.949,0:43:49.609
then once we've done that for all of the product size levels we can go to the next column and look at

0:43:52.320,0:43:58.739
a lot usage banned and do every level of usage banned and then state your level or state and

0:43:59.560,0:44:06.449
So forth and so there'll be some variable and some split level which gives the best

0:44:07.000,0:44:14.729
Root mean squared error of this really really simple model. And so then I'll say okay, that would be our first binary decision

0:44:15.370,0:44:20.249
it gives us two groups and then we're going to take each one of those groups separately and

0:44:21.580,0:44:24.479
Great find another single binary decision

0:44:25.180,0:44:28.409
For each of those two groups using exactly the same procedure

0:44:29.080,0:44:35.580
But then we'll have four groups and then we'll do exactly the same thing again separately feature those four groups and so forth

0:44:40.150,0:44:41.529
So

0:44:41.529,0:44:43.200
Let's see what that looks like

0:44:43.200,0:44:49.439
And in fact, once we've gone through this you might even want to see if you can implement this algorithm yourself. It's not

0:44:50.140,0:44:56.549
Reveal, but it doesn't require any special coding skills. So hopefully you can find you will be able to do it

0:44:58.309,0:45:03.488
There's a few things we have to do before we can actually create a decision tree in terms of just some basic data munging

0:45:05.400,0:45:07.500
One is if we're going to take advantage of dates

0:45:09.010,0:45:15.850
We actually want to call faster eyes add date part function and what that does as you see after we call it

0:45:17.100,0:45:21.749
As it creates a whole different a bunch of different bits of metadata from that data

0:45:21.750,0:45:26.010
So all year there are months so a week say all day and so forth

0:45:28.990,0:45:33.129
So sale date of itself doesn't have a whole lot of information

0:45:34.280,0:45:37.599
Directly, but we can pull a lots of different information out of it

0:45:37.599,0:45:40.209
And so this is an example of something called feature engineering

0:45:40.700,0:45:47.439
Which is where we take some peace some piece of data and we try to grab create lots other lots of other pieces of data

0:45:47.540,0:45:53.619
From it. So is this particular day at the end of a month or not at the end of a year or not?

0:45:53.619,0:45:55.619
and so forth but

0:45:56.089,0:45:58.089
That handle states

0:45:58.580,0:46:04.719
There's a bit more cleaning. We want to do and faster UI provides some things to make meaning easier

0:46:05.359,0:46:11.348
We can use the tabular pandas class. We create a tabular data set Anders

0:46:12.710,0:46:13.500
and

0:46:13.500,0:46:15.500
specifically we're going to use to

0:46:15.960,0:46:21.679
fabula processes or tabular procs a tabular processor is basically just to transform and

0:46:21.930,0:46:25.760
Racine transforms before so go back and remind yourself where the transform is

0:46:27.150,0:46:31.700
Except it's just slightly different. It's like three lines of code if you look at the code for it

0:46:32.820,0:46:36.379
it's actually going to modify the object in place rather than

0:46:36.780,0:46:37.680
creating a new object

0:46:37.680,0:46:42.589
And giving it back to you and that's because often these tables of data are kind of really big and we don't want to waste

0:46:42.780,0:46:44.780
lots of RAM

0:46:45.089,0:46:49.909
And it's just gonna run the transform once and save the result rather than doing it

0:46:49.910,0:46:54.020
Lazily when you access it, but for the same reason we just want to make this a lot faster

0:46:55.210,0:46:56.570
so

0:46:56.570,0:47:03.399
Just think of them as transforms really. So one of them is called category FA and category fire is going to replace a column

0:47:04.130,0:47:09.970
With numeric categories using the same basic idea of like a vocab like we've seen before

0:47:11.930,0:47:14.169
Bill missing is going to find any

0:47:14.300,0:47:20.469
Columns with missing data and it's going to fill in the missing data with the median of the data and create a new column

0:47:20.720,0:47:24.879
Your boolean column which is set to true for anything that was missing though

0:47:24.880,0:47:29.650
These two things is basically enough to get you to a point where most of the time you'll be able to train a model

0:47:32.790,0:47:38.370
Now the next thing we need to do is think about our validation set as we discussed in lesson one

0:47:39.800,0:47:44.989
A random validation set is not always appropriate and certainly for something like predicting

0:47:45.690,0:47:51.650
Auction results it almost certainly is not appropriate because we're going to be wanting to use a model in the future

0:47:52.350,0:47:56.299
Not at some random date in the past. So the way this Kaggle

0:47:56.970,0:48:04.340
competition was set up was that that the test set the thing that you had to fill in and submit for the competition was

0:48:04.980,0:48:09.560
Two weeks of data that was after any of the training set

0:48:10.500,0:48:12.709
So we should do the same thing for validation set

0:48:13.870,0:48:15.870
we should create something which is

0:48:16.400,0:48:19.990
Where the validation set is the last couple of weeks of data?

0:48:20.780,0:48:22.040
and

0:48:22.040,0:48:29.230
And so then the training set will only be data before that. So we basically can do that by grabbing everything before

0:48:29.840,0:48:31.190
October 2011

0:48:31.190,0:48:34.690
I'll create our training and validation set based on that condition and

0:48:35.510,0:48:37.510
grabbing those bits

0:48:39.700,0:48:45.090
So that's going to spit our baiting certain validation set by date not randomly

0:48:46.670,0:48:48.710
We're also going to need to tell when you

0:48:49.260,0:48:50.930
Create a tabular pandas object

0:48:50.930,0:48:55.310
You're going to be passing in a data frame going to be passing in your tabular procs

0:48:55.310,0:48:58.909
And you also have to say what are my categorical and continuous variables?

0:48:59.460,0:49:00.900
We can use

0:49:00.900,0:49:03.050
faster use Conte cat split

0:49:03.180,0:49:08.990
We automatically split a data frame do continuous and categorical variables for you

0:49:09.510,0:49:11.510
So we can just pass those in

0:49:12.460,0:49:19.810
what what is the dependent variable you can have more than one and what are the indexes to split into training and valid and

0:49:20.150,0:49:22.130
This is a tabular object

0:49:22.130,0:49:29.260
so it's got all the information you need about the training set the validation set categorical and continuous variables and the dependent variable and

0:49:29.570,0:49:31.570
Any processes to run?

0:49:33.400,0:49:38.710
It looks a lot like a data set subject but has a dot train it has a dot valid

0:49:40.580,0:49:45.469
And so if we have a look at that show we can see

0:49:46.290,0:49:48.290
the data

0:49:48.450,0:49:51.859
But Dutch show is going to show us the kind of the string data

0:49:51.990,0:49:57.260
But if we look at but items you can see internally it's actually stored

0:49:57.990,0:50:02.180
These very compact numbers which we can use directly in a model

0:50:05.010,0:50:09.919
So first ideas basically got us to a point here where we have our data into a format

0:50:09.930,0:50:13.760
I'm ready for modeling and validation sets being created

0:50:14.640,0:50:16.640
to see how these numbers

0:50:16.980,0:50:23.629
Relate to these strings we can again just like we saw last week use the classes attribute

0:50:23.630,0:50:26.750
Which is a dictionary it basically tells us the vocab

0:50:26.750,0:50:34.609
So this is how we look up for example, 6 is 0 1 2 3 4 5 6 this is compact example

0:50:35.980,0:50:42.870
That processing took takes a little while to run so you can go ahead and save the tabular object

0:50:42.870,0:50:48.150
And so then you can load it back later without having to rerun or the processing

0:50:49.120,0:50:55.199
So that's a nice kind of fast way to quickly get back up and running without having to reprocess your data

0:50:56.950,0:50:59.040
So we've done the basic data munging

0:50:59.040,0:51:06.239
We need we can now create a decision tree and in scikit-learn a decision tree where the dependent variable is. Continuous is a

0:51:07.150,0:51:09.150
decision tree regressor

0:51:09.370,0:51:12.599
Now let's start by telling it. We just want a total of four

0:51:13.660,0:51:16.289
Leaf nodes. We'll see what that means in a moment and

0:51:17.110,0:51:20.160
In scikit-learn you generally call fit

0:51:20.160,0:51:26.339
so it looks quite a lot like fast AI and you pass in your independent variables and your dependent variable and

0:51:26.650,0:51:31.349
We can grab those straight from our tabular object training set. It's not X's and

0:51:32.290,0:51:38.429
But Y and we can do the same thing for validation just to save us and typing. Okay question

0:51:39.250,0:51:43.620
you have any thoughts on what data augmentation for tabular data might look like

0:51:49.420,0:51:53.950
I don't have a great sense of data organization with nebula dater

0:51:57.170,0:52:00.889
We'll be seeing later either in this course or in the next part

0:52:01.980,0:52:06.889
Drop out and mix up and stuff like that which they might be able to do that

0:52:07.619,0:52:09.619
in later layers in

0:52:10.380,0:52:13.640
The tabular model. Otherwise, I think you'd need to think about

0:52:14.930,0:52:19.669
The semantics of the data and think about what are things you could do to change the data without changing the meaning?

0:52:20.609,0:52:22.609
Sounds like a pretty tricky

0:52:22.619,0:52:24.270
well a

0:52:24.270,0:52:27.949
Question is fastly I distinguish between ordered

0:52:28.559,0:52:31.039
categories such as low medium high and

0:52:31.589,0:52:36.229
Unordered category world bearable. I guess that that was that ordinal thing

0:52:36.230,0:52:39.889
I told you about before and all it really does is it ensures that your

0:52:41.010,0:52:43.819
Classes list has a specific order

0:52:43.890,0:52:47.059
so then these numbers actually have a specific and

0:52:47.220,0:52:52.099
As you'll see that's actually going to turn out to be pretty important for how we trained our random first

0:52:55.210,0:52:58.260
Okay, so we can create a decision tree regressor we can fit it

0:52:59.140,0:53:02.160
And then we can draw it the faster I function

0:53:03.400,0:53:04.750
and

0:53:04.750,0:53:08.129
Here is the decision tree. We just trained

0:53:08.890,0:53:13.680
And we and behind the scenes. This actually used be

0:53:15.160,0:53:17.850
Basically the exact process that we described

0:53:18.700,0:53:22.889
Back here, right? So this is where you can like try and create your own

0:53:23.980,0:53:27.480
Decision tree implementation if you're interested in stretching yourself

0:53:29.559,0:53:31.400
So we're going to use one that's already

0:53:31.400,0:53:36.460
exists and the best way to understand what it's done is to look at this diagram from top to bottom so

0:53:37.609,0:53:45.578
This first step is it says like okay the initial model it created is a model with no binary splits at all

0:53:46.730,0:53:51.189
Pacifically it's always going to predict the value 10.1 or every single row

0:53:52.220,0:53:53.960
Why is that?

0:53:53.960,0:53:59.020
well because this is the simplest possible model is to take the average of the dependent variable and

0:53:59.690,0:54:04.329
Always predict that and so this is always should be you're kind of pretty much your basic

0:54:05.150,0:54:07.150
baseline for regression

0:54:07.640,0:54:10.420
There are four hundred and four thousand seven hundred and ten

0:54:10.940,0:54:17.770
rows options that we're averaging and the mean squared error of this incredibly simple model in which there are no

0:54:18.710,0:54:23.829
Rules at all though groups at all. Just a single average is a point weight

0:54:25.289,0:54:32.028
so then the next most complex model is to take a single column a plus system and

0:54:32.429,0:54:37.069
A single binary decision is coupler system less than or equal to 0.5

0:54:38.279,0:54:39.449
through

0:54:39.449,0:54:42.139
There are 360 thousand eight hundred forty-seven

0:54:43.919,0:54:51.319
Options where it's true and forty three thousand eight hundred sixty-three where it's false and now interestingly in the false case

0:54:52.619,0:54:57.649
You can see that there are no further binary decisions. So this is called a leaf node

0:54:57.859,0:55:00.859
it's a node where this is as far as you can get and

0:55:01.380,0:55:05.329
So if your coupler system is not less than or equal to 0.5

0:55:07.480,0:55:11.280
Prediction this model makes for euro sale price is nine point two one

0:55:12.820,0:55:14.010
Versus if it's true

0:55:14.010,0:55:19.139
It's ten point to one so you can see it's actually found a very big difference here and that's why it picked

0:55:19.140,0:55:24.810
This is the first binary split and so the mean squared error for this section here is 0.12

0:55:25.270,0:55:28.109
Which is far better than we started out at 0.5

0:55:30.710,0:55:31.470
Has

0:55:31.470,0:55:36.980
360,000 in it and so it does another binary split this time is the year that this

0:55:37.349,0:55:39.979
Piece of equipment made was at less than or equal to

0:55:41.070,0:55:44.899
1991 and a half if it was if it's true

0:55:45.510,0:55:46.520
Then we get a leaf node

0:55:46.520,0:55:52.849
And the prediction is nine point nine seven means grid arrow point three seven if the value is false

0:55:52.849,0:55:53.750
We don't have a leaf node

0:55:53.750,0:55:58.790
And we have another binary split and you can see eventually we get down to here coupler system

0:55:58.790,0:56:04.909
true year made false product size false mean square error point one seven

0:56:05.339,0:56:09.799
so all of these leaf nodes of MSE s that are smaller than

0:56:10.440,0:56:11.970
that

0:56:11.970,0:56:14.629
original baseline model of just taking the mean

0:56:15.690,0:56:17.690
so this is how you can grow a

0:56:17.849,0:56:23.929
Decision tree and we only stopped here because we said max leaf knows is for one two, three. Oh

0:56:24.630,0:56:28.339
Right, and so if we want to keep training it further

0:56:29.700,0:56:33.470
We can just use a higher number. Um, there's actually a very nice

0:56:35.760,0:56:37.999
Library by Terrence park or D Treves

0:56:38.760,0:56:44.119
Which can show us exactly the same information like so and so here are the same

0:56:44.609,0:56:51.109
Leaf nodes one through three or and you can see the kind of the chart of how many are there

0:56:51.109,0:56:55.219
This is the split coupler system point five. Here are the two groups

0:56:55.220,0:56:58.010
you can see the sale price in each of the two groups and

0:56:58.680,0:56:59.990
Then here's the leaf node

0:56:59.990,0:57:04.129
And so then the second split was on year made, and you can see here

0:57:04.230,0:57:10.490
Something weirds going on with um aid there's a whole bunch of here made sit or a thousand which is obviously not a sensible

0:57:11.460,0:57:15.560
Year for a bulldozer to be made so presumably that's some kind of missing value

0:57:17.210,0:57:22.480
So when we look at their kind of the picture like this it can give us some insights about what's going on in our data

0:57:22.480,0:57:24.480
and so maybe

0:57:24.500,0:57:28.659
We should replace those thousands with

0:57:29.359,0:57:35.979
Nineteen fifty because that's you know, obviously a very very early year for a bulldozer and so we can kind of pick it arbitrarily

0:57:36.320,0:57:41.199
It's actually not really going to make any difference to the model. That's created

0:57:41.690,0:57:45.609
Because all we care about is the order because we're just doing these binary splits

0:57:45.800,0:57:48.459
That'll make it easier to look at as you can see

0:57:48.859,0:57:55.179
Here's our nineteen fifties now. And so now it's much easier to see what's going on in that binary speed

0:57:57.329,0:58:01.789
So let's now get rid of max leaf nodes and build a bigger decision tree

0:58:03.410,0:58:05.569
And then let's just for the rest of this

0:58:06.660,0:58:10.399
Notebook create a couple of real functions one to create the root mean squared error

0:58:11.099,0:58:13.099
which is just here and

0:58:13.289,0:58:17.329
Another one to take a model and some independent independent variables

0:58:18.059,0:58:19.500
predict

0:58:19.500,0:58:24.589
From the model on the independent variables and then take the root mean squared error with a dependent variable

0:58:24.869,0:58:28.068
So that's going to be our models where it means great error

0:58:29.640,0:58:34.589
So for this decision tree in which we didn't have a stopping criteria, so as many leaf nodes as you like

0:58:35.529,0:58:37.529
the models root mean squared error is

0:58:38.470,0:58:40.380
zero

0:58:40.380,0:58:42.380
So we've just built the perfect model

0:58:44.210,0:58:48.849
So this is great news, right we have built the perfect option trading system

0:58:49.160,0:58:53.950
Well, remember we actually need to check the validation set. Let's check the check Mrs

0:58:53.950,0:58:57.879
E with a validation set and oh it's worse than zero

0:58:58.310,0:59:03.910
So our training set is zero our validation set is much worse than zero

0:59:04.730,0:59:06.730
Why is that happened?

0:59:07.130,0:59:13.240
Well, one of the things that a random forest in SK learn can do is it can tell you the number of leaf nodes number?

0:59:13.240,0:59:16.119
Of leaves there are 340 1000

0:59:17.930,0:59:19.349
Data points

0:59:19.349,0:59:20.790
400,000

0:59:20.790,0:59:23.659
So in other words, we have nearly as many leaf nodes as data points

0:59:23.910,0:59:29.059
Most of our leaf nodes only have a single thing in but they're taking an average of a single thing clearly

0:59:29.059,0:59:32.959
This makes no sense at all. So what we should actually do is

0:59:33.660,0:59:34.980
pick some

0:59:34.980,0:59:41.389
Different stopping criteria and let's say okay if you got a leaf node with 25 things or less in it

0:59:41.849,0:59:47.058
don't don't split or don't split things to create a leaf node with less than 25 things in it and

0:59:47.640,0:59:52.460
Now if we fit and we look at the root mean squared error for the validation set

0:59:53.040,0:59:55.219
It's going to go down from 0.33

0:59:56.520,0:59:57.540
0.32

0:59:57.540,1:00:04.639
So the training sets got worse from zero two point two four eight. The validation sets got better and now we only have

1:00:05.700,1:00:07.319
12,000 leaf nodes

1:00:07.319,1:00:09.319
So that is much more reasonable

1:00:10.860,1:00:11.540
All right

1:00:11.540,1:00:16.880
So let's take a five-minute break and then we're going to come back and see how we get the best of both worlds

1:00:16.880,1:00:19.760
How are we going to get something which has the kind of?

1:00:20.280,1:00:22.309
flexibility to get these

1:00:22.980,1:00:24.980
You know what? We're going to get down to zero

1:00:25.560,1:00:28.190
But to get you know really deep trees

1:00:28.950,1:00:36.559
But also without overfitting and the trick will be to use something called bagging. I'll come back and talk about that in five minutes

1:00:40.700,1:00:42.700
You

1:00:42.920,1:00:44.920
Okay, so welcome back so

1:00:46.260,1:00:50.429
We're going to look at how we can get the best of both worlds as we discussed and

1:00:53.090,1:00:59.600
Let's start by having a look at what we're doing with categorical variables first of all, and so you might notice that

1:01:01.320,1:01:05.719
Obviously with categorical variables for example in collaborative filtering we had two

1:01:08.549,1:01:13.018
You know kind of think about like how many embedding levels we have

1:01:13.019,1:01:19.649
For example, if you've used other modeling tools, you might have there in Singh's with creating dummy variables stuff like that

1:01:19.809,1:01:22.619
um random forests on the whole you don't have to

1:01:24.240,1:01:28.350
The the reason is that as we've seen all of our

1:01:30.150,1:01:32.150
Categorical variables

1:01:32.690,1:01:36.530
Turned into numbers and so we can perfectly well have

1:01:38.599,1:01:45.379
Session tree binary decisions which use those particular numbers now

1:01:46.700,1:01:49.339
The numbers might not be ordered at any interesting way

1:01:50.850,1:01:52.850
But if there's a particular level

1:01:52.870,1:01:57.089
Which kind of stands out as being important it only takes two

1:01:57.610,1:02:02.279
Binary splits the split out that level into a single

1:02:03.610,1:02:05.610
You know into a single piece

1:02:06.460,1:02:12.150
So generally speaking. I don't normally worry too much about kind of encoding

1:02:13.690,1:02:17.129
Categorical variables in a special way as I mentioned I do try to

1:02:17.980,1:02:24.510
Encode ordinal variables by seeing what the order of the levels is because often as you would expect

1:02:25.510,1:02:26.710
sizes, for example

1:02:26.710,1:02:32.640
You know medium and small or gonna mean kind of next to each other and large and extra-large would be next to each other

1:02:32.830,1:02:35.279
That's good to have those as similar numbers

1:02:37.150,1:02:41.279
Having said that you can out of one hot encode a

1:02:42.580,1:02:46.529
Categorical variable if you want to using get dummies in pandas

1:02:47.830,1:02:53.489
But there's not a lot of evidence that that actually helps. There's actually that has been bought in a paper

1:02:54.400,1:02:59.639
And so I would say in general for categorical variables. Don't worry about it too much just

1:03:00.340,1:03:02.969
Use what we've shown you. You have a question

1:03:03.610,1:03:07.949
For for ordinal categorical variables. How do you deal with?

1:03:08.950,1:03:11.100
when they have like n/a or

1:03:12.040,1:03:14.310
Missing values. Where do you put that in the order?

1:03:17.380,1:03:24.940
So in first AI na missing values always appear as the first item will always be the zero index

1:03:25.310,1:03:27.789
atom and also if you get something in

1:03:27.950,1:03:30.609
The validation or test set which was a level

1:03:30.610,1:03:35.349
We haven't seen in training that will be considered to be that missing or na value as well

1:03:37.440,1:03:39.440
All right, so

1:03:40.240,1:03:46.800
What we're gonna do to try and improve our random forest is we're going to use something called bagging and this was developed

1:03:47.380,1:03:48.520
by a

1:03:48.520,1:03:52.679
retired Berkeley professor named leo breiman in 1994 and

1:03:53.349,1:03:58.169
He did a lot of great work and perhaps you could see that most of it happened after he retired

1:03:59.200,1:04:06.480
His technical report was called bagging predictors and he described how you could create multiple versions of her predictor

1:04:06.480,1:04:08.380
so multiple different models

1:04:08.380,1:04:13.349
and you could then aggregate them by averaging over the

1:04:14.170,1:04:16.089
predictions and

1:04:16.089,1:04:21.328
Specifically, um, the way he suggested doing this was to create what he called bootstrap

1:04:21.910,1:04:29.730
Replicates in other words randomly select different subsets of your data now train a model on that subset and it store it away

1:04:30.089,1:04:33.119
As one of your predictors and then do it again a bunch of times

1:04:33.329,1:04:39.359
and so each of these models is trained on a different random subset of your data and then you

1:04:40.510,1:04:45.599
To predict you predict on all of those different versions of your model and average them

1:04:46.990,1:04:48.990
and it turns out that

1:04:50.079,1:04:54.299
bagging works really well, so this the sequence of steps is basically

1:04:55.000,1:04:57.000
randomly choose some subset of rows

1:04:57.970,1:04:59.349
Where in a model?

1:04:59.349,1:05:05.519
Using that subset gave that model and then returned to step one through that a few times to train a few models

1:05:06.779,1:05:10.978
And then to make a prediction predict with all the models and take the average

1:05:13.160,1:05:15.160
That is packing and

1:05:15.210,1:05:21.540
It's very simple, but it says punishingly powerful. And the reason why is that each of these models we've trained

1:05:23.560,1:05:26.159
Although they are not using all of the data

1:05:26.830,1:05:30.749
So they're kind of less accurate than a model that use all of the data

1:05:31.480,1:05:35.849
Each of them is the errors are not correlated

1:05:37.599,1:05:43.199
You know that the errors because of using that smaller subset are not correlated with the errors of the other models

1:05:43.510,1:05:45.510
because they're random subsets and

1:05:46.030,1:05:48.839
so when you take the average of

1:05:49.599,1:05:51.809
a bunch of kind of

1:05:52.780,1:05:55.109
Errors, which are not correlated with each other

1:05:55.990,1:05:58.589
The average of those errors is zero

1:05:59.830,1:06:01.930
So therefore the average of the models

1:06:03.290,1:06:07.600
Should give us an accurate prediction of the thing. We're actually trying to predict

1:06:07.910,1:06:15.129
So as I say here's an amazing result, we can improve the accuracy of nearly any kind of algorithm by training at multiple times

1:06:16.070,1:06:19.930
About different random subsets of data and then averaging the predictions

1:06:22.240,1:06:28.020
Though then Bremen in 2001 showed a way to do this specifically for decision trees

1:06:28.420,1:06:32.579
Where not only did he randomly choose a subset of rows for each model?

1:06:33.160,1:06:38.309
But then for each binary split he also randomly selected a subset of columns

1:06:39.640,1:06:41.640
and this is called the random forest and

1:06:42.380,1:06:48.400
It's perhaps the most widely used most practically important machine learning method and astonishingly

1:06:49.310,1:06:51.310
simple

1:06:51.670,1:06:56.609
To create a random forest regressor you use SK Loans random first aggressor

1:06:56.920,1:07:04.109
if you pass n drops minus one, it will use all of the CPU cores that you have to run as fast as possible n

1:07:05.320,1:07:08.820
Estimators says how many trees how many models to train?

1:07:10.120,1:07:13.259
Max sample says how many rows to use?

1:07:14.020,1:07:18.030
randomly chosen rows to use in each one max features is how many

1:07:18.820,1:07:22.440
randomly chosen columns to use for each binary split point

1:07:23.410,1:07:26.279
Min samples leaf is the stopping criteria

1:07:27.700,1:07:34.139
And we'll come back to so here's a little function that will create a random first regressor and fit it through

1:07:34.840,1:07:37.950
Some set of independent variables and a dependent variable

1:07:39.170,1:07:41.170
So we can give it a few

1:07:41.700,1:07:44.869
default values and create a random forest and

1:07:46.049,1:07:47.760
train and

1:07:47.760,1:07:51.559
Our validation set our MSE is 0.23

1:07:52.710,1:07:54.710
If we compare that

1:07:54.809,1:07:56.809
of what we had before

1:07:57.940,1:07:59.940
You

1:08:01.540,1:08:03.540
We had

1:08:04.450,1:08:07.200
Point three - so dramatically better

1:08:08.860,1:08:10.860
By using a random forest

1:08:12.140,1:08:14.140
Um, so

1:08:15.099,1:08:17.258
So what's happened when we

1:08:18.080,1:08:24.369
Called random first aggressor is it's just using that decision tree builder that we've already seen but it's building

1:08:24.619,1:08:30.729
multiple versions with these different random subsets and for each binary split it does it's also

1:08:31.310,1:08:33.310
randomly selecting a subset of columns

1:08:34.609,1:08:40.099
And then when we create a prediction, it is averaging the predictions of each of the trees

1:08:41.370,1:08:46.829
and as you can see, it's giving a really great result and one of the amazing things we'll find is that

1:08:47.380,1:08:50.970
It's gonna be hard for us to improve this very much, you know

1:08:50.970,1:08:53.970
The kind of the default starting point tends to turn out to be pretty great

1:08:58.719,1:09:00.719
The the SK learn documentation

1:09:04.989,1:09:06.989
Crease the number of estimators

1:09:07.270,1:09:09.270
out as the

1:09:09.489,1:09:15.119
accuracy improve error rate proofs or different max features levels

1:09:16.650,1:09:19.609
And in general the more trees you add

1:09:20.609,1:09:26.388
The more accurate your model there the it's not going to overfit right because it's averaging more of these

1:09:30.659,1:09:38.189
These week models more of these models that are trained on subsets of the data so train as many users many estimators as you like

1:09:39.130,1:09:43.980
Really just a case of how much time do you have? And whether you kind of reach a point where it's not really improving anymore

1:09:45.560,1:09:52.100
You can actually get at the underlying decision trees in a model in a random first model using estimators underscore

1:09:52.440,1:09:56.810
so with a list comprehension we can call predict on each individual tree and

1:09:57.570,1:10:04.880
So here's an array an umpire array containing the predictions from each individual tree for each row in our data

1:10:06.210,1:10:08.210
So if we take the mean

1:10:09.690,1:10:12.509
Across the 0 axis Rockette

1:10:14.700,1:10:17.530
Exactly the same number as remember

1:10:19.700,1:10:21.709
First does is it takes the main?

1:10:23.180,1:10:25.189
of the trees predictions

1:10:27.160,1:10:30.189
So one cool thing we could do is we could look at

1:10:30.830,1:10:32.830
the 40

1:10:32.930,1:10:37.720
Estimators we have and grab the predictions or the first eye

1:10:40.280,1:10:42.280
Those trees and take their mean

1:10:43.710,1:10:45.710
Then we can find the root mean square error

1:10:47.210,1:10:51.230
In other words here is the accuracy when you've just got one tree

1:10:51.900,1:10:53.219
root trees

1:10:53.219,1:10:55.489
Three trees four trees five trees, etc

1:10:55.560,1:11:00.499
and you can see so it's kind of nice right you can you can actually create your own and have

1:11:01.860,1:11:05.810
Build your own tools to look inside these things and see what's going on

1:11:05.810,1:11:09.229
And so we can see here that as you add more and more trees

1:11:09.480,1:11:14.689
The accuracy did indeed keep improving or the root mean squared error kept improving although it

1:11:15.270,1:11:17.270
That the improvement slowed down

1:11:17.580,1:11:19.580
after a while

1:11:20.940,1:11:22.940
The

1:11:23.300,1:11:29.210
Set is worse and the training set and there's a couple of reasons that could have happened

1:11:30.390,1:11:37.220
The first reason could be because we're still overfitting which is not necessarily a problem. This is something we could I did

1:11:37.950,1:11:43.190
or maybe it's because the the fact that we're trying to predict the last two weeks is

1:11:43.590,1:11:48.230
Actually a problem and that the last two weeks are kind of different to the other

1:11:48.840,1:11:51.500
Options in our data set maybe something changed over time

1:11:51.990,1:11:58.790
So how do we tell which of those two reasons there are what what is the reason that our validation set is worse?

1:11:59.940,1:12:03.469
We can actually find out using a very clever trick called out-of-bag error

1:12:03.990,1:12:07.369
oob era, and we use our B error for lots of things you

1:12:08.340,1:12:10.340
Can grab the OOBE error?

1:12:12.380,1:12:14.509
Well, you can grab the arrow be predictions

1:12:15.180,1:12:22.159
From the model with oral-b prediction and you can grab the rmse and you can find that the arrow be error

1:12:22.440,1:12:24.440
RSR MSE is

1:12:24.510,1:12:26.190
0.21

1:12:26.190,1:12:29.149
Which is quite a bit better than 0.2 3

1:12:30.180,1:12:33.979
So let me explain what our OB error is

1:12:34.920,1:12:38.180
What our be error is is we look at

1:12:38.850,1:12:40.850
each

1:12:41.080,1:12:46.350
Row of the training set not the validation set each row of the training set and

1:12:46.810,1:12:49.320
We say so fix a reset for row. Number one

1:12:50.530,1:12:52.530
Which trees?

1:12:52.750,1:12:57.089
Included row number one in the training and will say okay. Let's not use those

1:12:57.790,1:13:02.519
For calculating the error because it was part of those trees training. I would just

1:13:03.100,1:13:06.210
Calculate the error for that row using the trees

1:13:07.030,1:13:13.199
Where that row was not included in training that tree because remember every tree is using only a subset of the data

1:13:13.900,1:13:21.029
So we do that for every row we find the prediction using only the trees that were not used

1:13:22.210,1:13:24.389
That that that row is not used

1:13:25.389,1:13:30.159
and those are the Aerobie predictions but other words, this is like giving us a

1:13:30.919,1:13:34.419
Validation set result and without actually needing a validation

1:13:35.860,1:13:37.900
But the thing is it's not

1:13:38.570,1:13:41.679
With that time offset is not looking at the last two weeks

1:13:41.680,1:13:46.659
It's looking at the whole training set but this basically tells us how much of the error is

1:13:47.180,1:13:48.470
due to

1:13:48.470,1:13:49.490
overfitting

1:13:49.490,1:13:51.609
Versus Giroux to being the last couple of weeks

1:13:52.160,1:13:53.350
So that's a cool trick

1:13:53.350,1:13:58.600
Our B error is something that very quickly kind of gives us a sense of how much we're we're overfitting

1:13:59.090,1:14:01.360
And we don't even need a validation set to do it

1:14:03.300,1:14:08.300
But there's our OB error, so that's telling us a bit about what's going on in our model, um,

1:14:10.050,1:14:15.809
Then there's a lot of things we'd like to find out from our model and I've got five things in particular here

1:14:15.809,1:14:17.809
Which I generally find pretty interesting

1:14:18.099,1:14:19.239
which is

1:14:19.239,1:14:21.239
How confident are we?

1:14:21.280,1:14:23.940
About our predictions. There's some particular

1:14:24.849,1:14:29.279
Prediction we're making like we can say this is what we think the prediction is

1:14:29.559,1:14:35.339
But how confident are we is that exactly that or is it just about that or we really have no idea

1:14:37.150,1:14:39.759
For particular predicting a particular

1:14:40.460,1:14:42.050
item

1:14:42.050,1:14:46.000
Which factors were the most important in that prediction and how did they influence it?

1:14:47.410,1:14:53.610
Overall, which columns are making the biggest difference in empro. We once could be maybe throw away and it wouldn't matter

1:14:55.120,1:15:01.360
Which columns are basically redundant with each other as how we don't really need both of them

1:15:02.630,1:15:10.190
And as we vary some column, how does it change the prediction? So those are the five things that work that I'm interested in

1:15:10.860,1:15:13.759
Figuring out and we can do all of those things with a random first

1:15:15.170,1:15:17.170
Let's start with the first one

1:15:17.469,1:15:19.779
the first one we've already seen

1:15:20.540,1:15:22.370
that we can

1:15:22.370,1:15:24.370
grab all of the predictions

1:15:25.480,1:15:28.529
But all of the trees and take their main

1:15:29.960,1:15:33.230
To get the actual predictions of the model and then to get the rmse

1:15:33.300,1:15:36.709
But what if instead of saying mean we did exactly the same thing?

1:15:38.300,1:15:42.139
Like so but instead it said standard deviation

1:15:44.080,1:15:52.080
This is going to tell us for every row and our dataset how much did the trees vary and

1:15:52.960,1:16:00.060
so if our model really had never seen kind of data like this before it was something where

1:16:00.670,1:16:05.040
You know different trees were giving very different predictions. It might give us a sense that maybe

1:16:06.790,1:16:13.799
This is something that we're not at all confident about and as you can see when we look at the standard deviation of the trees

1:16:13.989,1:16:21.989
For each prediction. Let's just look at the first five. They vary a lot at 0.2 0.1 0.0. Nine point zero point three

1:16:22.780,1:16:24.250
Okay

1:16:24.250,1:16:25.930
so

1:16:25.930,1:16:27.930
this is

1:16:29.090,1:16:31.610
A really interesting. It's not something that a lot of people talk about

1:16:31.610,1:16:34.699
But I think it's a really interesting approach to kind of figuring out

1:16:35.940,1:16:42.980
Whether we might want to be cautious about a particular prediction because maybe we're not very confident about it

1:16:44.090,1:16:47.119
There's one thing we can easily do with the random first

1:16:47.639,1:16:52.159
the next thing and this is I think the most important thing for me in terms of interpretation is

1:16:52.679,1:16:54.679
feature importance

1:16:55.229,1:17:01.009
Here's what feature importance looks like we can call feature importance on a model. There's some independent variables

1:17:01.289,1:17:04.639
Let's say grab the first ten this says these are the 10

1:17:05.159,1:17:07.159
most important features

1:17:08.350,1:17:15.460
In this random processes are the things that are the most strongly driving sale price or we could plot them

1:17:16.750,1:17:18.350
And so you can see here

1:17:18.350,1:17:25.329
There's just a few things that are by far the most important. What year the

1:17:26.000,1:17:29.319
Equipment was made bulldozer or whatever. How big is it?

1:17:30.350,1:17:35.319
Up, ler system, whatever that means and the product class, whatever that means

1:17:36.170,1:17:37.850
and so

1:17:37.850,1:17:39.850
You can

1:17:40.190,1:17:46.930
Get this by simply looking inside your train model and grabbing the feature importances attribute

1:17:47.880,1:17:51.659
And so here for making it better to print out. I'm just sticking that into a data frame and

1:17:52.449,1:17:54.189
sorting

1:17:54.189,1:17:56.189
ascending by importance

1:17:57.310,1:17:59.310
So, how is this actually being done

1:17:59.350,1:17:59.850
um

1:17:59.850,1:18:07.019
it's it's actually really neat what scikit-learn does and and Ryman the inventor of random forests described is

1:18:07.360,1:18:09.360
That you can go through each tree

1:18:10.210,1:18:13.149
and then start at the top of the tree and look at each branch and

1:18:13.489,1:18:18.579
At each branch see what feature was used the split rich binary

1:18:18.580,1:18:26.350
which the binary split was based on which column and then how much better was the model after that split compared to beforehand and

1:18:26.930,1:18:32.409
we basically then say okay that column was responsible with that amount of improvement and

1:18:32.660,1:18:35.049
so he can add that up across all of the

1:18:35.960,1:18:39.489
splits across all of the trees for each column and

1:18:40.100,1:18:44.499
then you normalize it so they all add to one and that's what gives you

1:18:45.290,1:18:51.850
These numbers which we show the first few of them in this table and the first 30 of them here in this chart

1:18:53.150,1:18:58.989
So this is something that's fast and it's easy and it kind of gives us a good sense of like well

1:18:59.360,1:19:01.450
maybe the stuff that are less than

1:19:02.270,1:19:03.650
0.005

1:19:03.650,1:19:05.650
We could remove

1:19:06.200,1:19:11.019
So if we did that that would leave us with only 21 columns

1:19:12.840,1:19:16.410
So let's try that, let's just but just say okay exes

1:19:16.410,1:19:20.879
Which are important the exes which are in this list of ones to keep?

1:19:21.550,1:19:23.550
Do the same a valid?

1:19:23.740,1:19:25.859
retrain our random forest

1:19:27.020,1:19:29.020
and have a look at the result and

1:19:29.280,1:19:32.360
Basically, our accuracy is about the same

1:19:33.390,1:19:37.819
But we've gone down from 78 columns the 21 color

1:19:38.800,1:19:40.380
So I think this is really important

1:19:40.380,1:19:41.829
It's not just about

1:19:41.829,1:19:46.049
Creating the most accurate model you can but you want to kind of be able to fit it in your head

1:19:46.119,1:19:48.899
As best as possible and so 21 columns

1:19:48.900,1:19:53.609
it's going to be much easier for us to check that any data issues and understand what's going on and

1:19:53.920,1:19:56.190
the accuracy is about the same or the

1:19:57.340,1:19:59.019
rmse

1:19:59.019,1:20:04.379
So I would say okay, let's do that. Let's just stick with X's important for now on

1:20:05.510,1:20:09.679
And so here's this entire set of the 21 features

1:20:11.370,1:20:13.439
You can see it looks now like year made and

1:20:13.990,1:20:20.129
Product size of the two really important things and then there's a cluster of kind of mainly product related things

1:20:20.320,1:20:22.469
That are kind of at the next level of importance

1:20:24.910,1:20:29.309
One of the tricky things here is that we've got like

1:20:30.710,1:20:33.799
a product class desk model lady

1:20:34.590,1:20:41.120
Secondary desk model desk base model a model descriptor. They all look like there might be similar ways of saying the same thing

1:20:42.120,1:20:47.899
So one thing that can help us to interpret the feature importance better and understand better. What's happening the model

1:20:49.410,1:20:53.809
Is to remove redundant features

1:20:55.639,1:20:59.439
So one way to do that is to call faster use cluster columns

1:20:59.440,1:21:03.669
which is basically a thin wrapper for stuff that scikit-learn already provides and

1:21:04.730,1:21:08.079
What that's going to do is it's going to find pairs of columns

1:21:08.750,1:21:11.409
Which are very similar you can see here

1:21:11.409,1:21:15.339
So year and sale elapsed see how this line is way out to the right

1:21:15.769,1:21:19.389
Or else machine ID and model ID is not at all as way out to the left

1:21:19.610,1:21:22.569
But that means that say all year and sale elapsed are

1:21:23.300,1:21:27.670
Very very similar when one is lower the other tends to be low and vice versa

1:21:28.429,1:21:34.388
here's a group of three which all seem to be much the same and then product group desk and product group and

1:21:34.820,1:21:37.630
Then fi best base model and fi model desk

1:21:38.179,1:21:41.109
But these all seem like things where maybe we could remove

1:21:42.260,1:21:46.869
One of each of these pairs because they're basically aimed to be much the same

1:21:47.570,1:21:52.659
You know, they're very, you know, they're when one is higher the other is high and vice versa

1:21:55.690,1:22:02.129
So let's try removing one of each of these

1:22:04.590,1:22:09.949
Now it takes a little while to train a random forest and so for the just to see whether

1:22:10.620,1:22:15.260
Removing something makes it much worse. We could just do a very fast version

1:22:15.870,1:22:19.579
So we could just train something where we only have 50,000 rows

1:22:20.190,1:22:21.750
per tree

1:22:21.750,1:22:23.750
the train for each tree

1:22:24.360,1:22:26.360
And we'll just use 40

1:22:26.820,1:22:28.820
trees

1:22:30.120,1:22:34.439
And let's then just get the arrow B or

1:22:35.750,1:22:41.839
So for that fast simple version our basic arrow be with our important X's

1:22:43.230,1:22:45.230
0.87 7

1:22:46.550,1:22:48.800
And here our be a higher numbers better

1:22:49.790,1:22:55.129
So then let's try going through each of the things. We thought we might not need and try dropping them

1:22:56.500,1:23:04.359
Then getting the oob error or our exes with that one column removed and so compared to 877

1:23:05.530,1:23:07.119
most of them

1:23:07.119,1:23:08.980
Don't seem to hurt

1:23:08.980,1:23:10.329
Very much

1:23:10.329,1:23:13.589
They'll elapsed hurt quite a bit. Alright, so for each of those

1:23:15.870,1:23:21.570
Oops, let's go and see which one of the ones seems like we could remove it. But here's the five I found

1:23:24.040,1:23:25.700
Move the whole lot and

1:23:25.700,1:23:31.929
See what happens and so the oeob went from eight seven seven to eight seven four

1:23:32.390,1:23:34.390
There are hardly any difference at all

1:23:35.660,1:23:39.760
Despite the fact we managed to get rid of five about variables

1:23:41.040,1:23:47.630
So let's create something called X's final which is the X is important and then dropping those five

1:23:50.039,1:23:52.039
Save them for later

1:23:52.110,1:23:54.110
We can always load them back again

1:23:54.560,1:23:58.519
And then let's check our random forest using those and again

1:24:00.110,1:24:01.440
Point two three three

1:24:01.440,1:24:07.010
Point two three four, so we've got about the same thing, but we've got even less columns now

1:24:07.140,1:24:12.530
So we're getting a kind of a simpler and simpler model without hurting our accuracy is great

1:24:14.570,1:24:21.820
So the next thing we said we were interested in learning about is but the columns that are particularly the columns that are most important

1:24:22.780,1:24:24.850
How does what's the relationship?

1:24:25.429,1:24:31.658
Between that column and the dependent variable. Sorry, for example, what's the relationship between product size and sale price?

1:24:32.690,1:24:36.819
Now the first thing I would do would be just to look at a histogram

1:24:38.120,1:24:41.409
So one way to do that is with a value counts in

1:24:42.650,1:24:44.330
pandas

1:24:44.330,1:24:46.330
and we can see here our

1:24:47.179,1:24:49.159
different

1:24:49.159,1:24:54.039
Levels of product size and one thing to note here is actually missing

1:24:55.469,1:25:01.319
Is actually the most common and then next most is compact and small and then mini is pretty funny

1:25:03.650,1:25:08.150
So we can do the same thing for a year made, I'm now four year made we can't just

1:25:09.960,1:25:14.810
See the the basic bar chart here we according to his premise not it's a partial

1:25:15.630,1:25:18.319
four year made, we actually need a histogram which

1:25:19.199,1:25:22.789
pandas has enough like this built in so we can just call histogram and

1:25:23.699,1:25:29.329
That nineteen fifty you remember we created it that's kind of this missing value thing, which used to be a thousand

1:25:29.760,1:25:31.760
but most of them seemed to have been

1:25:32.159,1:25:34.159
Well into the 90s to thousands

1:25:35.230,1:25:40.529
So let's now look at something called a partial dependence plot. I'll show it to you first

1:25:41.769,1:25:43.300
here is a

1:25:43.300,1:25:45.300
partial dependence plot

1:25:46.740,1:25:48.430
Of

1:25:48.430,1:25:50.430
Ear made

1:25:51.370,1:25:53.370
Against national dependence

1:25:54.699,1:26:00.158
What does this mean? Well, we should focus on the part where we actually have a reasonable end of data

1:26:00.159,1:26:02.379
So at least well into the 80s

1:26:03.650,1:26:10.719
Around here. And so let's look at this bit here. Basically what this says, is that as year made?

1:26:11.809,1:26:13.739
increases

1:26:13.739,1:26:15.739
the predicted

1:26:17.040,1:26:20.299
Sale price log sale price, of course also increases

1:26:21.420,1:26:26.489
You can see and the log sale price is increasing linearly on other roughly

1:26:26.489,1:26:31.409
So roughly then this is actually an exponential relationship between year made

1:26:32.230,1:26:34.230
And sale price. Um

1:26:35.440,1:26:41.129
Why do we call it a partial dependence so we're just plotting a kind of the year against the average sale price

1:26:41.650,1:26:45.089
well, no, we're not we can't do that because a

1:26:45.850,1:26:47.850
Lot of other things change from year to year

1:26:49.440,1:26:50.850
Example

1:26:50.850,1:26:53.269
Maybe more recently people tend to buy

1:26:53.940,1:26:55.890
bigger bulldozers or

1:26:55.890,1:26:58.339
more bulldozers with air conditioning or

1:26:59.970,1:27:06.289
More expensive models of bulldozers and we really want to be able to say like know just what's the impact of a year and

1:27:06.480,1:27:09.470
nothing else and if you think about it from a kind of a

1:27:10.020,1:27:12.020
inflation point of view

1:27:12.030,1:27:14.030
You would expect that

1:27:15.570,1:27:18.710
Older bulldozers would be kind of

1:27:20.070,1:27:23.030
Like that bulldozers would get a kind of a constant ratio

1:27:26.250,1:27:32.750
Cheaper the further you go back which is what we see. So what we really want to say is all other things being equal

1:27:33.510,1:27:37.370
what happens if only the year changes and

1:27:38.070,1:27:41.900
There's a really cool way. We can answer that question with a random forest

1:27:42.900,1:27:46.400
So how does year made impact sale price all other things being equal?

1:27:48.449,1:27:54.319
What we can do is we can go into an actual data set and replace every single value in the year made column

1:27:54.870,1:28:00.289
with 1950 and then can calculate the predicted sale price for every single option and

1:28:00.540,1:28:05.509
then take the average over all the options and that's what gives us this value here and

1:28:06.150,1:28:13.909
Then we can do the same for 1951 19 2 and so forth until eventually we get to our final year of 2011

1:28:14.969,1:28:20.419
So this isolates the effect of only year made

1:28:23.139,1:28:29.228
So it's a kind of a bit of a curious thing to do but it's actually it's a pretty neat trick

1:28:29.229,1:28:32.529
But trying to kind of pull apart and create this

1:28:32.989,1:28:38.109
Partial dependence say what might be the impact of just changing your maid

1:28:40.820,1:28:42.820
And we can do the same thing for product size

1:28:42.990,1:28:47.329
And one of the interesting things if we do it for product sizes, we see that the lowest

1:28:48.180,1:28:50.180
value of

1:28:50.190,1:28:52.460
predicted sale price log sale price is

1:28:55.050,1:28:56.650
N/a

1:28:56.650,1:28:58.300
Which is a bit of a worry

1:28:58.300,1:29:04.619
because we kind of want to know well that means it's really important the question of whether or not the product size is labeled is

1:29:05.020,1:29:06.700
really important and

1:29:06.700,1:29:09.750
That is something that I would want to dig into before

1:29:09.750,1:29:15.119
I actually use this model to find out well, why is it that sometimes things aren't labeled and what does it mean?

1:29:15.120,1:29:17.610
you know, why is it that that's actually that's just

1:29:18.400,1:29:20.400
important predictor

1:29:21.239,1:29:25.819
So that is the partial dependence pot and it's so really pepper brick

1:29:29.369,1:29:35.598
So we have looked at four of the five questions we said we wanted to answer at the start of this section

1:29:36.950,1:29:39.609
so the last one that we want to answer is

1:29:41.300,1:29:44.259
One here we're predicting with a particular row of data

1:29:44.870,1:29:48.459
What were the most important factors and how did they influence that predict?

1:29:49.010,1:29:54.730
This is quite related to the very first thing we saw so it's like imagine you were using this

1:29:54.980,1:30:02.950
Auction price model in real life you had something on your tablet and you went into some auction and you looked up what the predicted

1:30:03.770,1:30:08.739
auction price would be for this lot that's coming up to find out whether

1:30:09.410,1:30:13.720
It seems like it's being under or overvalued and then you decide what to do about that

1:30:15.250,1:30:20.969
So one thing we said we'd be interested to know is like well are we actually confident in our prediction and

1:30:21.219,1:30:26.308
Then we might be curious to find out like oh, I'm really surprised. It was predicting such a high value

1:30:26.920,1:30:28.980
Why was it predicting such a high value?

1:30:30.199,1:30:32.749
So who find the answer to that question?

1:30:33.390,1:30:36.319
We can use a module cord tree interpreter

1:30:37.810,1:30:39.810
and trie interpreter

1:30:40.760,1:30:43.719
The way it works is that you pass in a

1:30:44.270,1:30:52.089
Single row so it's like here's the oxygen that's coming up. Here's the model. Here's the auctioneer ID cetera et cetera

1:30:53.090,1:30:55.090
please predict the

1:30:55.160,1:30:57.160
value

1:30:57.450,1:31:00.530
From the random forest. What's the expected sale price?

1:31:00.530,1:31:05.810
and then what we can do is we can take that one row of data and put it through the first decision tree and

1:31:06.360,1:31:10.880
We can see what's the first split that's selected and then based on that split

1:31:11.520,1:31:15.080
Does it end up increasing or decreasing the predicted?

1:31:15.630,1:31:18.650
price compared to that kind of raw baseline model

1:31:18.650,1:31:23.270
we've just take the average and then you can do that again at the next bit and again at the next bit and the

1:31:23.430,1:31:27.950
Expert so for each split we see what the increase or decrease in

1:31:28.530,1:31:30.530
the

1:31:30.660,1:31:37.340
Diction that's not right. We see what the increase or decrease in the prediction is

1:31:39.460,1:31:41.460
Except while I'm here

1:31:45.880,1:31:53.570
Compared to the parent node, and so then do that for every tree and then add up the total change and importance by variable

1:31:54.910,1:31:58.360
And that allows you to draw something like this

1:31:59.270,1:32:02.740
so here's something that's looking at one particular row of data and

1:32:03.890,1:32:05.890
overall

1:32:05.900,1:32:08.350
We start at zero and so 0 is

1:32:09.920,1:32:11.900
The initial

1:32:11.900,1:32:15.190
Endpoint 1 remember this number 10.1 is the average

1:32:16.040,1:32:22.749
Log sale price of the whole data set they call it the bias B interpreter. And so if we call that 0

1:32:23.330,1:32:26.019
Then for this particular row we're looking at

1:32:26.630,1:32:32.920
ear made as a negative for point to impact on the prediction and then

1:32:33.320,1:32:35.349
Product size has a positive point, too

1:32:37.100,1:32:42.620
Plus system has a positive 0.046 model ID has a positive 0.127

1:32:44.110,1:32:50.199
And so forth right and so the red ones are negative and the green ones are positive and you can see how they all join

1:32:50.199,1:32:52.199
Up until eventually overall

1:32:52.790,1:32:57.879
The prediction is that it's going to be negative point one to two compared to

1:32:58.880,1:33:00.619
ten point one

1:33:00.619,1:33:02.619
which is equal to

1:33:04.400,1:33:06.400
Point nine eight

1:33:06.670,1:33:08.949
So this kind of plot is called a waterfall plot

1:33:10.650,1:33:16.170
And so basically when we say tree interpret or predict it gives us back the

1:33:18.699,1:33:20.699
Prediction which

1:33:20.880,1:33:23.279
Number we get back from the random forest the bias

1:33:23.409,1:33:28.499
Which is just always this ten point one for this data set and then the contributions

1:33:29.320,1:33:35.489
Which is all of these different values. It's how much how important was each

1:33:36.370,1:33:39.749
factor and here I've used a threshold, which means

1:33:40.929,1:33:45.239
Anything that was less than 0.08 or gets thrown into this other

1:33:46.440,1:33:50.129
A degree. I think this is a really useful kind of thing to have in production

1:33:51.550,1:33:56.670
Because it can help you answer questions whether it will be to the customer or for you know

1:33:56.670,1:34:01.710
Whoever's using your model if they're surprised about some fiction. Why is that prediction?

1:34:05.949,1:34:07.949
So I'm going to show you something

1:34:08.260,1:34:10.179
really interesting

1:34:10.179,1:34:12.179
using some synthetic data and

1:34:12.429,1:34:14.429
I want you to really have a think about

1:34:14.559,1:34:16.239
Why this is happening

1:34:16.239,1:34:20.158
Before I tell you oh, and I pause the video if you're watching the video

1:34:20.679,1:34:27.058
When I get to that point, um, let's start by creating. Um, some synthetic data like so so we're going to grab

1:34:27.820,1:34:31.380
40 values evenly spaced between 0 and 20 and

1:34:32.050,1:34:34.739
Then we're just going to create the y equals x line

1:34:35.860,1:34:37.480
and add

1:34:37.480,1:34:39.280
some

1:34:39.280,1:34:42.030
normally distributed random jitter on that

1:34:42.820,1:34:44.619
Here's the scatter plot

1:34:44.619,1:34:49.259
So here's some data we want to try and predict if we're going to use a random forest

1:34:49.260,1:34:51.260
You know kind of a bit of a overkill here

1:34:53.559,1:34:58.259
Now in this case, we only have one independent variable

1:34:59.559,1:35:01.859
Scikit-learn expects is to have more than one

1:35:03.610,1:35:05.909
So we can use

1:35:06.820,1:35:13.559
Unscrews in pi torch to add that go from a shape of 40 in other words a vector with 40 elements

1:35:13.869,1:35:20.699
To a shape of 40 comma 1 in other words a matrix of 40 rows with one column, but this unscrews one

1:35:21.219,1:35:24.418
Means at a unit axis here

1:35:26.120,1:35:31.700
I don't use on squeeze very often because I actually generally prefer the index with a special value none

1:35:31.940,1:35:34.520
this works in pi torch and none pay and

1:35:35.340,1:35:42.409
This work the way it works is to say okay, excellent. Remember that size is a vector of length 40 every row and

1:35:43.260,1:35:50.570
Then none means insert a unit axis here on so these are two ways of doing the same thing

1:35:50.910,1:35:55.820
But this one is a little bit more flexible. So that's what I use more often. But now that we've got

1:35:56.640,1:36:03.349
The shape that is expected which is a back to tensor and a row or an array with two dimensions or axes

1:36:03.690,1:36:05.430
We can create a random forest

1:36:05.430,1:36:11.450
We can fit it and let's just use the first 30 data points and it's so kind of top here

1:36:13.590,1:36:19.349
And then let's do a prediction, all right, so let's plot the original data points and then also plot a prediction

1:36:19.869,1:36:23.879
and look what happens on the prediction it acts it's kind of nice and accurate and

1:36:24.489,1:36:25.869
then suddenly

1:36:25.869,1:36:31.499
What happens? Yes, this is the bit where if you watch in the video, I want you to pause and never think biases flat

1:36:34.000,1:36:36.060
So what's going on here

1:36:36.179,1:36:36.449
well

1:36:36.449,1:36:41.219
remember a random forest is just taking the average of predictions of a bunch of trees and

1:36:41.409,1:36:43.859
a tree the prediction of a tree is just

1:36:44.739,1:36:47.669
The average of the values in a leaf node

1:36:48.940,1:36:52.989
And remember we fitted using a training set containing only the first 30

1:36:54.850,1:37:00.749
So none of these appeared in the training set, so the highest we could get would be the average of

1:37:01.480,1:37:06.659
values that are inside the training set in other words says this maximum we can get to

1:37:07.210,1:37:10.620
So random forests cannot extrapolate

1:37:11.679,1:37:14.908
outside of the bounds of the data that they've training set

1:37:14.909,1:37:20.609
This is going to be a huge problem for things like time series prediction where there's like an underlying trend for instance

1:37:21.810,1:37:23.810
but really

1:37:24.160,1:37:26.889
It's more about general issue than just time variables

1:37:27.230,1:37:29.180
it's going to be hard for rent or

1:37:29.180,1:37:35.320
Impossible often for random for us to just extrapolate outside the types of data that it's seen in a general sense

1:37:36.110,1:37:39.939
So we need to make sure that our validation set not contain

1:37:40.880,1:37:42.880
output domain data

1:37:43.790,1:37:45.790
So, how do we find out of domain data?

1:37:47.020,1:37:50.249
So we might not even know if our test set is

1:37:51.160,1:37:56.400
Distributed in the same way as our training data, so if they're from two different time periods, how do you kind of tell?

1:37:57.040,1:38:04.109
How they vary right or if it's a cattle competition. How do you tell if the test set and the training set?

1:38:04.110,1:38:07.319
Which Cargill gives you have some underlying differences?

1:38:09.300,1:38:13.920
Is actually a cool trick you can do which is you can create a column chord is valid

1:38:15.590,1:38:22.219
Contains zero for everything in the training set and one for everything in the validations that

1:38:23.510,1:38:27.110
It's concatenating all of the independent variables together

1:38:27.110,1:38:31.940
So it's so so it's coordinating the independent variables for both the training and validation set together

1:38:33.260,1:38:35.260
So this is our independent variable

1:38:36.060,1:38:41.609
And this becomes our dependent variable and we're going to create a random forest, not for predicting price

1:38:43.099,1:38:48.889
But a random forest that predicts is this row from the validation set or the training set

1:38:50.409,1:38:55.719
The validation set in the training set are from kind of the same distribution if they're not different

1:38:55.790,1:38:59.139
Then this random forest should basically have zero

1:38:59.960,1:39:01.960
predictive power

1:39:02.169,1:39:03.949
If it has any predictive power

1:39:03.949,1:39:10.688
Then it means that our training and validations that are different and to find out the source of that difference we can use

1:39:11.689,1:39:13.689
feature importance

1:39:14.650,1:39:22.600
And so you can see here that the difference between the validation set and the training set

1:39:23.920,1:39:30.429
Is not surprisingly sale elapsed. So that's the number of days since I think like 1970 or something

1:39:30.650,1:39:32.050
So it's basically the date

1:39:32.050,1:39:33.040
Oh, yes, of course

1:39:33.040,1:39:35.560
you can predict whether something is in the

1:39:35.630,1:39:41.199
Validation set or the training set by looking at the date because that's actually how to find them. That makes sense

1:39:41.290,1:39:43.290
This is interesting sales ID

1:39:43.460,1:39:48.279
so it looks like the sales ID is not some random identifier, but it increases over time and

1:39:48.980,1:39:50.980
ditto for machine ID

1:39:52.530,1:39:55.590
And then there's some other smaller ones here that kind of makes sense

1:39:56.800,1:40:02.880
So I guess for something like model desk. I guess there are certain models that were only made in later years for instance

1:40:05.920,1:40:08.199
Can see these top three columns are a bit of an issue

1:40:10.389,1:40:18.249
So then we could say like okay what happens if we look at each one of those columns those first three and remove them

1:40:19.680,1:40:21.680
and then see

1:40:22.280,1:40:27.710
Now it changes our our MSC on our

1:40:30.960,1:40:36.140
Sales price model on the validation set we start from point two three two

1:40:37.810,1:40:39.670
Removing sales ID

1:40:39.670,1:40:41.670
Actually makes it a bit better

1:40:41.740,1:40:45.480
They'll elapsed makes it a bit worse machine ID about the same

1:40:45.790,1:40:51.150
But we can probably remove sales ID and machine ID without losing any accuracy and yep

1:40:51.150,1:40:58.920
it's actually slide improve but most importantly it's going to be more resilient over time, right because we're trying to remove the

1:40:59.770,1:41:01.770
time-related features

1:41:03.760,1:41:07.769
Another thing to note is that since it seems that you know

1:41:07.770,1:41:14.430
This kind of sale elapsed issue that maybe it's making a big difference is maybe looking at the sale year

1:41:15.760,1:41:17.680
Distribution this is the histogram

1:41:17.680,1:41:19.740
Most of the sales are in the last few years anyway

1:41:20.350,1:41:27.840
Now what happens if we only include the most recent few years, let's just include everything after 20 2004

1:41:29.420,1:41:31.420
So that is exes filtered

1:41:33.170,1:41:35.160
And if I trained on that subset

1:41:35.160,1:41:39.410
Then my accuracy goes improves a bit more from to be three one

1:41:40.080,1:41:41.699
three three Oh

1:41:41.699,1:41:43.699
So that's interesting, right?

1:41:44.040,1:41:51.379
We're actually using less data less rows and getting a slightly better result because the more recent data is more representative

1:41:53.750,1:42:00.470
So that's about as far as we can get with our random forest, but what I will say is this

1:42:01.739,1:42:03.739
this issue of extrapolation

1:42:04.999,1:42:06.630
Would not happen

1:42:06.630,1:42:12.469
with a neuro net but it because a neural net is using the kind of the underlying layers are linear layers and

1:42:12.749,1:42:15.289
So linear layers can absolutely extrapolate

1:42:16.170,1:42:19.399
So the obvious thing to think then at this point is well

1:42:19.400,1:42:27.319
Maybe but a neural net there a better job of this that's gonna be the thing next up to this question a question first

1:42:27.749,1:42:28.889
how do

1:42:28.889,1:42:32.089
How does feature importance relate to correlation?

1:42:36.410,1:42:39.639
Which importance doesn't particularly relate to a correlation

1:42:40.970,1:42:44.439
Correlation is a concept for linear models. And this is not a linear model

1:42:44.720,1:42:49.209
so remember feature importance is calculated by looking at the

1:42:51.260,1:42:57.339
Improvement in accuracy as you go down each tree and you go down each binary spit

1:43:00.150,1:43:06.900
If you're used to linear regression then I guess our relation sometimes can be used as a measure of

1:43:08.980,1:43:12.509
Importance but this is a much more kind of direct version

1:43:13.300,1:43:14.590
it's

1:43:14.590,1:43:16.330
taking account of these

1:43:16.330,1:43:19.890
Nonlinearities and interactions as well. So it's a much more

1:43:21.060,1:43:21.760
a

1:43:21.760,1:43:26.039
flexible and reliable measure and release picture importance

1:43:27.410,1:43:29.410
any more question

1:43:29.809,1:43:32.288
So do the same thing with a neural network

1:43:32.289,1:43:36.279
I'm going to just copy and paste the same lines of code that I had

1:43:36.530,1:43:41.650
from before but this time I call it n n D F n n and these are the same lines of code and

1:43:41.809,1:43:46.929
I'll grab the same list of columns we had before in the dependent variable to get the same data frame

1:43:49.510,1:43:55.289
Now as we've discussed four categorical columns, we probably want to use embeddings so to create embeddings

1:43:56.820,1:44:02.969
To know which columns should be treated as categorical variables and as we discussed we can use Conte cats bit for that

1:44:03.460,1:44:06.540
One of the useful things we can pass that is the maximum cardinality

1:44:08.079,1:44:13.559
So max card equals nine thousand means if there's a column with more than nine thousand levels

1:44:13.659,1:44:19.469
You should treat it as continuous and if it's got less than nine thousand levels rigid as categorical

1:44:19.810,1:44:26.999
So that's you know, it's a simple little function that just checks the cardinality and splits them based on how many discrete levels they have

1:44:27.760,1:44:30.179
and of course it their data type if it's not

1:44:30.969,1:44:33.269
Actually a numeric data type it has to be categorical

1:44:35.079,1:44:37.079
So there's our

1:44:37.429,1:44:39.429
There's a speed

1:44:40.969,1:44:42.969
And then

1:44:44.810,1:44:47.269
From there, what we can do is we can say oh

1:44:47.270,1:44:52.430
We've got to be a bit careful of sale lapsed because actually sale elapsed I think has less than nine thousand categories

1:44:52.590,1:44:55.580
But we definitely don't want to use that as a categorical variable

1:44:55.680,1:45:02.510
The whole point was to make it that this is something that we can extrapolate are we certainly anything? That's kind of I'm dependent

1:45:02.510,1:45:06.019
Oh, you think that we might see things outside the range of inputs?

1:45:06.930,1:45:08.160
in the

1:45:08.160,1:45:10.160
Training data we should make them

1:45:10.440,1:45:15.980
Continuous variables. So let's make sale elapse to put it in continuous neural net and remove it from

1:45:16.650,1:45:18.650
categorical

1:45:20.700,1:45:27.499
So here's the number of unique levels this is from pandas for everything in our neural net data set for the categorical variables and

1:45:28.110,1:45:30.620
I get a bit nervous when I see these really high numbers

1:45:31.140,1:45:34.939
So I don't want to have too many things with like lots and lots of categories

1:45:38.040,1:45:43.200
The reason I don't want lots of things with lots and lots of categories is just they got to take up a lot of parameters

1:45:43.570,1:45:48.299
Because you know embedding matrix. This is you know, every one of these is a row in an embedding mech in

1:45:48.910,1:45:54.119
This case I notice model ID and model desk might be describing something very similar

1:45:54.550,1:45:57.210
so it quite like to find out if I could get rid of one and

1:45:57.340,1:45:59.670
An easy way to do that would be to use a random forest

1:46:00.790,1:46:02.790
So let's try removing

1:46:03.250,1:46:04.390
the

1:46:04.390,1:46:06.190
model desk

1:46:06.190,1:46:07.990
and

1:46:07.990,1:46:09.990
Let's create a random forest

1:46:11.060,1:46:16.220
See what happens and oh, it's actually a tiny bit better and certainly not worse

1:46:16.350,1:46:21.229
So that suggests that we can actually get rid of one of these levels or one of these variables

1:46:22.330,1:46:24.330
So let's get rid of that one

1:46:24.929,1:46:27.569
So now we can create a tabular panda's object just like before

1:46:28.630,1:46:30.630
But this time we're going to add one more

1:46:31.510,1:46:33.510
processor which is normalized and

1:46:34.150,1:46:39.329
The reason we need normalize so normalize is subtract the mean divided by the standard deviation

1:46:40.840,1:46:43.690
We didn't need that for a random post because for a random forest

1:46:43.760,1:46:48.340
We're just looking at less than or greater then through our binary splits

1:46:48.620,1:46:54.220
So all that matters is the order of things how they're sorted doesn't matter whether they're super big or super small

1:46:54.890,1:46:59.349
But it definitely matters for neural nets because we have these linear layers

1:47:01.030,1:47:03.030
So we don't want to have

1:47:03.310,1:47:09.120
You know things with kind of crazy distributions with some super big numbers and super small numbers is it's not going to work

1:47:09.670,1:47:14.940
So it's always a good idea to normalize things here on it's where we can do that in a tabular

1:47:16.570,1:47:19.979
Zero net by using the normalize a typical approach

1:47:21.190,1:47:24.580
So we can do the same thing that we did before with creating our tabular pandas

1:47:25.340,1:47:27.340
That bill object the neural net

1:47:28.820,1:47:30.820
And then we can create data loaders

1:47:31.080,1:47:35.870
From that with a batch size and this is a large batch size because tabular models

1:47:36.120,1:47:39.560
don't generally require nearly as much CPU RAM as a

1:47:42.580,1:47:45.600
Convolutional neural net or something or an iron or something?

1:47:48.740,1:47:54.309
Since it regression model we're going to want our range. So let's find the minimum and maximum of our dependent variable

1:47:55.580,1:47:59.860
This and then we can now go ahead and create a tabular Lerner

1:48:01.080,1:48:04.680
But a temple alone is going to take outdated loaders our way range

1:48:05.980,1:48:07.820
How many?

1:48:07.820,1:48:13.600
Activations. Do you want in each of the linear layers? And so you can have as many linear layers as you like? Yeah

1:48:15.880,1:48:21.310
I'm how many outputs are there. This is a regression with a single output and what loss function do you want?

1:48:23.910,1:48:25.910
We can use LR find

1:48:26.840,1:48:29.029
And then we can go ahead and use pet one cycle

1:48:29.639,1:48:34.789
There's no pre trained model, obviously because this is not something where people have got pre train models for

1:48:36.989,1:48:40.309
Industrial equipment options, we just use fit one cycle and

1:48:41.610,1:48:43.610
train for a minute

1:48:45.179,1:48:49.549
We can check and our our MSE is 0.2

1:48:50.880,1:48:52.080
2x

1:48:52.080,1:48:56.059
Which here? It was 0.23. Oh, but that's amazing

1:48:56.060,1:49:02.479
We actually have you know straight away a better result and the random first. It's a little more fussy

1:49:02.480,1:49:04.480
It took something takes a little bit longer

1:49:04.530,1:49:06.799
and as you can see, you know for

1:49:07.980,1:49:10.970
Interesting data sets like this we can get some great results

1:49:12.469,1:49:14.469
with neural nets

1:49:19.349,1:49:21.349
So here's something else we could do

1:49:22.230,1:49:29.520
The random forest in the neural net. They each have their own pros and cons as something sacred atomism. They're less good

1:49:31.970,1:49:36.260
So maybe we can get the best of both worlds and a really easy way to do that is to use

1:49:37.110,1:49:41.059
Ensemble, we've already seen that a random forest is a decision tree ensemble

1:49:41.060,1:49:46.700
But how we can put that into another ensemble you can have an ensemble of the random forest and you're on it

1:49:47.520,1:49:49.410
There's lots of super fancy ways

1:49:49.410,1:49:53.599
You can do that had a really simple way is to take the average

1:49:53.880,1:49:57.619
so sum up the predictions from the two models by bow to

1:49:59.900,1:50:06.560
Use that as a prediction so that's our ensemble prediction is just literally the average of the random forest prediction and the neuron a prediction

1:50:07.989,1:50:11.678
And that gives us pointer to three

1:50:13.070,1:50:15.070
This is point two to six

1:50:15.739,1:50:17.739
So how good is that?

1:50:18.380,1:50:19.820
well

1:50:19.820,1:50:22.329
It's a little hard to say because unfortunately this

1:50:23.239,1:50:28.388
Competition is old enough that we can't even submit to it and find out how we would have gone on cackle

1:50:29.150,1:50:33.250
so we don't really know and so we're relying on our own validations that

1:50:33.770,1:50:39.039
But it's quite a bit better than even the first place score on the test set

1:50:40.550,1:50:42.940
So if the validation set is

1:50:43.550,1:50:44.690
You know

1:50:44.690,1:50:49.449
Doing a good job. Then this is a good sign that this is a really really good model

1:50:50.239,1:50:56.948
which wouldn't necessarily be that surprising, um, because you know in the last few years I

1:50:57.739,1:50:59.659
guess we've learned a lot about

1:50:59.659,1:51:01.659
building these kinds of models and

1:51:02.150,1:51:08.259
we're kind of taking advantage of a lot of the tricks that have that have appeared in recent years and

1:51:09.349,1:51:11.769
Yeah, maybe this goes to show that well

1:51:11.770,1:51:18.999
I think it certainly goes to show at both random forests and neural nets have a lot offer and

1:51:20.480,1:51:23.500
Try both and maybe even find both

1:51:28.240,1:51:31.260
We've talked about an approach to ensemble in court bagging

1:51:31.780,1:51:37.439
which is where we train lots of models on different subsets of the data like the average of

1:51:38.760,1:51:43.610
Another approach to ensemble in to Cleon fumbling of trees is called boosting

1:51:44.980,1:51:47.259
and boosting involves training a

1:51:47.930,1:51:54.999
Small model which under fits your data set, but maybe like just have a very small number of if notes

1:51:56.469,1:51:58.868
Then you calculate the predictions using the small model

1:52:00.430,1:52:02.979
And then you subtract the predictions from the targets

1:52:02.980,1:52:08.230
So these are kind of like the errors of your small under fit model. We call them residual

1:52:09.099,1:52:10.219
and

1:52:10.219,1:52:14.529
Then go back to step one, but now instead of using the original targets

1:52:15.079,1:52:21.518
Use the residuals the Train a small model which under fits your dataset attempting to predict

1:52:22.099,1:52:24.099
residuals

1:52:24.670,1:52:30.339
Again in in until you reach some stopping criterion such as the maximum number of trees

1:52:31.310,1:52:36.699
Now you that will leave you with a bunch of models, which you don't average

1:52:37.670,1:52:39.670
But which use some?

1:52:39.830,1:52:45.009
Because each one is creating a model that's based on the residual of the previous one

1:52:45.010,1:52:49.780
So we've subtracted the predictions of each new tree from the residuals of the previous tree

1:52:50.180,1:52:54.460
But the residuals get smaller and smaller and then to make predictions

1:52:54.460,1:52:57.460
We just have to do the opposite which is to add them all together

1:52:59.280,1:53:01.280
So there's lots of variants of this

1:53:04.320,1:53:10.820
But you'll see things like gbms or gradient boosted machines or GB TTS for gradient boosted decision trees

1:53:11.610,1:53:13.610
and there's lots and

1:53:13.860,1:53:18.619
Minor details surround, you know, and they're insignificant details

1:53:18.619,1:53:24.439
But the basic idea is is what I've shown to question. All right, let's take the questions

1:53:24.960,1:53:30.619
They're dropping features in a model as a way to reduce the complexity of the model and thus reduce overfitting

1:53:30.619,1:53:35.929
Is this better than adding some regularization like weight decay a Bueller? I

1:53:39.000,1:53:43.250
Didn't claim that removed columns to avoid overfitting

1:53:48.030,1:53:55.670
We remove the columns to simplify here are things analyze and then

1:53:58.640,1:54:04.479
It should also mean we don't need as many trees but there's no particular reason to believe that this will regular raise and

1:54:05.630,1:54:12.370
Well, the idea of regularization doesn't necessarily make a lot of sense to random forests and always add more trees

1:54:13.380,1:54:14.530
is

1:54:14.530,1:54:19.619
There a good heuristic for picking the number of linear layers in the tabular model

1:54:22.080,1:54:25.280
Not really well if there is I don't know what it is

1:54:28.550,1:54:30.430
I guess

1:54:30.430,1:54:32.260
sir

1:54:32.260,1:54:34.260
Three hidden layers works pretty well

1:54:35.150,1:54:37.150
So, you know what I showed

1:54:37.160,1:54:39.160
Those numbers I showed her

1:54:40.119,1:54:42.458
Pretty good for a large ish model

1:54:44.370,1:54:48.960
A default is as 200 and 100. So maybe start with the default and then got

1:54:49.510,1:54:53.699
502 50 if that ends an improvement and like just keep doubling them

1:54:54.580,1:54:57.510
And ups improving or you run out of memory odd, huh?

1:55:00.150,1:55:06.290
Anything to note about boosted models is that there's nothing to stop us from overfitting you add more and more trees

1:55:06.960,1:55:14.270
The bagging model sort of a random forest. It's going to get going to generalize better and better as your each time

1:55:14.270,1:55:17.870
You're using your model, which is based on a subset of the day

1:55:19.770,1:55:21.770
But boosting

1:55:21.960,1:55:24.930
Each model will fit the training set better and better

1:55:26.050,1:55:28.860
virtually over fit or in law, so

1:55:29.710,1:55:34.500
roasting methods do require journaling I parameter tuning and fiddling around with

1:55:36.250,1:55:38.250
You know you certainly have regularization

1:55:40.220,1:55:48.139
Listing they're pretty sensitive to their hyper parameters, which is why they're not normally much first quarter

1:55:50.980,1:55:54.730
They they often they more often when cattle competition

1:55:55.219,1:55:59.919
Random forests do like they tend to be good at getting that last little bit of performance

1:56:03.820,1:56:09.329
But the last thing I'm going to mention is something super neat which a lot of people don't seem to know

1:56:10.300,1:56:15.779
Exists. There's a Shang's at super core, which is something from the entity embeddings paper

1:56:16.330,1:56:21.150
The table from it where what they did was they built a neural network?

1:56:22.090,1:56:24.299
they got the entity embeddings ee

1:56:26.010,1:56:33.179
And then they try to random forest using the entity embedding as predictors

1:56:36.829,1:56:44.829
The approach I described with just the Royal categorical variables and the the error for a random forest went from

1:56:45.349,1:56:46.639
0.16

1:56:46.639,1:56:49.299
0.11 a huge improvement and

1:56:50.059,1:56:58.029
Very simple method K and N and from point two nine to 0.1 one basically all of the methods when they used entity embeddings

1:56:58.849,1:57:00.849
Suddenly improved a lot

1:57:01.070,1:57:06.730
the one thing you you should try if you have a look at the further research section after the questionnaire is

1:57:07.159,1:57:08.630
it asks to try to

1:57:08.630,1:57:14.019
Do this actually take those entity embeddings that we trained in the neural net and use them in the random post

1:57:14.300,1:57:18.730
and then maybe try on some bowling again and see if you can beat the

1:57:19.940,1:57:22.359
point root to three

1:57:23.210,1:57:25.989
That we had this is a really nice idea

1:57:25.989,1:57:31.328
It's like you get you know, all the benefits of we've boosted decision trees

1:57:32.239,1:57:35.589
but all of the nice features of entity embeddings

1:57:36.920,1:57:43.269
And so this is something that not enough people tend to be playing with for some reason

1:57:44.969,1:57:46.969
So overall

1:57:47.729,1:57:55.069
You know random forests are nice and easy to train, you know, they're very resilient they don't require much pre-processing they're trained quickly

1:57:55.070,1:57:57.070
They don't over fit

1:57:58.679,1:58:01.128
You know, they can be a little less accurate and

1:58:02.550,1:58:09.019
They can be a bit slow at inference time because for inference you have to go through every one of those trees

1:58:09.570,1:58:13.879
Having said that a binary tree and be pretty heavily optimized

1:58:15.360,1:58:17.360
though

1:58:18.370,1:58:25.870
You know, it is something you can basically create a totally compiled version of a tree and they can certainly also be done entirely

1:58:27.680,1:58:29.680
In parallel

1:58:30.100,1:58:33.289
So that's something to consider. Um

1:58:35.150,1:58:41.260
Radiant boosting machines are also fast to train on the hole but a little more fussy about hyper parameters

1:58:41.260,1:58:43.260
You have to be careful about overfitting

1:58:43.430,1:58:45.430
But get more accurate

1:58:47.270,1:58:52.749
Neural nets may be the fussiest to deal with they've kind of got the least

1:58:54.530,1:58:58.089
Rules of thumb around or tutorials around saying this is and how to do it

1:58:58.090,1:59:00.819
It's just a bit a bit newer a little bit less. Well, understood

1:59:02.180,1:59:04.809
But they can give better results in many

1:59:04.910,1:59:09.609
Situations than the other two approaches or at least with an ensemble can improve the other two approaches?

1:59:10.250,1:59:12.250
So I would always start with a random fire

1:59:12.680,1:59:15.550
And then see if you can beat it using these

1:59:17.679,1:59:23.668
So, yeah, why don't you now see if you can find a cable competition with tabular data whether it's running now

1:59:23.669,1:59:26.398
It's a past one and see if you can repeat this process

1:59:26.769,1:59:32.849
For that and see if you can get in the top and percent if the private leaderboard that would be a really great

1:59:33.429,1:59:35.429
stretch goal at this point

1:59:37.300,1:59:40.229
Implement the decision tree algorithm yourself. I think that's an important one

1:59:40.269,1:59:47.128
Do you really understand it and then from there create your own random forests from scratch. You might be surprised. It's not that hard

1:59:51.929,1:59:58.558
Look at the tabular model source code and at this point, this is pretty exciting. You should find you pretty much though

1:59:58.630,2:00:01.380
What all the lines do with two exceptions?

2:00:02.199,2:00:06.509
And if you don't, you know dig around and explore an experiment and see if you can figure it out

2:00:07.840,2:00:09.840
And with that we are

2:00:11.540,2:00:18.169
I am very excited to say at a point where we've really dug all the way into the end of these

2:00:19.459,2:00:21.679
real valuable effective

2:00:22.349,2:00:25.909
Fast AI applications and we're understanding what's going on inside them

2:00:26.760,2:00:28.760
What should we expect for next week?

2:00:29.790,2:00:35.100
Oh next week we will at NLP and division

2:00:36.440,2:00:39.680
The same kind of idea is no the old deep to see what's going on

2:00:43.600,2:00:45.660
Thanks everybody see you next week
