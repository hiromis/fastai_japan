https://youtu.be/krIVOb23EH8


[0:00:00] barnacl (Status: Not Started)
Welcome to lesson five and we'll be talking about ethics for data science and this corresponds to chapter 3 of the book. I've also Just taught a six-week version of this course, I'm currently teaching an eight-week version of this course and will release some some combination or subset of that as a Fast AI and USF ethics for data science class if you want more detail Coming in July. I am Rachel Thomas I am the founding director of the center for applied data ethics at the University of San Francisco and also co-founder a fast day I together with Jeremy Howard my background I have a PhD in math and worked as a data scientist and software engineer in the tech industry and then have been working at USF and on fast AI for The past four years now So ethics issues are in the news These these articles I think are all from this fall kind of showing up at this this intersection of how How technology is impacting our world in many kind of increasingly powerful ways many of which really raised raised concerns and I want to start by talking about three cases that I hope everyone working in technology knows about and is on the lookout for So even if you only watch five minutes of this video, these are kind of the the three three cases I want you to see and one is feedback loops and so feedback loops can occur whenever your model is controlling the next round of data you get So the data that's returned quickly becomes flawed by the software itself. And this this can show up in many places One examples with recommendation systems and so recommendation systems are ostensibly about predicting what content the user will like But they're also determining what content the user is even exposed to and helping determine what what has a chance of becoming popular and so YouTube has gotten a lot of a lot of attention about this for kind of Highly recommending many conspiracy theories many kind of very damaging conspiracy theories. There is also They've kind of put together recommendations of paedophilia picked out of what we're kind of in a home movies but when are kind of strung together ones that happen to have young girls and bathing suits or in their pajamas So there's some really really concerning Results and this is not something that any anybody intended and we'll talk about this more later Um, I think particularly for many of us coming from a science background we're often used to thinking of like oh you know like we observe the data but really whenever you're building products that interact with the real world you're also Kind of controlling what the data looks like Second a second case study. I want everyone to know about it comes from Software that's used to determine poor people's health benefits It's used in over half of the 50 states and the verge did an investigation on what happened when it was rolled out in Arkansas and what happened is there was a bug and the software implementation that incorrectly cut coverage for people with cerebral palsy or diabetes Including Tammy Dobbs who's pictured here and was interviewed in the article and so these are people that really needed this health care and it was a roni roni ously cut due to this bug and so they were really and they couldn't get any sort of explanation and there was no appeals or recourse process in place and Eventually, this all came out through a lengthy court case But it's something where it caused a lot of a lot of suffering in the meantime And so it's really important to implement systems with a way to identify and address mistakes and to do that quickly in a way that hopefully minimizes damage because we all know software can have bugs our Code can behave in unexpected ways and we need to be prepared for that. I Wrote more about this idea in a post two years ago what HBR gets wrong about algorithms and bias and then third case study that everyone should know about so this is Latanya Sweeney who's director of the data privacy lab at Harvard has a PhD in computer science and She noticed several years ago that when you google her name you would get these ads saying Latanya, Sweeney Arrested implying that she has a criminal record. She's the only Latanya Sweeney and has never been arrested She paid $50 to the background check company and confirmed that she's never been arrested She tried googling some other names and she noticed rigs an example Kristen Lindquist Got much more neutral ads that just say we found Kristen Lindquist Even though Kristen Lindquist has been arrested three times and so being a computer scientist. Dr Sweeney to study this very systematically she looked at over 2,000 names and found that this pattern held in which disproportionately African American names were getting these ads suggesting that the person had a criminal record regardless of whether they did and

[0:05:04] kushaj (Status: Not Started)
traditionally a European American or white names were getting more neutral ads and This problem of kind of bias in advertising shows up a ton advertising is kind of the Profit model for most of the major tech platforms and it kind of continues to pop up in high-impact ways Just last year there was research showing how Facebook's ad system discriminates Even when the person placing the ad is not trying to do so so for instance the same housing ad exact same text if you change the Photo between a white family and a black family. It's served two very different audiences And so this is something that can really impact people when they're looking for housing when they're applying for jobs And is a definite area of concern So now I want to kind of step back and ask why why does this matter And so a very kind of extreme extreme example, it's just that data collection has played a pivotal role in several genocides including including the Holocaust and so this is a photo of Adolf Hitler meeting with the CEO of IBM at the time. I think this photo was taken in 1937 an IBM Continued to partner with the Nazis kind of long past when many other companies broke their ties They produced computers that were used in concentration camps to code other people were Jewish how they were executed and this is also Different from now where you might sell somebody a computer and they never hear from them again these machines require a lot of maintenance some kind of ongoing relationship with vendors to Kind of upkeep and repair them and it's something that a Swiss judge ruled It does not seem unreasonable to deduce that IBM's technical assistance facilitated the task of the Nazis in the commission of their crimes against humanity acts also involving accountancy and Classification by IBM machines and utilized and the concentration camps themselves. I'm told that they haven't gotten around to apologizing yet. Oh I guess they've been busy. Terrible - yeah Okay Yeah, and so this is a very kind of a very sobering example But I think it's important to keep in mind kind of what can go wrong and how technology can be used for for harm For very very terrible harm And so this just kind of raises a question questions that we all need to grapple with of how would you feel if you discovered that you had been part of a system that Ended up hurting society. Would you would you even know would you be open to finding out? Kind of how how things you had built may have been harmful And how can you help make sure this doesn't happen. And so I think these are questions that we all all need to grapple with It's also important to think about unintended consequences on how your tech could be used or misused whether that's by harass sirs by authoritarian governments for propaganda or disinformation and Then on a kind of a more concrete level you could even end up in jail And so there was a Volkswagen engineer who got prison time For his role in the diesel cheating case so you remember this is where Volkswagen was cheating on emissions test and one of the kind of Programmers that was a part of that and that person was just following orders from what their boss told them to do, but that is not Not a good excuse for for doing something that's unethical And so something to be aware of So ethics is the the discipline in dealing with what's good and bad it's a set of moral principles It's not a set of answers But it's kind of learning what sort of what sort of questions to ask and even how to weigh these decisions And I'll say some more about Kind of ethical foundations and different ethical philosophies later later on in this lesson But first I'm going to kind of start with some some use cases Ethics is not the same as religion laws social norms or feelings. Although it does have overlap with all these things It's not a fixed set of rules It's well founded standards are right and wrong and this is something we're Clearly not everybody agrees on the ethical action in in every case But that doesn't mean that kind of anything goes or that all actions are considered Equally ethical there are many things that are widely agreed upon and there are kind of a philosophically Safa chol underpinnings for kind of making these decisions and I think this is also the ongoing study and development of our ethical standards It's a kind of never-ending process of learning to kind of practice our ethical wisdom And I'm gonna refer it several times to so here I'm referring to a few articles from the Markkula Center for tech X at Santa Clara University

[0:10:06] ram_cse (Status: Not Started)
In particular the work of Shannon valour Brian Greene and arena Ray COO who is fantastic and they have a lot of resources Some of which I'll circle back to later Later in this talk. I spent years of my life studying ethics. It was my major at University and How much time on the question of what is ethics? I think I'll take away from that is studying the philosophy ethics was not particularly helpful in Learning about ethics. Yes, and I will try to keep this kind of very very applied and very practical Also, very kind of tech industry-specific of what what do you need in terms of applied ethics? Mykola said it's great They somehow they take stuff that I thought was super dry and turn it into useful checklists and things I Did want to note this was really neat so Casey fees lawyers a professor at University of Colorado that I really admire and She created a crowd-sourced spreadsheet of tech ethics syllabi This was maybe two years ago and got over 200 syllabi entered into this this crowd-sourced spreadsheet and then she did a meta-analysis on them of kind of looking at all sorts of aspects of the syllabi and what's being taught and how it's being taught and And published a paper on it. What do we teach when we teach technics and a few interesting things about it? Is it raises there a lot of? ongoing discussions and lack of agreement on how to how to best teach technics Should it be a standalone course versus worked into every course in the curriculum? who should teach it a computer scientist a philosopher a sociologist and And she analyzed for the syllabi What was the course home in the instructor home? And you can see that the the instructors came from a range of courses? including computer a range of disciplines computer science information science philosophy science and tech studies engineering law math business What topics to cover a huge range of topics that can be covered? Including a law and policy privacy and surveillance in equality justice and human rights Environmental impact AI and robots and professional ethics work in labor cybersecurity The list goes on and on and so this is clearly more than can be covered in any even a full semester length course and certainly not in a kind of a single single lecture What learning outcomes this is an area where there's a little bit more Agreement, we're kind of the number one skill that courses were trying to teach was critique followed by spotting issues making arguments and so a lot of this is just even learning to spot what the issues are and how to critically evaluate Kind of a piece of technology or a design proposal to see you know, what could go wrong what the the risks could be All right So so we're gonna go through kind of a few different Core topics and as I suggested this is gonna be a kind of extreme subset of what could be covered I was trying to pick things that we think are very important and high-impact So one is read course and accountability so I already shared this example earlier of You know the system that just determining poor people's healthcare benefits having a bug And something that was kind of terrible about this. Nobody took responsibility even once the bug was found so the creator of the algorithm was interviewed and Asked they asked him you know should people be able to get an explanation for why their benefits have been cut and he gave this very callous answer of You know Yeah, they probably should but I should probably dust under my bed, you know Like who's gonna do that which is very callous and then he ended up blaming the policymakers For how they had rolled out the algorithm the policymakers You know could blame the software engineers that implemented it and so there was a lot of passing the buck here Dana Boyd has said that You know, it's always been a challenge for Bureaucracy to assign responsibility or bureaucracy is used to evade responsibility and today's algorithmic systems are often extending bureaucracy a Couple of questions and comments about Cultural context yeah of any notes that they didn't seem to be any mention of cultural contexts for ethics as part of those Syllabi, and somebody else was asking How do you deal you know is this culturally dependent? And how do you deal with that? And it is culturally dependent? I will mention this briefly later on so I'm gonna share three different ethical philosophies that are kind of from the west and we'll talk just Briefly have one slide on for instance right now

[0:14:59] sandeepsign (Status: Not Started)
There are a number of indigenous data sovereignty movements. And I know the Maori data sovereignty movement has been particularly active But different, you know different cultures do have different views on ethics. And I think that the cultural context is incredibly important And we will not get into it tonight But there's also kind of a growing field of algorithmic colonialism and kind of studying. What are some of the issues when you have Technologies built in one, you know particular country and culture being implemented, you know Halfway across the world in very different cultural context often with little to no input from people people living in that culture and Although I do want to say that there are things that are Widely, although not universally agreed on and so for instance the Universal Declaration on human rights despite the name it is not Universally accepted but many many different countries have accepted that as a human rights framework and as those being fundamental rights and so there are kind of principles that are often held cross culturally although Yeah, it's rare for something probably to be truly truly universal So returning to this topic of kind of accountability in recourse Keep in mind is the data contains heirs. And so there was a dank database used in California It's tracking supposedly gang members and an auditor found that there were 42 babies under the age of 1 who had been entered into this database and Something concerning about the database is that it's basically never updated I mean people are added but they're not removed and so once you're in there you're in there and And 28 of those babies were marked as having admitted to being gang members and so keep in mind that this is just a really obvious example of the air, but how many other kind of Totally wrong entries are there another example of data containing errors involves the the three credit bureaus in the United States the FTC's large-scale study of credit reports found that 26% had at least one mistake in their files and 5% had errors that could be devastating. I'm gonna this is the headline of an article that was written by a public radio reporter who went to get a an apartment and the landlord called him back afterwards and said You know your background check showed up that you had firearms convictions and This person did not have any firearms convictions and it's something where in most cases the landlord would probably not even tell Tell you and let you know, that's why you weren't getting the apartment. And so and This guy looked into it I should note that this guy was white, which I'm sure helped him in getting the benefit of the doubt and found this ere and He made Dozens of calls and could not get it fixed Until he told them that he was a reporter and that he was going to be writing about it Which is something that most of us would not be able to do But it was even once he had pinpointed the air and he had to you know, talk to The you know like County Clerk and the place he used to live It was still a very difficult process to get it updated and this can have a huge huge impact on people's lives There's also the issue of when technology is used in ways that the creators may not have intended so for instance with facial recognition It is pretty much entirely being developed for adults yet NYPD is putting the photos of children as young as age 11 into into databases And we know the error rates are higher. This is not how it was developed So this is this is a serious serious concern And there are a number of kind of misuses the Georgetown Center for privacy and Technology, which is fantastic You should definitely be following them Did a report garbage in garbage out looking at how police we're using facial recognition and practice and they found some really concerning examples for instance in one case NYPD had a photo of a suspect and they Wasn't returning any matches and they said well this person kind of looks like Woody Harrelson so then they googled the actor Woody Harrelson and put his face into the Facial recognition and use that to generate leads And this is clearly not the correct you said all but it's it's a way that it's being it's being used And so there's kind of total lack of accountability here And then another kind of study of cases in all 50 states of police officers kind of abusing confidential databases to look up X romantic partners or to look up activists and so You know here this is not necessarily an and the data, although that can be present as well But kind of keeping in mind how it can be misused by the users All right

[0:20:08] pnvijay (Status: Not Started)
the next topic is feedback loops and metrics and so I talked a bit about feedback loops in the beginning is kind of one of one of the three key use cases And so this is a topic. I wrote a blog post about this fall The problem with metrics is a big problem for AI and then together with David you Minsky who's director of the data Institute Expanded this into a paper Reliance on metrics is a fundamental challenge for AI and this was accepted to the ethics and data science conference but over emphasizing metrics can lead to a number of problems including manipulation gaming myopic focus on short-term goals because it's easier to track short-term quantities unexpected negative consequences and Much of AI and machine learning centers on optimizing a metric This is kind of both, you know, the strength of machine learning is it's gotten really really good at optimizing metrics But I think this is also kind of inherently a weakness or a limitation I'm going to give a few examples and this can happen even not just in machine learning kind of but in analog Examples as well So this is from a study of when English is England's public health system implemented a lot more targets Around numbers in the early 2000s and the study was called. What's what's measured is what matters? And so they found so one of the targets was around reducing ER wait times, which seems like a good goal however, this led to cancelling scheduled operations to draft extra staff into the ER So if they felt like there were too many people that you are they were just start canceling operations so they could get more doctors Requiring patients to wait in queues of ambulances because time waiting an ambulance didn't count towards your your er wait time Turning stretchers into beds by putting them in hallways I mean, there are also big discrepancies in the numbers reported by hospitals versus by patients And so if you ask the hospital on average how long people waiting You get a very different answer than when you're asking the patients. How long did you have to wait? Another another example is of SAE grading software And so this essay grading software I believe is being used in 22 states now in the United States Yes, 20 states and it tends to focus on metrics like sentence length vocabulary spelling subject verb agreement because these are the things that we we know how to measure and how to measure with a computer but I can't evaluate things like creativity or novelty However, gibberish essays with lots of sophisticated words score well, and there are even examples of people Creating computer programs to generate these kind of gibberish sophisticated essays and then there you know graded by this other computer program and highly rated and There's also bias in this essays by african-american students received lower grades from the computer than from expert human graders and essays by students from mainland China received higher scores from the computer than from expert human graders and authors of the study thought that they this this results suggest they may be using chunks of pre memorized text that score well, And this is these are just kind of two examples I have a bunch more in the blog post and even more in the paper of ways that metrics can invite manipulation and gaming whenever they're they're given a lot of emphasis and this is a good hearts laws of kind of a law that a lot of people talk about and it's this idea that the More you rely on a metric the kind of the less reliable it becomes So returning to this example of feedback loops and recommendation systems Guillaume has lot is a former google, youtube engineer YouTube is owned by Google and He wrote a really great plant post and he's done a ton to raise awareness about this issue and founded the nonprofit I'll go transparency which kind of externally tries to monitor YouTube's recommendations He's partnered with the Guardian and the Wall Street Journal to do investigations But he wrote a post around how have been though in the earlier days the recommendation system was designed to maximize watch time and So and this is this is something else that's often going on with metrics. Is that Any metric is just a proxy for what you truly care about. And so Here, you know the team at Google was saying well, you know, if you're watching more YouTube it signals to to us that they're happier however, this also ends up incentivizing content that tells you the rest of the media is lying because Kind of believing that everybody else is lying. We'll encourage you to spend more time on a particular platform So Gilmer at a great post about this Kind of mechanism that's at play and you know, this is not just YouTube. This is any recommendation system. Could I think be Susceptible to this and there have been a lot of talk about kind of issues with many recommendation systems across platforms

[0:25:33] wyquek (Status: Not Started)
But it is it is something to be mindful of and something that the kind of creators of this did not anticipate And last year um kind of gathered this data on So here the x-axis is the number of channels Number of YouTube channels recommending a video and the y-axis is the log of the views and we see this extreme outlier Which was Russia's today take Russia. Today's take on the Mueller report, and this is something that Guillaume observed and then was picked up by the the Washington Post. But this this strongly suggests that Russia today has perhaps gamed the the Recommendation algorithm which is which is not surprising and it's something that I think many content creators are conscious of and trying to you know experiment and see what what gets more heavily recommended and thus more views so it's important to note that our online environments are designed to be addictive and so when kind of what we click on is often used as a proxy of of What we enjoy or what we like that's not necessarily though four of our kind of like our best selves or our higher selves It's you know, it's what we're clicking on in this kind of highly addictive environment that's often appealing to Some of our kind of lower instincts saying up to effect. She uses the analogy of a cafeteria. That's kind of shoving salty sugary Fatty foods in our faces and then learning that hey people really like salty sugary fatty foods Which I think most of us do in a kind of very primal way, but we often you know kind of our higher self is like oh I don't want to be eating junk food all the time and online we often kind of don't have a great mechanisms to say, you know like oh I Really want to read like more long-form articles that took months to research and are gonna take a long time to digest While we may want to do that our online environments or not. I'm not always conducive to it Yes ceramic their comment about the false sense of security argument, which is very relevant Hoff's, did you have anything to say about its false sense of security? Can you say more There's a Common feedback at the moment that people Don't wear masks because they're my little sense of security That kind of makes sense to you I don't think that's a good argument at all In general There's so many other people including Jeremy have pointed this out. There's so many actions we take to make our lives safer whether that's wearing seatbelts or wearing helmets when biking and Practicing safe sex like all sorts of things where we really want to Maximize our safety and so I think is a net effect. You had a great thread on this today of It's not that there can never be any sort of impact in which people have a false sense of security But it is something that you would really want to be gathering data on and build a strong case around and not just assume It's gonna happen and that in Most cases people can think of even if that is a small second-order? Effect the effect of doing something that increases safety tends to have a much larger impact on actually increasing safety You have anything to add to that or And yes I mentioned before a lot of our incentives are focused on short term metrics long term things are much harder to measure and often involve kind of complex relationships and then the the fundamental business model of most of the tech companies is around manipulating people's behavior and Monopolizing their time and these things I don't think an advertising is inherently bad But they I think it can be negative when when taken to an extreme There's a great essay by James grimmelman the platform is the message and he points out these platforms are structurally at war with themselves The same characteristics that make outrageous and offensive content unacceptable are what make it go viral in the first place and so there's this kind of real tension here in which Often things. Yeah, that kind of can make content really offensive or unacceptable to us are also what are kind of fuelling their popularity and Being promoted in many cases. I mean this is it this is an interesting essay because he he just this like really in-depth dive on the Tide pod challenge which was this meme around to eating Tide Pods, which are poisonous Do not eat them and he really analyzes it though it's a great look at meme culture, which is very common and how kind of argues there's probably no example of someone talking about the tide pot challenge that isn't partially ironic which is common in memes that even Kind of whatever you're saying. They're kind of layers of irony and different groups are interpreting them Differently and that even when you try to counteract them, you're still promoting them

[0:30:57] ark_aung (Status: Not Started)
So with the tide pot challenge a lot of like celebrities we're telling people don't eat Tide Pods But that was also then kind of perpetuating the the popularity of this meme. So it's this is an essay I would recommend I think it's pretty insightful And so this is a we'll get to disinformation shortly But the the major tech platforms Often incentivize and promote disinformation and this is unintentional but it's it is somewhat built into their design and architecture Their recommendation systems and ultimately their business models And then on the on the topic of metrics I'm I just wonder bring up so there's this idea of blitzscaling and the premise is that if a company grows big enough and fast enough profits will eventually follow Um, it probably aura ties the speed over efficiency and risks potentially disastrous defeat and Tim O'Reilly wrote a really great article Last year talking about many of the problems with this approach, which I would say is incredibly widespread And is I would say the fund kind of fundamental model underlying a lot of venture capital And in it though investors kind of end up anointing winners as opposed to market forces It tends to lend itself towards creating monopolies and duopoly x' it can it's bad for founders and people end up kind of spreading themselves too thin so there are a number a number of significant downsides to this Why am I bringing this up in an ethics lesson? When we were talking about metrics But hockey hockey stick growth requires automation and a reliance on metrics Also prioritizing speed above all else doesn't leave time to reflect on ethics Um, and that is something that's hard that I think it you do often have to kind of pause to to think about ethics and that Following this model when you do have a problem it's often going to show up on a huge scale if you've if you've scaled very quickly, so I think this isn't thing to At least at least be aware of So one person asks about Is there a dichotomy between AI ethics which seems like a very first world problem and Wars poverty environmental exploitation has been a problem I guess and There's an answer here. Which something else. Maybe you can comment on whether you agree or I have anything to add which is that AI X they're saying is very important also for other parts of the world particularly in areas with high cellphone usage example many countries in Africa Of high cell penetration people get their news from Facebook. And what's up in YouTube and though it's useful It's been the source of many problems. Did you have any comments on? Yes, I think the first question so AIX as I noted earlier And I'm using the phrase data ethics here, but it's this very broad and it refers to a lot of things I think if people are talking about the you know In the future can computers achieve sentience and what are the ethics around that? And that is not my focus at all I'm very much focused on and this is our mission with the Center for Applied data ethics at the University of San Francisco Is kind of how are people being harmed now? What are the most immediate harms? and so in that sense, I don't think that data, ethics has to be a First-world or kind of futuristic issue. It's what's happening now and yeah, and as the person said in a few examples, well one example, I'll get to later is definitely the the genocide in Myanmar in which the Muslim minority the ROC inga or experiencing genocide the UN has ruled that Facebook played a determining role in that which is really intense and terrible and so I think that's an example of Technology. Yeah leading to very real harm now. They're also yeah whatsapp, which is owned by owned by Facebook there have been issues with People spreading disinformation and rumors, and it's led to several lynching dozens of lynchings in India of people kind of spreading these false rumors of oh There's a kidnapper coming around and in these kind of small remote villages and then a visitor or stranger shows up and gets killed Whatsapp also played a very important role or bad role in the election of Wilson Aero and Brazil election of to where today and the Philippines so I think technology is having a kind of very Immediate impact on on people and that those are the types of ethical questions I'm really interested in and that I hope I hope you are interested in as well Do you have anything else to say about that or? And I will I will talk about disinformation I realize those were kind of some disinformation Focus and I'm gonna talk about bias first. I think it's biased than just information. Yes

[0:36:03] kofi (Status: Not Started)
Question when we talk about ethics how much of this is intentional unethical behavior? I see a lot of the examples as ma graffiti competent behavior or bad modeling where the product her models rushed without sufficient Nesting thought around bias so forth but not necessarily Maryland and yeah. No, I agree with that. I think that most of this is unintentional I do think there's a Genoa well We'll get into this some cases. I think that I think in many cases the profit incentives are misaligned and I do think that when people are earning a lot of money it is very hard to consider actions that would reduce their profits Even if they would prevent harm and increase kind of ethics and so I think that you know, there's at some point where Valuing profit over how people are being harmed is You know, when does when does that become intentional is, you know a question to debate but I you know I don't I don't think people are setting out to say like I want to cause a genocide or I want to help authoritarian leader get elected and Most people are not are not starting with that but I think sometimes it's a carelessness and a thoughtlessness But that I I do think we are responsible for that And we're responsible to kind of be more careful and more thoughtful and how we approach things Alright so bias so biased I think is a an issue That's probably gotten a lot of attention Which is great and I want to get a little bit more in-depth because sometimes discussions on bias stay a bit superficial There was a great paper by Hirini Suresh and John Guttag Last year that looked at kind of came with this taxonomy of different types of bias and how they had kind of different sources the machine learning kind of pipeline And it was really helpful because you know different sources have different causes and they also require different different approaches for addressing then The Hirini wrote a blog post version of the paper as well, which I love when researchers do that I hope more of you if you're writing an academic paper also write the blogpost Version and I'm just going to go through a few of these types So one is representation bias And so I would imagine many of you have heard of joy Balan wienies work Which is rightly received a lot of publicity in gender shades. She internet gabru investigated commercial computer vision products from Microsoft IBM and face plus plus and then joy Balan weenie and Deborah She did a follow-up study that looked at Amazon and Karos and several other companies and the typical results they kind of found basically everywhere was that these products performed significantly significantly worse on dark skinned women so they were kind of doing worse on people with darker skin Compared to lighter skinned worse on women than on men and then the kind of intersection of that dark skinned women had these very high Error rates and so one example is IBM their product was ninety-nine point seven percent accurate on light skinned men and Only sixty-five percent accurate on dark skinned woman. And again, this is a commercial computer vision product that was released question It's a question from the Tremmel study group Volkswagen example In many cases its management that drives and rewards unethical behavior What can an individual engineer do in a case like this especially in a place like Silicon Valley where people move companies so often? Yeah, so I think I think that's a great point Yeah, and that is an example where I would have I would have much rather seen people that were higher ranking Doing jail time about this because I think that they were they were driving that and I think that yeah it's great to remember that I know many people in the world Don't have this option, but I think for many of us working in tech particularly in Silicon Valley To have a lot of options and often more options than we realize like I talked to people frequently that feel trapped in their jobs even though You know They're a software engineer in Silicon Valley and and so many companies are hiring And so I think it is important to use that leverage I think a lot of the kind of employee organizing movements are very promising and that can be useful but really trying to kind of vet the the ethics of the company you're joining and also being willing to walk away if you if If you're able to do so That's a great great question So what this example of representation bias here the Kind of way to address this is to build a more representative data set It's very important to keep consent in mind of the the people if you're using pictures of people But joy Valentinian Tim net gabru did this as part of as part of gender shades?

[0:41:03] SOVIETIC-BOSS88 (Status: Not Started)
However, this is a the fact that this was a problem not just for one company, but basically kind of every company they Looked at Was due to this underlying problem, which is that in machine learning bench benchmark data sets were on a lot of research However, kind of several years ago all the kind of popular Facial data sets were primarily of light skinned men. For instance. IG ba a Kind of popular face dataset several years ago only 4% of the images were of dark-skinned women Yes Question I've been worried about covert 19 contact tracing and the erosion of privacy location tracking private surveillance Companies, etc. What can we do to protect? Digital rights post covert. Can we look to any examples in history of what to expect? That is and that is a huge question and something I have been thinking about as well. I am I'm gonna Put that off till later to talk about and that is something where in the course I teach I have an entire Unit on privacy and surveillance, which I do not in tonight's lecture, but I can share some materials although I am already Really even just like rethinking how I'm gonna teach privacy and surveillance in the age of covet 19 Compared to two months ago when I taught it the first time but that is something I think about a lot and I will talk about later if we have time or Or on the forums. If we if we don't that's a great question a very important question On the topic and I will say and I have not had the time to look into them yet I do know that there are groups that are working on what are kind of more privacy protecting Approaches for for tracking and they're also groups putting out like if we are going to use some sort of Tracking what are the safeguards that need to be in place to do it responsibly Yes, I've been looking at that too it does seem like this is a solvable problem with with technology not all of these problems are but You can certainly store tracking history on somebody's cell phone And then you could have something where you say When you've been infected and at that point You could tell people that they've been infected by sharing the location and obviously preserving away. I Think some people are trying to work on that. I'm not sure. It's Actually, technically a problem. So I think there's sometimes there are ways to provide the minimum and of level you Know kind of application with it with whilst keeping privacy yeah, and then II think it's very important to also have things of you know, clear like Expiration date like we you know like looking back at 911 in the United States that kind of a assured in all these laws that were now kind of stuck with that have really eroded privacy of Anything we do around Coppa 19 being very clear. We are just doing this for Coppa 19 and then There's a time limit and expires and it's kind of for this clear purpose And they're also issues though of you know, I mentioned earlier about data containing errors, you know This has already been an issue and some of other countries that we're doing Kind of more surveillance focused approaches of you know, what about like when it's wrong and people are getting kind of Quarantined and they don't even know why and for no reason and so to be mindful of those But yeah, well, we'll talk more about this kind of later on back to back to bias Yeah, we had kind of the the benchmarks so in the benchmark that's you know widely used has bias then that is really kind of a Replicated at scale and we're seeing this with image net as well, which is you know Probably the most widely studied computer vision data set out there Two-thirds of the image net images are from the West. So this pie chart shows that the 45% of the images in image net are from the United States 7% from Great Britain 6% from Italy 3% from Canada 3% from Australia You know and we're covering a lot of a lot of this pie without having having gotten to outside the west and so then this has shown up in concrete ways of Classifiers trained on image net. So one of the categories is bridegroom man getting married There are a lot of you know cultural components to that and so they have you know much higher error rates on on bridegroom's from from the Middle East or from the global south And there are there are people now kind of working to diversify these data sets But it is quite dangerous that they can really be Kind of widely built on its scale or have been widely built on its scale before these biases were recognized

[0:46:02] Hadus (Status: Not Started)
Another key study is the compass recidivism algorithm, which is used in Determining who? Who has to pay bail? So in the u.s A very large number of people are in president who have not even had a trial yet just because they're too poor to afford bail As well as sentencing decisions and parole decisions and ProPublica, I did a famous investigation in 2016 that I imagine many of you have heard of in which they Found that the false positive rate for black defendants was nearly twice as high as for white defendants so black defendants who were live from Dartmouth found that it was the The software is no more accurate than Amazon Mechanical Turk workers. So random people on the internet It's also the software is you know, this proprietary black box using over 130 inputs And it's no more accurate than a linear classifier on three variables Yet it's still in use and it's an use in many states Wisconsin as one place where it was challenged yet. The Wisconsin Supreme Court upheld its use if you're interested in the kind of topic of how you define fairness because there is a lot of intricacy here and I mean, I don't know anybody working on this who thinks that what compass is doing is his right? But they're using this different different definition of fairness Arvind Ron Yan has a fantastic tutorial 21 fairness definitions and their politics that I that I highly recommend And So, um going back to kind of this taxonomy if types of bias This is an example of historical bias and historical bias is a fundamental structural issue With the first step of the data generation process and it can exist even given perfect sampling and feature selection So kind of with the the image classifier that was something where we could you know? Go gather a more representative set of images and that would help address it that is not the case here. So gathering kind of more data on the US criminal justice system It's all going to be biased because that's really kind of baked into baked into our history and our current state And so this is I think good good to recognize One thing that can be done to try to at least mitigate this is to to really talk to domain experts and by the people impacted and so a really positive example of this is a Tutorial from the fairness accountability and transparency Conference that Christian Lum who's the lead? Statistician for the Human Rights data analysis group and now professor UPenn organized together with a former public defender Elizabeth bender Who's the staff attorney for New York's Legal Aid Society and Terence Wilkerson an innocent man who was arrested and cannot afford bail and Elizabeth and Terence were able to provide a lot of insight to how the criminal justice system works and practice which is often Kind of very different from the you know more kind of clean logical abstractions that computer scientists deal with but it's really Important to understand those kind of intricacies of how this is going to be implemented and used in these You know messy complicated real world systems question Out of the AI biases transferred from real life biases for instance I like people being treated differently isn't everyday phenomenon and then - That's correct, yes, and so this is often yeah coming from from real-world biases and I'll come to this in a moment, but Algorithmic systems can amplify those biases so they can make them even worse. But yeah, they are often being learned from from existing data. I asked it because I guess I often say this being raised as it's kind of a reason not to worry about Well, I'm gonna get to that in a moment actually Think in two slides, so hold on to that question. I Just wanna talk about one other type of bias first measurement bias. And so this was an interesting paper bias in deal milena 'then and I had Obermeyer where they looked at historic electronic health record data to try to determine what factors are most predictive of stroke and they said, you know this could be useful like prioritizing patients at the ER and So they found that the number one most predictive factor was prior stroke, which that makes sense second was Cardiovascular disease that's also that seems reasonable and then third most kind of still very predictive factor was accidental injury followed by Having a benign breast lump a colonoscopy or sinusitis. And so I'm not a medical doctor But I can tell something weirds going on with Factors three through six here. Like why would these things be? Predictive of a stroke. Does anyone want to think about about why this might be?

[0:51:09] morgan (Status: Not Started)
I mean guesses you want to read Oh someone's yeah Okay, the first answer was they test for it any time someone has stroke Confirmation bias overfitting is because they happen to be in hospital already Biased data eh I record these events Because the data was taken before certain advances in medical science These are these are all good guesses. Not not quite what I was looking for, but good good thinking That's such a nice way of saying So what that what the researchers say here is that this was about Their patients they're people that utilize health care a lot and people that don't and they call it kind of high utility versus low utility Of health care and there are a lot of factors that go into this I'm sure just who has health insurance and who can afford their co-pays there may be cultural factors There may be racial and gender by there is racial and gender bias on how people are treated So a lot of factors and basically people that utilize health care a lot they will go to a doctor when they have sinusitis and they will also go in when they're having a stroke and people that do not utilize health care much are probably not going to go in possibly for either and so so what the authors write is that we haven't measured stroke which is you know region of the brain being denied a Kind of new blood and you oxygen what we've measured is who had symptoms who went to the doctor received tests and then got this diagnosis of stroke and you know That seems like it might be a reasonable proxy for for who had a stroke but a proxy is you know Never exactly what you wanted and in many cases that that gap ends up being significant and so this is just one form that the measurement bias can take but I think it's something to really Kind of be on the lookout for because it can be quite subtle And so now starting to return to a point that was brought up earlier Aren't aren't people biased. Yes. Yes, we are and so there have been Dozens and dozens if not hundreds of studies on this But I'm just going to quote a few all of which are linked to in this Sandy'll milena the New York Times article if you want to find find the studies, so this all comes from, you know, peer-reviewed research but when doctors were shown identical files They were much less likely to recommend a helpful cardiac procedure to black patients compared to white patients And so that was you know, same file, but just changing the race of the patient When bargaining for a used car black people were offered initial prices $700 higher and received fewer concessions responding to apartment rental ads on Craigslist with a black name elicited fewer responses and with a white name an All-white jury was 16 points more likely to convict a black defendant than a white one But when a jury had just one black member it convicted both at the same rate and so I share these to show that kind of no matter what type of data you're looking working on whether that is Medical data or sales data or housing data or criminal justice data that it's very likely that there's there's bias in it There's a question No, it's gonna say I find that last one really interesting like this kind of idea that a single black member of the jury Yes, it has some kind of like anchoring impact I kind of suggests that oh, I'm sure you're going to talk about diversity later but it's gonna keep this in mind that maybe like even A tiny bit of diversity here just reminds people that there's a, you know, a range of different types of people and perspectives No, that's it. That's a great point. Yeah And so the the question that was kind of asked earlier is so why does algorithmic bias matter like I have just shown you that humans are really biased - so Why are why are we talking about algorithmic bias and people have brought this up kind of like what's what's the fuss about it? and there I Think algorithmic bias is a very significant worth talking about and I'm going to share four reasons For that one is the machine learning can amplify bias So it's not just encoding existing biases But in some cases it's making them worse and there have been a few studies on this one I like is from Maria de arteaga of CMU and here they were they took people's I think job descriptions from LinkedIn and what they found is that Imbalances ended up being pounded and so in the group of surgeons Only 14% were women however in the true positives so they were trying to predict the the job title from the summary Women were only 11% in the true positives. So this kind of imbalance has gotten worse And basically there was kind of this asymmetry where the you know, the algorithm has learned it's safer For for women to kind of not not guess surgeon Another so this is one reason another reason that Algorithmic bias is a concern is that algorithms are used very differently than human decision-makers in practice and so people sometimes talk about them as though they are plug-and-play and are changeable of you know with Humans this bias and the algorithm is you know, this bias. Why don't we just substitute it in?

[0:56:28] PoonamV (Status: Not Started)
However, your the the whole system around it ends up kind of being different and in practice One one kind of aspect of this is people are more likely to assume algorithms are objective or error-free Even if they're given the option of a human override and so if you give a person, you know Even if you just say hey, I'm just giving the judge this recommendation They don't have to follow it If it's coming from a computer many people are gonna take that as objective in some cases Also, there may be, you know pressure from their boss to you know, not disagree with the computer more times, you know Nobody's gonna get fired by going with the computer recommendation Algorithms are more likely to be implemented with no appeals process in place. And so we saw that earlier when we were talking about recourse Algorithms are often used. It's scale. They can be replicating an identical bias at scale and Algorithmic systems are cheap and all of these I think are interconnected. So in many cases, I think that algorithmic systems are being implemented not because they produce better outcomes for everyone but has They're kind of a cheaper way to do things at scale You know offering a recourse process is more expensive being on the lookout for errors is more expensive. So this is kind of cost-cutting Measures and Cathy O'Neil talks about many of these themes in her book weapons of mass destruction Kind of under the idea that the the privileged a process by people the pourer process by algorithms. There's a question Your questions. Hmm This seems like an intensely deep topic aiding specialized Expertise to avoid getting it wrong if you were building an ml product. Would you approach an academic institution? Haitian on this you see a data product Development triad becoming IDEO quartet involving an ethics or data privacy. Yes So I think interdisciplinary work is very important I would I Would definitely focus on trying to find kind of domain experts on whatever your particular domain is who understand the intricacies of that domain? is important And I think with yeok With the academic it depends you do want to make sure you get someone who is kind of applied enough to Kind of understand how how things are happening in an industry But yeah, I think involving more people and people from more fields is is a good a good approach on the whole Someone invents and publishes a better ml technique like attention or transformers and then next to graduate student demonstrates using it to improve Facial recognition by 5% and then a small start-up publish There's an app that does better facial recognition and then a government uses the app to study downtown walking patterns and endangered species and after These successes for court-ordered monitoring and then a repressive government then takes that method to identify in this ethnicity and then you get a genocide No one's made a huge ethical error at any incremental step yet. The result is horrific I have no doubt that Amazon will soon serve up a personally customized price for each item that maximizes their profits How can such ethical creep be addressed where the effect is remote for many small causes? And it's all yeah, so that that's a great summary of how yeah, these things can happen somewhat incrementally I'll talk about some tools to implement It kind of towards the end of this lesson that hopefully can help us so some of it is I think we do need to get better at kind of trying to think a few more steps ahead. Then we have been You know in particular we've seen examples of people, you know There was the study of how do I identify protesters in a crowd even when they had scarves or sunglasses their hats on? You know and when the the researchers on that were questioned They were like, oh it never even occurred to us that bad guys would use this, you know We just thought it would be for finding bad people And so I do think kind of everyone should be building their Ability to think a few more steps ahead and part of this is like it's great to do this in teams. Preferably in diverse teams Can help with that that process I mean on this question of computer vision there has been you know, just in the last few months Is it Joe Redman? creator of Yolo who has said that he's no longer working on computer vision just because he thinks the the Misuse is so far outweigh the the positives Intimidate gabru said she's she's considering that as well So I think there are there are times where you have to consider And then I think also really Actively thinking about how to what safeguards do we need to put in place to kind of address the the misuses that are happening Yes, I just wanted to say somebody really liked the Kathy O'Neil quote Privileged are processed by people the poor processed by algorithms thing. They're looking forward to learning more reading more from Kathy O'Neal's head of Yes. Yeah and in Kathy Anya also writes in the in Kathy O'Neill's a fellow fellow math PhD and But she also has written a number of good articles And it the book kind of goes through a number of those case studies of how algorithms are being used in different places So kind of in in summary of Humans are biased why do why are we making a fuss about algorithmic bias? So one is we saw earlier machine learning can create feedback loops. So it's you know, it's not just Kind of observing what's happening in the world? But it's also determining outcomes and it's kind of determining what future data is Machine learning can amplify bias Algorithms and humans are used very differently in practice and it'll also say technology is power and with that comes responsibility

[1:02:09] foobar8675 (Status: Not Started)
and I think for all of us to have access to deep learning we're still in a kind of very Fortunate and small percentage of the world that is able to use this technology right now And I hope I hope we will all use it responsibly and really take our power seriously and I just I just noticed the time and I think we're about to start a next section on on analyzing or Kind of steps steps we can take so this would be a good a good place to take a break. So Let's meet back in seven minutes at 7:45 All right, let's start back up and actually I was at a slightly different place than I thought But just a few Questions that that you can ask about projects you're working on and I hope you will ask about projects you're working on The first is should we should we even be doing this? And considering that maybe there's some work that we shouldn't do There's a paper when the implication is not to design technology As engineers we often tend to respond to problems with you know What can I make or build to address this but sometimes the answer is to not make or build anything? One example of research that I think has a huge amount of downside and really no upside I see was Kind of to identify the ethnicity particularly for people of ethnic minorities And so there was work done identifying the Chinese we Gers which is the Muslim minority in western China Which has since you know over a million people have been placed in internment camps And I think this is a very very harmful harmful line of research. I Think that the You know there have been at least two attempts of building building a classifier to try to identify someone's sexuality which is it's Probably just picking up on kind of stylistic differences But this is something that a call could also be quite quite dangerous as in many countries. It's it's illegal To be gay. Yes So this is a question for me, which I don't know the answer to yeah As that title says a standard scientist says he built the gate art Using the lamest AI possible to prove the point and my understanding is that pointless to say you know, I guess it's something like hey, you could use fast AI lesson 1 For an arrow too. You can build this thing. Anybody can do it you know, how do you feel about this idea that there's a role to Demonstrate what's readily available with the technology we have? Yeah, that's the thing that I think So I appreciate that and I'll talk about this a little bit later open ai with GPT 2 I think was trying to raise a raise a debate around Around dual use and what is responsible release of of? dual use Technology and what's a kind of responsible way to raise? Raise awareness of what is possible in the in the cases of researchers that have done this on the sexuality question? to me it hasn't seemed like they've put adequate thought into how they're conducting that and who they're collaborating with to ensure that it is something that is Leading to kind of helping address the problem But I think you're right that I think there is probably some place for letting people ya know What is probably widely available now? It reminds me a bit of my pen testing Info? Yeah, where where It's kind of considered it. Well, there's an ethical way that you can go about pointing out that it's fairly easy to be a system Yes. Yeah, I would I would agree with that that there there is an ethical way But I think that's something that we as a community Still have more work to do and even determining what that is Other questions to consider are what biases in the data and something I should highlight is People often ask me. You know, how can I Devious my data or ensure that its bias free and that's not possible all data contains bias. And the kind of most most important thing is just to Understand kind of how your data set was created and what its limitations are so that you're not blindsided by that But you're never going to fully remove it and some of the I think most promising approaches in this area are Work like timid Abreu's data sheets for data sets Which is kind of going through and asking kind of a bunch of questions about how your data set was created

[1:06:56] abhigupta4981 (Status: Not Started)
And for what purposes and how it's being moved maintained and you know what are the risk in that just to really kind of be aware of of the context of your data and The code and data be audited. I think particularly in the United States. We have a lot of issues with when private companies are Creating software that's really impacting people through the criminal justice system, or hiring and when these things are You know kind of their proprietary black boxes that are protected in court That's this creates a lot of kind of of issues of you know, what are what are our rights around that? Looking at error rates for different subgroups is really important and that's what so kind of so powerful about Joy, Balam wienies work if she had just looked at Light-skinned versus dark skin and men versus women she wouldn't have identified Just how poorly the algorithms were doing on dark-skinned women What is the accuracy of a simple rule based alternative and this is something I think Jeremy talked about last week Which is just kind of good good machine learning practice to have a baseline but particularly in cases like the compass recidivism where this 130 Variable black box is not doing much better than a linear classifier On three variables that race is kind of a question of why are why are we using this? And then what processes are in place to handle appeals or mistakes because there will be errors in the data There may be bugs and the implementation and we need to have a process for recourse Yes Can your explicit for me know? Nobody voted them up at all What's the thinking behind this idea that a simpler model Is you gotta take a simpler model or other things being the same? You should pick the simpler one? Is that what this baselines for what and if so, what's the kind of thinking behind that? Well with the compass recidivism algorithm Some of this for me is linked to the Proprietary black box nature and so you're right if maybe if we had a way to introspect and what we're our rights around appealing something But I would say yeah, like why use the more complex thing if the the simpler one works the same And then how diverse is the team that built it and I'll talk more about team diversity later later in this lesson Okay, Jeremy at the start, but I'm not the teacher So it actually there's Jeremy do you think transfer learning makes this tougher? Auditing the data that led to the initial model. I assume they mean Jeremy, please ask Rachel No, they were they were asking you That's that's a good question Again, I think it's important I would I would say I think it's important to have information probably on both datasets what the initial data set used was and What the the data set you use to fine-tune it? Do you have thoughts on that what she said And then I'll see so while Bias and fairness as well as accountability and transparency are important. They aren't everything and so there's this great paper a mulching proposal by aw skis at all and Here they talk about a system for turning the elderly into high nutrient slurry, so this is something that it's clearly an ethical but they Proposed a way to do it that is fair and accountable and transparent and meets these qualifications and so that kind of shows that some of the limitations of this framework as well as kind of being a good a good technique for kind of inspecting whatever framework you are using of Trying to find something that's clearly unethical that code that can meet meet the standards. You've put forth That that technique I really like it it's like a it's my favorite technique from philosophy. It's this idea that you You say okay given this premise? Here's what it implies and then you try and find an implied Result which intuitively is clearly Saying and it's a really it's it's yeah, it's the number one philosophical thinking tall I got out of university and I sometimes we can have a lot of fun with it like this time to Thank you All right, so the next kind of big case study your topic I want to discuss is disinformation So in 2016 in Houston a group called heart of Texas posted about protests outside an Islamic Center And they told people to come armed another Facebook group posted about a counter protest to show up supporting freedom of religion and inclusivity and so

[1:12:01] melonkernel (Status: Not Started)
there were kind of a lot of people present is this more people on the the side supporting freedom of religion and A reporter though for the Houston Chronicle noticed something odd which he was not able to get in touch with the organizers for either side and It came out many months later that both sides had been organized by Russian trolls And so this is something where you had The people protesting were you know? genuine Americans kind of protesting their beliefs, but they were doing it in this way that had been kind of completely framed very disingenuously by by Russian operatives And So when thinking about disinformation it is not people often think about So-called fake news, you know and inspecting like a single post is this, you know Is this true or false? But really disinformation is often about orchestrated campaigns of manipulation and that it involves Kind of all the seeds of truth kind of the best propaganda Always involves kernels of truth at least it also involves kind of misleading Context and and can can involve very kind of sincere sincere people that get good swept in it a report came out this fall and investigation from Stanford's Internet Observatory where Rene arrests and Alex Stamos work of Russia's kind of most recent disinformation or most recently identified disinformation campaign And it was operating in six different countries in Africa It often purported to be local news sources It was multi-platform. They were encouraging people to join their whatsapp and telegram groups and they were hiring local people's reporters and a lot a lot of the content was not Not necessarily disinformation. It was stuff on culture and sports and local weather. I mean there was a lot of kind of very pro-russia coverage But that it covered a range of topics and so this is kind of a very Sophisticated phase of disinformation and in many cases it was hiring hiring locals kind of as reporters to work for these sites And I should say well I've just given two examples of Russia Russia - certainly does not have a monopoly on disinformation there are plenty of Plenty of people involved and producing it Kind of on a topical topical Issue, there's been a lot of disinformation around around coronavirus in kovat 19 I In terms of kind of a personal level if you're looking for advice on spotting disinformation or to share with loved ones about this Mike Caulfield is a great person to follow and he's even So he tweets ed Holden and then he has started an info Demick blog specifically about the about kovat 19 but he he talks about his approach and how People have been trained in schools for 12 years Here's a text read it use your critical thinking skills to figure out what you think about it But professional fact checkers do the opposite they get to a page and they immediately get off of it and look for kind of higher higher quality sources to see if they can find confirmation and Caulfield also really promotes the idea of A lot of critical thinking techniques that have been taught Take a long time. And you know, we're not going to spend 30 minutes evaluating each tweet that we see in our Twitter stream It's better to give people an approach that they can do in 30 seconds that you know It's not gonna be fail proof If you're just doing something for 30 seconds But it's better to to check than to have something that takes 30 minutes that you're just not going to do at all So I wanted to kind of put this out there as a resource I mean as a whole kind of set of lessons at lessons dot check, please see see and he's a he's a professor And I in the Data, ethics course I'm teaching right now. I made my first lesson the first half of which is kind of specifically about Coronavirus disinformation I've made that available on YouTube. I've already shared it. And so I'll add a link on the forums If you want if you want a lot more detail on on disinformation than just kind of this the short bit here But so going back to kind of like what is disinformation? It's important to think of it as an ecosystem again, it's not just a single post or a single news story that You know, it's misleading or has false elements in it. But it's this really this broader ecosystem

[1:16:44] AndreaPi (Status: Not Started)
Claire Wartell first draft news, who is a Leading expert on this and does a lot around kind of training journalists and how journalists can report responsibly talks about the trumpet of amplification and this is where I rumors or Memes or things can start on 4chan and h han and then move to closed messaging groups Such as whatsapp telegram Facebook messenger from there to community concern reddit or YouTube then to kind of more mainstream social media and then picked up by the professional media and Politicians and so this can make it very hard to address that it is this kind of multi-platform in many cases campaigns may be Utilizing kind of the differing roles or loopholes between between the different platforms And I think we've certainly are seeing more and more examples where it doesn't have to go through all these steps But can can can jump jump forward And online discussion is very very significant because people It helps us form our opinions and then this is tough because I think most of us think of ourselves as pretty independent minded but discussion really does you know we evolved as kind of social beings and to be Influenced by by people in our in group and in opposition to people in our out group and so online discussion impacts us people discuss all sorts of things online here's a reddit discussion about whether the US should cut defense spending and you have comments you're wrong and the defense budget is a good example of how badly the u.s. Spends money on the military and Someone else says yeah, but that's already happening. There's a huge increase in the military budget the Pentagon budgets already increasing I didn't mean to stop a sound like stop paying for the military I'm not saying that we cannot pay the bills, but I think it would make sense to cut defense spending Does anyone want to guess what subreddit this is from? On popular opinion news changed my view net neutrality is Heard good guesses but they're wrong. I love the way you say though This is all from what it is. It's from the Sub simulator gptt oh, so these comments are all Written by GPT too, and this is in good fun. It was clearly labeled on the subreddit That it's coming in GP g2 is a language model from open AI that was kind of in a Trajectory of research that many many groups were on And so it was released I guess about a year ago and Should I read the unicorn story Jeremy, okay So if many of you have probably have probably seen this so here and then this this was cherry-pick But this is still very very impressive So human written prompt was given to the language model in a shocking finding scientists discovered a herd of unicorns living in a remote previously unexplored Valley in the Andes Mountains Even more surprising to the researchers was the fact that the unicorn spoke perfect English and then the next part is all Generated by the language model. So this is a deep learning model that produced this and the the computer model Generated dr. Jorge Perez what appeared to be a natural fountain surrounded by two peaks of rock and silver snow? Perez and the others then ventured further into the valley By the time we reached the top of one peak the water looked blue with some crystals on tops at Perez Perez and his friends were astonished to see the Unicorn heard These creatures could be seen from the air without having to move too much to see them. They were so close They could touch their horns While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English Perez stated we can see for example that they have a common language something like a dialect or dialectic And so I think this is really compelling prose to have been have been generated by a computer in this form So we've also have seen advances in computers generating pictures as specifically gans so Katie Jones was listed on LinkedIn as a Russia and Eurasia fellow she was connected to several people from mainstream Washington think tanks and The Associated Press discovered that she is not a real person. This photo was generated by a gang And so this, I think it's kind of scary when we start thinking about how How compelling the text that's being generated is and combining that with Pictures these photos are all from this person does not exist dot-com generated by Ganz and there's a very very Real and eminent risk that online discussion will be swamped with fake manipulative agents To have even greater extent than it than it already has and that this this can be used to influence public opinion So Oh, actually I guess it well no keep going going back in time to 2017 the FCC was considering repealing net neutrality

[1:22:16] jona (Status: Not Started)
And so they opened up for comments to see you know How do Americans feel about net neutrality and this is a sample of many of the comments that were opposed to net neutrality? They wanted to repeal it and included. I'll just read a few clips Americans as opposed to Washington bureaucrats deserve to enjoy the services they desire Individual citizens as opposed to Washington bureaucrats should be able to select whichever services they desire People like me as opposed to so-called experts should be free to buy whatever products they choose And these have been helpfully color-coded so you can kind of see a pattern that this was a bit of a Mad Libs Where you had a few choices for green for the first? noun and then in Orange or red? I guess it's as opposed to or rather than Orange we've got either Washington bureaucrats so-called experts the FCC and so on and this this analysis was done by Jeff Cao who's now a Computational journalist at Pro Publica doing great work and he did this analysis discovering this this campaign in which These comments were designed to look unique but had been created kind of through through some mail merge style Kind of putting together Mad libs. Yes So this was this was great work by Jeff he found that So while they received the FCC received over 22 million comments Less than 4% of them were truly unique and this is this is not all malicious activity You know, there are many kind of ways where you get a template to contact your legislator about something But you know in the example kind of shown previously. These were designed to look like they were unique when they weren't and More than 99% of the truly unique comments wanted to keep net new to allottee However, that was not not the case if you looked at the full the full 22 million comments However, this was this was in 2017, which may not sound that long ago but in in the field of natural language processing we've had like an entire kind of a Revolution since then there's just been so much progress made and this would be I think virtually impossible to catch today Using it, you know if someone was using a sophisticated language model to to generate comments So chess asks a question, which I'm gonna treat it as a two-part question I think it's not necessarily what happens when there's so much AI trolling That most of what gets straight from the web is AI generated text And then the second part and then what happens when you use that to generate more AI generated text Yes for the first part. Yeah, this is a real risk or not risk But kind of challenge we're facing of real humans can get drowned out when so much text It's gonna it's gonna be AI trolling and And we're already seeing and ie in the interest of time I have I can talk about disinformation for hours and I had to cut a lot of stuff out and but Many people have talked about how kind of the the new form of censorship is about drowning people out and so it's not necessarily kind of forbidding someone from saying something but just totally totally just drowning them out with the massive quantity of Text and information and comments and AI can really facilitate that and so I do not have a good solution to that in terms of AI learning from AI Text I mean, I think you're gonna get systems that are potentially kind of less and less relevant to humans, and may have harmful effects if they're kind of being used to create software that is interacting with or impacting humans, so that's a Concern I mean one of the things I find fascinating about this is We could get to a point where 99.99% of to it's and fast AI forum posts, whatever Auto-generated particularly on kind of more like political type places where a lot of it's pretty low content pretty basic And the thing is like if it was actually good You wouldn't even know so What if I told you that seventy five percent of the people you're talking to on the forum right now? I actually bots can tell which ones they are How would you prove whether I'm right or wrong? Yeah, I know name day. I think this is a real issue on Twitter You know particularly People you don't know of you know wondering like is this an actual person or a bot? I think it's a common question people people wonder about And can be hard to tell

[1:27:19] Jess (Status: Not Started)
But I think it has significance for Has a lot of significance for kind of how human government works, you know I think there's something about humans being in society and having norms and rules and mechanisms that this can really undermine and make difficult And so when when gp2 2 came out Jeremy Howard co-founder of fast AI was quoted in The Verge article on it I've been trying to warn people about this for a while We have the technology to totally fill Twitter Email in the web up with reasonable sounding Context appropriate prose which would drown out all of their speech and be impossible to filter So one kind of step towards addressing this Is the need for digital signatures or an Etsy on e? the head of the Allen Institute on AI wrote about this in HBR He wrote recent developments at AI point to an age where forgery of documents pictures audio recordings videos and online Identities will occur with unprecedented ease AI is poised to make high fidelity forgery inexpensive and automated Leading to potentially disastrous consequences for democracy security and society and proposes kind of digital signatures as a means for authentication and I will say here a kind of one of the one of the additional risk of kind of All this forgery and fakes. Is that it also? Undermines people speaking the truth and Zainab's fxg who does a lot of research on Protests around the world and in different social movements has said that she's often approached by kind of whistleblowers and dissidents who in many cases will risk their lives to try to publicize like a wrongdoing or human rights violation only to have Kind of bad actors say oh that picture was photoshopped that was faked and that it's kind of now this big issue for for whistleblowers and dissidents of how How can they verify what they are saying and that kind of that need? need for verification And then someone you should definitely be following on this topic is Renee - Resta And and she wrote a great article with Mike Godwin last year Framing that we really need to think disinformation as as a cybersecurity problem, you know, it sees kind of coordinated campaigns of manipulation And bad actors and there's I think some Important work happening at Stanford as well on this All right questions on disinformation Okay, so uh next step at the coal foundations so now So the fast day I approach we always like to kind of ground everything and what are the real-world case studies before we get to? Kind of the the theory underpinning it and I'm not going to go too deep on this at all. And So if there is a fun article, what would an Avenger do and hat tip to Casey fees? Ler for suggesting this? And it goes through kind of three common ethical philosophies Utilitarianism and gives the example of iron man. I'm trying to max good Deontological ethics of Captain America being an example of this adhering to the right and then virtue ethics Thor Living by a code of honor and so I thought that was a nice reading. Yes Where do you stand on the argument that social media companies are just Neutral platforms and that problematic content is the entire responsibility if he uses just the same way that phone companies Aren't held responsible when phones are used for scams or car companies held responsible when vehicles used to say terrorist attacks And so I do not think that The plot that the platforms are neutral because they make a number of Design decisions and enforcement decisions around even kind of what their what their Terms of Service are and how those are Enforced and that in keeping in mind harassment can drive many people off of platforms and so kind of Many of those decisions is not that Oh, everybody gets to keep free. Speech when there's no enforcement. It's Just changing kind of who who who is silenced I do think that there are a lot of really difficult questions that are raised about this Because I also I think that the the platform's You know, it's they are they're not Publishers, but they are in this I think kind of a intermediate Area where they are performing many of the functions that Publishers used to perform so, you know like a newspaper whiskey which is curating which which articles are in it Which is not what platforms are doing, but they are Getting closer closer to that I mean I something I come back to is it is it is an uncomfortable amount of power For for private companies to have yeah, and so it does raise a lot of difficult decisions

[1:33:02] Mindtrinket (Status: Not Started)
But I I do not believe that they are they are neutral So, um for for this part I mentioned the markoulis Center earlier definitely check out their site ethics and technology practice They have a lot of useful Useful resources And I'm gonna go through these relatively quickly as just kind of examples so they give some kind of deontological questions that technologists could ask and so Deontological ethics or where you kind of have various? Kind of rights or duties that you might want to respect and this can include principles like privacy or autonomy How might the dignity and autonomy of each stakeholder be impacted by this project What considerations of trust and of justice or relevant? This is some project involved any conflicting moral duties to others in some cases, you know, there'll be a kind of conflict between different Different rights or duties you're considering and so this is this kind of an example and they have more More in the reading of the types of questions you could be asking kind of when evaluating of just even how do you evaluate? Kind of whether whether a project is ethical Consequentialist questions Who will be directly affected who will be indirectly affected? Well, the end consequentialist includes Utilitarianism as well as common good Will the effects in aggregate create more good than harm and what types of good and harm? Are you thinking about all the relevant types of harm and benefits including psychological? political environmental moral cognitive emotional institutional cultural also looking at long term long term benefits and harms And then who experiences them? Is this something where the risk of the harm are gonna fall disproportionately on the least powerful Who's going to be the ones to accrue the benefits? Have you considered dual use and so these are these they're again kind of questions you could use when trying to trying to evaluate a project and I think and the recommendation of the markoulis Center is that this is a Great activity to kind of to be doing as a team and as a group Yes, I was gonna say like I can't I Can't overstate how useful this tool is like it, you know, I think oh, it's just a list of questions Yeah, you know but like this is kind of to me This is that this is the big gun tool for for how you how you handle This is like if somebody is helping you think about the right set of questions and then you let go through them with a diverse Group of people and discuss the questions. I Mean that's that. It's this is this is goal like don't, you know go back and reread these and don't don't just skip over them because Take them to work use them next time you're talking about a project. They're a really great Great set of questions to use a great tool in your toolbox yeah, and go to the original reading has even kind of more detail and more elaboration on the questions And then they kind of give a summary of five potential ethical lenses The the rights approach which option in best respects the rights of all who have a stake The justice approach which option treats people equally or proportionally and so these two are both a deontological The utilitarian approach which option will produce the most good and do the least harm the common good approach which option best serves the community as a whole not just some members and so here three and four are both a consequentialist and then Virtue approach which option leads me to act as the sort of person I want to be and that can involve, you know particular Virtues of you know, do you value trustworthiness or truth or courage? And so I mean a great activity if this is something that you're studying or talking about it work with your teammates The the Markkula Center has a number of case studies That you can talk through and will even ask you to kind of evaluate them you know evaluate them through these five lenses and how does that kind of impact your your take on what the what the right thing to do is It's kind of weird for a programmer or a computer program or data science in some way in some ways to like think of these as tools like a steai your pants or whatever but I mean they absolutely are this is like These like software tools for your brain, you know to help you you kind of go through a program that might help you debug your thinking Great. Thank you. And Then if someone brought up earlier, so that was a kind of very Western centric intro to ethical philosophy there are other ethical lenses and other Cultures and I've been doing some reading particularly on the the Maori worldview. I Don't feel Confident enough in my understanding that I could represent it, but it is very good to be mindful that yeah

[1:38:15] reshama (Status: Not Started)
there are other other ethical lenses out there and I do very much think that You know the people being impacted by a technology like their their ethical lens is kind of what matters and that this is is a particular issue and we have so many kind of multinational corporations there's an interesting project going on in New Zealand now or the New Zealand government is kind of Considering its AI approach and is at least ostensibly kind of wanting to wanting to include the Maori view on that So that's a that's kind of a little a little bit of theory But now I want to talk about some kind of practices you can implement in the workplace Again, this is from the markoulis Center So this is their ethics toolkit, which I particularly like and I'm just I'm not going to go through all of them I'm just going to tell you a few of my favorites So tool one is ethical risk sweeping and this I think is similar to the idea of kind of pen testing that Jeremy mentioned earlier from security but to have regularly-scheduled ethical risk sqweep's and While no vulnerability Vulnerabilities found is generally good news. That doesn't mean that it was a wasted effort and you keep doing it Keep looking for for ethical risk. One moment And then assume that you missed some risk in the initial project development Also, you have to set up the incentives properly where you're rewarding team members for spotting new ethical risk All right Yet so got some comments here So my comment here is about the learning rate finder and I'm not going to bother with the exact mathematical Definition partly because I'm a terrible mathematician and partly because it doesn't matter but if you just remember, oh, sorry, that's actually not me I am just reading something that Patti Hendricks has trained a language model of me. So that was me greeting the language model of me the real me Thank you This is a tool one. I would say another kind of example of this. I think it's like bread teaming of, you know, having a team within your get sky and to find your vulnerabilities a tool 3 Another one. I really like expanding the ethical circle So whose interest desires skills experiences and values have we just assumed rather than actually consulted? Who are all the stakeholders who will be directly affected? And have we actually asked them what their interests are Who might use this product that we didn't expect to use it or for purposes that we didn't initially intend and so then a great implementation of this comes from the University of Washington's Tech policy lab did a Project called diverse voices and it's neat they have both a Academic paper on it and then they also kind of have like a guy Lengthy guide on how you would implement this but the idea is how to kind of organize expert panels around Around new technology and so they they did a few samples one Was there considering augmented reality and they held expert panels with people with disabilities people who are formerly are currently incarcerated And with women to cut their get their input and make sure that that was included They did a second one on an autonomous vehicle strategy document and organized expert panels with youth with people that don't drive cars and with extremely low-income people and So I think this is a great guide if you're kind of unsure of how do you even go about? setting something like this up to expand your circle include include more people and get Get perspectives that may be underrepresented by your employees So I just want to let you know that this resource is out there Tool six is think about the terrible people And and this can be hard because I think we're often you know thinking Kind of positively or thinking about people like ourselves who don't have terrible intentions, but really think about Who might want to abuse steal misinterpret hacks Troy or weaponize? What we build? Who will use it with alarming stupidity or rationality? What rewards incentives openings has our design inadvertently created for those people? And so kind of remembering back to the section on metrics you know, how are people going to be trying to game or manipulate this and How can how can we then remove those rewards or incentives? And so this is this is an important kind of important step to take And

[1:43:08] nchukaobah (Status: Not Started)
Then tool seven is closing the loop ethical feedback and iteration remembering this is never a finished task and identifying feedback channels that will give you kind of reliable data and integrating this process with quality management and user support and developing formal procedures and chains of Responsibility for ethical iteration and this tool reminded me of a blog post by Alex Pierce that I really like Alex fearest was previously the chief legal officer at medium and Yeah, I guess this was a year ago he interviewed or something like 15 or 20 people that have worked in trust in safety and trust in safety includes content moderation, although it's not not solely content moderation and Kind of one of the ideas that came up that I really liked was one of one of the people and so many of these People have worked interest in safety for years at big-name companies and one of them said the separation of product people in trust people worries me because in a world where product managers and engineers and visionaries cared about the stuff it would be baked into how things get built if Things stay this way that product and engineering our Mozart and everyone else is Alfred the butler. The big stuff is not going to change And so I think at least two people in this kind of talk about this idea of needing to better integrate trust and safety Which are often kind of on the front lines of seeing? Abuse and misuse of a technology product integrating that more closely with product and ends so that it can kind of be more directly incorporated and you can have a tighter feedback loop there about What's going wrong? And and how how that can be designed against? Okay, so those were these were well I Link to a few blog posts and research I thought relevant but inspired by the mark Markkula centers tools For protects and hopefully those are practices you could think about potentially implementing at your at your company So next I want to get into a diversity, which I know came up earlier and so Only 12% of machine learning researchers are women as this is kind of a very very dire statistic There's also a kind of extreme lack of racial diversity and age diversity and other factors and this is this is significant a Kind of positive example of what diversity can help with and a post Tracy Chow who was a early early an engineer at Quora and later at Pinterest Wrote that the first feature and so I think she was like one of the first five employees at Quora The first feature I built when I worked at Quora was the block button I was eager to work on the feature because I personally felt antagonized and abused on the site and she goes on to say that If she hadn't been there, you know they might not have added the the block button as soon and so that's kind of like a direct example of how How having a diverse team can help? Somebody kind of kee-kee advice for anyone wanting to increase diversity is to start at the opposite end of the pipeline from from where people talk about The the work play I wrote a blog post Five years ago if you think women and tack is just a pipeline problem. You haven't been paying attention and this was the most popular thing I had ever written until Jeremy and I wrote the the kovat 19 post last month of it. So the second most most popular thing I've written But I linked to a ton of ton of research in there a key statistic to understand is that 41% of women working in tack end up leaving the field compared to 17% of men and so this is something that Recruiting more girls into in decoding or tack is not going to address this problem if they keep leaving at very high rates. I Just had a little peek at the YouTube Chat, and I see people are asking questions there I just want to remind people that we are not That grateful and I do not look at that. If you want to ask ask questions, you should use the Forum thread and and if you see questions that you like then please put them up such as this one How about an ethical issue bounty program just like the bug bounty programs that some companies have No, I think that's a neat idea you have rewarding people for for finding ethical issues And so the the reason that women are more likely to leave tech Is and this was found in a meta-analysis of over 200 books white papers articles Women leave the tech industry because they're treated unfairly underpaid less likely to be fast-tracked than their male colleagues and unable to advance And and too often Diversity efforts end up just focusing on white women, which is wrong Interviews with 60 women of color who work in stem research found that 100 percent had experienced discrimination and their particular Stereotypes buried by race and so it's very important to focus on women of color in in diversity efforts as a kind of the top priority a Study found that men's voices are Perceived as more persuasive fact-based and logical than women's voices even when reading I denticles Scripps Researchers found that women receive more of a feedback and personality criticism and performance evaluations Whereas men are more likely to receive actionable advice tied to concrete business outcomes

[1:48:52] Albertotono (Status: Not Started)
When women receive mentorship is often advice on how they should change and gain more self-knowledge when men receive mentorship It's public endorsement of their authority only one of these has been Statistically linked to getting promoted. It's the public endorsement of authority and all these studies are linked to another post I wrote called the real reason women quit tack and how to address it that a question Jeremy or Yeah, so if you're interested kind of a these two blog post I link to a ton of ton of relevant research on this And I think this is kind of the the workplace is the the place to start in addressing these things So another issue is tech interviews are terrible for everyone So now kind of working one step back from people that are already in your in your workplace, but thinking about the interview process And they wrote a post on how to make tech interviews a little less awful and Went through a ton of research and I will I will say that the the interview problem I think is a hard one I think it's very time consuming and hard to to interview people well But kind of the two most interesting pieces of research I came across one was from triple bite which is a Recruiting company that interviews kind of does this first round technical interview for people and then they interview it Y Combinator it's a Y Combinator company and they under review at my ax Combinator companies and they've this very interesting data set where they've kind of given everybody the same technical interview and then they can see Which companies people got offers from when they were, you know interviewing at many of the same? companies and The number one finding from their research. Is that the types of Programmers that each company looks for often have little to do with what the company needs or does rather they reflect company culture in the backgrounds of the founders and This is something where they even they even gave the advice of if you're job hunting look for try to look for companies where the founders have a similar background to you and that's something that while I That makes sense that's going to be much easier for certain people to do than others and particularly given the the Gender and racial disparities in VC funding that's gonna make a big difference Yes Actually, I would say that was the most common advice. I heard from feces when I became a founder in the Bay Area was when recruiting focus on getting people from your network and people that are Like-minded and similar as possible. That was by far the most common advice that I heard yeah, I mean this is Controversial opinions, I I do feel like ultimately like I get why people hire from their network and I think that long term We all need to be developed. Well particularly white people need to be developing more diverse networks, and that's like a you know Like ten year project. That's not something you can do right when you're Hiring but really kind of developing A diverse network of friends and trusted acquaintances a kind of over time But yeah, thank you for that perspective to Jeremy And then kind of the other study. I found really interesting Was one where they they gave people resumes and in one case So one resume had more academic Qualifications and then one had more practical experience and then they switched the gender one was a woman. When was a man? Or you know male named a female name And basically people were more likely to hire the male and then they would use a post-hoc justification of oh Well, I chose him because he had more academic experience or I chose him because he had more practical experience and that's something I think it's very human to use post hoc justifications, but it's a it's a real risk that Definitely shows up in hiring Ultimately AI or any other technology I developed or implemented by companies for financial advantage ie more profit maybe the best way to incentivize ethnic open is to tie financial or Reputational risk to good behavior in some ways similar to how companies are now investing in cybersecurity because they don't want to be the next Equifax And grassroots campaigns help in better ethical behavior with regards to the use of AI. Oh That's a good question. Yeah, and I think there are a lot of analogies with cybersecurity and I know that For a long time I think was hard for people to make Our people had trouble making the case to their bosses of why they should be investing in cybersecurity Particularly because cybersecurity is you know, something like when it's working Well, you don't notice it and so that can be can be hard to build the case So I think that there there is a place for grassroots campaigns And I'm gonna talk more I'm gonna talk about policy in a bit It can be hard in in some of these cases where there Are not necessarily meaningful alternatives

[1:54:04] quantum (Status: Not Started)
So I do think like monopolies can kind of a kind of make that harder that's it yeah a good good question All right, so next step Actually on this slide is the the need for policy. And so I'm gonna start with a case study of What what's one thing that gets companies to take action? And so as I mentioned earlier a investigator for the UN found that Facebook played a determining role in the ROE hanga genocide I Think the best article I've read on this was by Timothy McLaughlin who did a super super in-depth dive on Facebook's role in Myanmar and People people warned Facebook executives in 2013 and in 2014 and in 2015 How the platform was being used to spread hate speech and to incite violence one person in 2015 even told Facebook executives that Facebook could play the same role in Myanmar that the radio broadcast played during the Rwandan genocide and radio broadcasts played a very terrible and kind of pivotal role in the Rwandan genocide Somebody close to it This said that's not 20/20 hindsight the scale of this problem was significant and it was already apparent and despite this in 2015 I believe Facebook only had four contractors who even spoke Burmese the language of the of Myanmar question That's an interesting one How do you think about our opportunity to correct biases in artificial systems versus the behaviors we see in humans? Example a sentencing algorithm can be monitored and adjusted This is a specific biased judge who remains in their role for a long time I Think I feel a bit hesitant about the it's it'll be easier to correct bias in algorithms because I feel like the You still need people kind of making the decisions to Prioritize that like it requires kind of an overhaul of the systems priorities, I think It also starts with the premise that there are people who can't be Fired or disciplined or whatever I guess maybe for some judges that's true, but they're kind of maybe suggests that judges shouldn't be lifetime appointments Yeah, even then I think you kind of need the the change of heart of the people advocating for the new system Which I think can Would be necessary another case kind of and that that's kind of the critical piece of getting the the people that are Wanting to overhaul the values of a system So returning to to this issue of The Ginga genocide and this is a good kind of continuing continuing issue Yeah, this is something that's just kind of really stunning to me that That there were so many warnings and that so many people tried to raise an alarm on this and that so little action was taken and Even this was last year Zuckerberg finally said that Facebook would add or maybe maybe this was actually this was probably two years ago said that Facebook would add but this is you know after genocides already happening Facebook would add dozens of Burmese language content reviewers So in contrast So we have this this is how Facebook really failed to respond in any any significant way in Myanmar Germany passed a much stricter law about hate speech and nets dgg DC and the The potential penalty would be up to like 50 million euros Facebook hired 1200 people in under a year because they were so worried about this penalty. And so And I'm not saying like this is a law we want to replicate here. I'm just illustrating the difference between being told that you're Contributing are playing a determining role in a genocide versus a significant Financial penalty. We have seen what the one thing that makes Facebook take action is and so I think that that is really significant in remembering what the what the power of a Credible threat of a significant fine is and it has to be a lot more than you know, just like a cost of doing business

[1:59:04] gautam_e (Status: Not Started)
So I I really believe that we need both policy and Ethical behavior within industry. I think that policy is the appropriate tool for addressing negative externalities misaligned economic incentives race to the bottom situations and enforcing accountability however, ethical behavior of Individuals and of data scientists and software engineers working in industry is very much necessary as well Because the law is not always going to keep up. It's not going to cover all the edge cases We really need the people in industry be making kind of ethical ethical decisions as well And so I believe both are significant and important And then and something to note here is that Many many examples of kind of AI ethics issues and I haven't talked about all of these but there was Amazon's facial recognition the ACLU did a study finding that it correctly matched 28 members of Congress to criminal mug shots and this disproportionately included Congress people of color and there's also This was a terrible article. Not that the article was good, but the story is terrible of a City that's using this IBM dashboard for predictive policing and a city official said, oh like whenever you have machine learning it's always 99% accurate, which is false and quite concerning We had we had the issue in so in 2016 Pro Publica discovered that You could place a housing ad on Facebook and say you know Like I don't want Latino or black people or I don't want wheelchair users to see this housing ad which seems like a violation of the the Fair Housing Act and So there's this article and Facebook was like we're so sorry and then over a year later It was still going on ProPublica went back and wrote another article about it There's also this issue of dozens of companies were Placing ads on facebook job ads and saying like we only want young people to see this There's the Amazon creating the recruiting tool that penalized resumes that had the word Women's in that and so something something to note about these examples and many of the examples we've Talked about today is that many of these are about human rights and civil rights? It's a good article by dominique harrison of the Aspen Institute on this And I kind of agree with an email - is framing I mean he wrote there is no technology industry anymore Tech is being used in every industry. And so I think in particular We need to consider human rights and civil rights such as housing education employment criminal justice voting and medical care and think about what rights we we want a safeguard and I do think policy is The appropriate way to do that And I think I mean it's very easy to be discouraged about about regulation But I think sometimes we overlook the the positive Or the cases where where it's worked well And so something I really liked about data sheets for data sets by - Nick Abreu at all. Is that they go through three case studies of how standardization and regular Regulation came to different industries And so it's the electronics industry around circuits and resistors And so there that's kind of around the standardization of you know what the specs are and what you write down about them and The pharmaceutical industry and car safety and and none of these are perfect But it's still it was a kind of very illuminating the the case studies there I mean in particular I got very interested in the car safety one, and there's also a great 99% invisible episode. This is a design podcast about it. And so some some things I learned is that Early cars had sharp metal knobs on the knobs on the dashboard that could Lodge in people's skulls in a crash Non collapsible steering columns would frequently impale drivers and then even after the collapsible steering column was invented It wasn't actually implemented because there was no economic incentive to do so But it's the collapsible steering column has This had saved more lives than anything other than the seatbelt when it comes to car safety And there was also this just this widespread belief that cars were dangerous because of the people driving them and it took it took Consumer safety advocates decades to just even change the culture of discussion around this and to start kind of gathering and tracking the data and to put more of an onus on car companies around safety GM hired a private detective to Trail Ralph Nader and try to dig up dirt on him

[2:03:58] lin.crampton (Status: Not Started)
and so this was really a battle that we kind of I take for granted now and and so Kind of shows how much how much it can take to to change Change the needle they are and then that kind of a more recent Issue is that it wasn't until I believe 2011 that it was required that crash test dummies start representing the average female anatomy in addition to Previously was kind of just crash test dummies or like men and that in a crash with the same impact women were 40% more likely to be injured than men because that's kind of who the the cars were being designed for So I thought I thought all this was very interesting and it can be helpful to kind of remember and remember some of the successes we've had and another area that's very relevant is Environmental protections and kind of looking back and May check the cloud ski has a great article on this but you know just remembering like in the US we used to have rivers that would catch on fire and London had terrible terrible smog and that these are things that were You know very would not have been possible to kind of solve as an individual. We really needed kind of coordinated coordinated regulation on All right, is then on a Kind of closing note. So I think a lot of the problems I've touched fun tonight are really huge huge and difficult problems and they're often kind of very complicated and I When I go into more detail on this in the course, so please please check out the course once it's once it's released I always try to offer some like steps towards solutions, but I realize they're not they're not always you know as satisfying as I would like of like this is gonna solve it and that's cuz these are really really difficult problems and Julia Angwin a former Journalist from ProPublica and now the editor in chief of the mark-up Gave a really great interview on privacy last year that I liked and found very encouraging She said I strongly believe that in order to solve a problem You have to diagnose it and that we're still in the dot the diagnosis phase of this If you think about the turn of the entry and industrialization, we had I don't know 30 years of child labor unlimited work hours terrible working conditions and it took a lot of journalists muckraking and Advocacy to diagnose the problem and have some understanding of what it was and then the activism to get laws changed. I See my role is trying to make as clear as possible what the downsides are and Diagnosing them really accurately so that they can be solvable that's hard work and lots more people need to be doing it Once I found that really encouraging and that I do I do think we should be working towards solutions But I think just at this point even better diagnosing and understanding kind of the complex problems. We're facing is valuable work Couple of people are very keen to see your full course on ethics. Is that something that they might be able to Attend or buy or something. So it will be released for free at some point this summer and it was there was a paid in person version and offered at the data Institute as a certificate kind of Similar to how this this course will was supposed to be offered, you know in person The data ethics one was in person and that took place in January in February and then I'm currently teaching a version version for the Masters of data science students at USF and I will be releasing the free online version and yeah later Sometime before July Thank you. I will see you next time