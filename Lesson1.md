皆さんこんにちは、コーダーのためのディープラーニング、レッスン１へようこそ。今年で4年目になりますが、これまでとは違った特別なバージョンになっています。

まず第一に、完全にシャットダウンされた初日から生放送でお届けします。完全なシャットダウンではありませんがサンフランシスコはほぼ完全にシャットダウンしています。
 この世界的なパンデミックの真っ只中で ２ヶ月間に渡って 録画する予定です。もしこのコースで時々少し慌ただしいように見えたら謝罪します。しかし、このようなことが起きているのが理由です。

もう一つの特別な理由は これを私たちの決定版にしようとしているからです。長い間続けてきたことで やっと自分たちが何を言ってるのか、わかった気がしてきました。シルヴァンと私は実際に本を書いて、ゼロからfastai2というソフトウェアも書きました。 このライブラリについては査読付きの論文も書いています。 ですから、このバージョンのコースは、うまくいけばしばらくは続くと思います。

シラバスはこの本に非常に密接に基づいています。 だから、もしあなたがちゃんと読みたいのであれば、ぜひ買ってください。 そして、「買ってください」と言っているのは、実際にはジュピターノートブックという形で全てが無料で手に入るからです。 これはオライリー・メディアの寛大なご厚意のおかげです。 コースのウェブサイトにアクセス方法が掲載されていますが、ここにあるfastbookのレポでは全ての内容を読むことができます。今のところ、ご覧のようにそれは草稿ですが、あなたがこれを読む頃にはそうはなっていないでしょう。

そこで、ここで大きな要望があります。 ジュピターノートブックとして無料で読むことができますが、Kindleや紙の本などで読むのと比べると便利ではありません。 だから、これをPDFにするのはやめてくださいね。読むことを目的としたフォーマットにはしないでください。 そもそもの目的はよかったらあなたに買ってもらうことです。 オライリーの寛大さを利用して、タダで貰えないと分かっているものを作るのはやめてください。実際にはそれが、私たちがこれを提供しているライセンスなのです。これは主にまともな人間であってくださいとの要請です。 もし他の誰かがまともな人間ではなく、書籍版を盗んでいるのを見たら、"そんなことしないでくれ、迷惑だから "と言ってあげてください。 そして、そんな人にならないようにしましょう。ということで、どちらにしても、本の中のシラバスに沿って読んでください。

このノートにはいくつかのバージョンがあります。全部の文章、写真、全てが載っている完全なノートがあります。私たちは実際にジュピターノートブックを印刷する本にするためのシステムを作ったたのですが、それがちょっと変な感じに見えることがあります。
例えば、ここに変な表がありますが、実際の本の中を見てみると、実際にはちゃんとした表のように見えますよね。
私たちと言えば、私たちとは誰ですか？「私たち」の重要な部分の一つがシルヴァンです。シルヴァンは本とfastaiバージョン2ライブラリの私の共著者ですので、彼は私の犯罪のパートナーです。もう一人はレイチェル・トーマスです。レイチェル、挨拶をしに来てください。彼女はfastaiの共同設立者です。

こんにちは、私はfastaiの共同創設者です。そして、あ、低く。すみません。私はジェレミーよりも背が高いんです。そして私はサンフランシスコ大学の応用データ倫理センターの創設ディレクターです。このコースに参加できることを本当に楽しみにしています。そして私の声がフォーラムからの質問を聞く声になります。

レイチェルとシルヴァンは、このグループの中で実際に数学を理解している人たちです。私は単なる哲学の卒業生です。レイチェルは博士号を持っています。シルヴァンは数学について10冊の本を書いているので、もし数学の質問が来たら、私はそれを委託すかもしれません。でも、この課題をよく理解している人たちと一緒に仕事をする機会があるのは、とても嬉しいことです。はい、はいレイチェル、ありがとうございます。

レイチェルが言ってたように、彼女の知っている他の領域は、実際の世界クラスの専門知識を持っているのは、データ倫理であり、彼女は応用データ倫理センターの創設ディレクターです。
サンフランシスコ大学です 
サンフランシスコ大学ですね、ありがとうございます。私たちはコースを通してデータ倫理について話します なぜなら、私たちはたまたまそれが非常に重要だと考えているからです。私が一般的にそれらを発表することになりますが、彼女は実際に何を言っているのかを知っているので、全体的にはレイチェルの仕事をベースにしています仕事に基づいたものになります。でも、彼女のおかげで、私も私が話していることを少しは知っています。そうですか、そういうことですね。

あなたはここにいるべきでしょうか。あなたがを理解することにどんな目的があるのか（私は正しいボタンを押したと思った）深層学習を理解しようとすることに意味があるのでしょうか？
OK 、あなたは何を・・・あなたはここにいるべきでしょうか。あなたが深層学習を学んでメリットがありますか？、あなたがあまり頭がよくないですか？、または十分高速なコンピューターを持っていないですか？、などなど。それは多くの人々が私たちに言ってくることです。彼らは、博士号を持ったチームとGPUを搭載した大規模なデータセンターが必要だと言っています。そうでなければ無意味だと。
心配しないでください、それは全く真実ではありません、真実からこれ以上遠くなることはありません。実際、大多数の世界的な研究や世界的な産業プロジェクトの多くは、fastaiの卒業生やfastaiライブラリを使ったプロジェクトや他の場所から生まれたもので、大学院レベルの技術的な専門知識を持たない人たちから、数十、数百のデータポイントを使って単一のGPUで作られたものです。私はといえば大学レベルの専門知識もありません。私は哲学部です。しかし、ディープラーニングを使って素晴らしいことをするためには、たくさんの数学は必要ないし、たくさんのデータも必要ないし、高価なコンピュータも必要ないという明確な実証的証拠がたくさんあります。だから我々と一緒に我慢して付いてきてみてください。あなたは大丈夫です。
このコースを受けるには コードを書く必要があります。 できればPythonでコードを書く方法を 知っていることが望ましいです。 他の言語をやったことがあれば Pythonを学ぶことができます。 もしあなたがこれまでにやったことのある言語がMatlabのようなもので、スクリプトのようなものを使ったことがあるのであれば、少し難しく感じるかもしれません。しかし、それでいいんです。頑張ってください。Pythonはどんどん学習していけばいいのです。

深層学習を学ぶ意味はありますか？何かディープラーニングの得意なことはありますか？もしあなたが脳を作りたいと思っているのであれば、それはAGIです。なのでこのコースがあなたをの役に立つとは約束できません。

AGIは 人工知能（Artificial General Intelligence）の略です。

ありがとうございます しかし私が言えることは これらの分野のすべてにおいて 少なくとも多くのバージョンでは ディープラーニングが最もよく知られたアプローチであるということです。 ですから、これが有用なツールであるかどうかは、現時点では推測ではありません。たくさんの、たくさんの、たくさんの、たくさんの場所で役に立つツールなのです。極めて有用なツールです。そして、これらの多くのケースでは、人間のパフォーマンスと同等かそれ以上である（少なくとも、この種の分野で人間が行うことのいくつかの特定の狭い定義によると）。ディープラーニングは非常に素晴らしいものです。ここでビデオを一時停止して、興味がありそうなものをいくつかピックアップしてみてください。そしてそのキーワードとディープラーニングをGoogleで検索すれば、たくさんの論文や事例などが見つかるはずです。

ディープラーニングは、ニューラルネットワークの背景から来ています。見ての通り、ディープラーニングはニューラルネットワーク学習の一種です。深いものです。それが何を意味するのかは後ほど正確に説明します。そして、ニューラルネットワークは確かに新しいものではありません。少なくとも1943年までさかのぼります マックロックとピッツが 人工ニューロンの数学モデルを作成した時です。 そして、それがどこに到達するかについて非常に興奮しました。50年代には、フランク・ローゼンブラットが、その上に数学モデルを構築しました。彼は基本的に、その数学モデルに微妙な変更を加えました。そして彼は、これらの微妙な変更で、「人間の訓練や制御なしに 周囲の環境を看取、認識、識別できる機械の誕生を 目撃することができる」と考えました。そして彼はこの驚異的なものの開発を監督したのです。コーネル大学のマーク１パーセプトロンです この写真は１９６１年だったと思います。
ありがたいことに今日では、ニューラルネットワークを構築するために、ニューロンからニューロンへ（人工ニューロンから人工ニューロンへ）配線をを走らせる必要はありません。しかし、多くの接続が行われているという考えが見えてきます。 このコースでは、接続（コネクション）という言葉をよく耳にするでしょう。

それから、私たちは最初のAIの冬を迎えました。これは、MITのマービン・ミンスキー教授とパパートが ローゼンブラットの発明について パーセプトロンという本を書きました。その中で 彼らが指摘したのは、これらの人工ニューロンデバイスの単一の層は、実際にはいくつかの重要なことを学ぶことができないということです。ブール演算子のXOR演算子のような簡単なものを学ぶことは不可能でした。同じ本の中で、彼らは、人工ニューロン装置を何層にも重ねて使えば、実際に問題が解決することを示しています。人々は無視 - 本のその部分に気づかなかったのです。そして、制限にだけ気付き、人々は基本的にニューラルネットワークはどこにも行かないと決めました。そして何十年もの間、彼らはほとんど姿を消してしまいました。1986年までは。

その間にいろいろなことがありましたが、1986年に大きなことがありました。MITが「並列分散処理」という本を出版、2巻に分けてシリーズ化したものです。その中で、並列分散処理（Parallel Distributed Processing）と呼ばれるものを説明していました。並列分散処理とは、たくさんの処理装置と、アクティベーションと、出力機能と、接続性のパターンと、伝播ルールと、活性化ルール、学習ルールを持った処理ユニットが、環境の中で動作しているというものです。そして、これらの要件を満たすものが、どのようにして理論的にはあらゆる種類の驚くべき仕事をすることができるのかを説明しました。
これは、多くの、多くの研究者が協力して取り組んだ結果です。このプロジェクトにはグループ全体が関わっていて、それがこの非常に重要な本につながったのです。
そして、私にとって興味深いのは、このコースを受講した後に、この写真を見ていただければ、私たちがまさにこのようなことをしていることがわかると思います。私たちが学んでいることは、これらの8つのことをどのようにするかということです。環境が含まれているのは興味深いことですが、これはデータサイエンティストが無視することが多いからです。モデルを構築し、それを訓練し、何かを学習しました。それがどのようなコンテキストで動作するのか？それについては、次のレッスンでも度々お話しします。
80年代に、これがリリースされた後、人々は第二層のニューロンを構築し始めました。 ミンスキーの問題を回避するためです。 実際に、ニューロンの層を1層追加することで、これらのニューラルネットワークを使って、どのような数学モデルでも、どのような精度でも近似できることが、数学的に証明されました。これは、ミンスキーとは正反対のことでした。 "我々にできないことは何もない"ということが証明できたのです。

私がニューラルネットワークに関わり始めたのは、その頃でした。いや、少し遅れていました。私が関わったのは90年代前半だったと思います。 その頃、業界では非常に広く使われていました。私は銀行のリテール向けのターゲティングマーケティングのような、非常につまらないものに使っていました。それらの企業は、大金を持った大企業で、それらを利用している傾向がありました。しかし、ネットワークが大きすぎたり、遅すぎたりして役に立たないことが多いのも事実でした。確かにいくつかのことには役立っていましたが、私には何かの理由で期待通りの成果を上げているようには感じられませんでした。私が知らなかったのは、私が個人的に会った誰も知らなかったのですが、実際には30年前に、実用的な性能を得るためには、より多くのニューロンの層が必要だということを示した研究者がいたということです。数学的には、理論的には、たった一枚の層を増やせば、いくらでも精度を上げることができますが、実際には、それ以上の層が必要なのです。 それを良いパフォーマンスで行うためには、より多くの層が必要です。だから、ニューラルネットワークにもっと多くの層を追加すると、ディープラーニングが得られます。つまり、深いというのは神秘的な意味ではありません。それは単に、より多くの層を意味します。ただ一枚の層を追加するよりも、より多くの層を意味します。そのおかげで、ニューラルネットは今、その潜在能力を十分に発揮しています。ディープラーニングの得意分野で見たように ローゼンブラットが正しかったと 言えるようになりました。 我々は、人間の訓練や制御がなくても、周囲の環境を看取し、認識し、識別することができる機械を持っている。確かにその通りです。現在の技術に基づいて、この発言に議論の余地はないと思います。

だから、私たちはその方法を学ぶことになります。しかしあなた方の今までの数学や技術教育とは正反対の方法で学習していきます。シグモイド関数の２時間の授業や、線形代数の勉強、微積分の復習から始めるのではありません。その理由は、教え方や学び方を研究している人たちが、それがほとんどの人にとって正しい方法ではないことを発見したからです。ハーバード大学のデビッド・パーキンス教授や、同じような研究をしている他の人たちが、ゲーム全体をプレイするという考え方について話しています。 ゲーム全体をプレイすることは スポーツの例えに基づいています。野球を誰かに教える場合、教室に連れて行って放物線の物理学やボールの縫い方、100年の野球政治の3部作の歴史などを教え、10年後には試合を見させる。そして20年後にやっとゲームをさせる。これは数学教育のやり方に似ていますよね。野球の場合は、第一段階として、「野球を見に行こう」ということになります。どうですか？面白かったでしょ？ あの人、あそこで走者を出して、相手がボールを投げる前に、ヒットを打ってみないか？よし、君がボールを打って、僕がキャッチして、そして君があそこへ走って...と、ステップ１からゲーム全部をプレイしていることになります。

さらに付け加えるならば、人々は始めたばかりの頃には、チームが揃っていなかったり、9イニングをフルでプレーしていなかったりもすることが多いのですが、それでもゲームの全体像を把握していることが多いです。 

だから、ほとんどの人にこれが役立つ理由はたくさんありますが、みんながみんなそうではありません。基礎や原則から物事を構築していくのが好きな人がごく一部いて、驚くことではありませんが、大学ではそういう人の割合が非常に高くなっています。なぜなら、学者になる人たちは、（私によれば）物事の教え方が逆さまになっていることで成功している人たちだからです。しかし、大学の外では、ほとんどの人がトップダウンの方法で最もよく学びます。完全なコンテクストから始める方法です。ですから、7つの原則の第2段階では、最初の3つの原則にだけ言及しますが、ゲームをプレイする価値のあるものにするということです。野球をやっていると、競争があります。スコアをつけて、勝利を目指して、地域のチームを集めて、みんなで競い合います。誰が一番多くの得点を取ったかなどの リーダーボードがあります。 これはあなたがやっていることをきちんとやっているかどうかを確認することです。あなたはそれを全体的なものにしてコンテクストと関心を提供しています。 

 深層学習へのfastaiアプローチでは、今日はモデルを最後まで訓練します。私たちは実際にモデルを訓練します。そしてそれはただのくだらないモデルではありません。今日から最先端のワールドクラスのモデルです。 今日か次のレッスンのどちらかから(状況に応じて)、あなた自身の最先端の世界クラスのモデルを構築することを試みます。

そして、ハーバード大学の7つの原則の3番目は、難しい部分に取り組むことです。これは、この練習、意図的な練習の考え方に似ています。難しい部分を鍛えるということは、ただ単に毎回バットを振り回していればいいというわけではありません。適切なトレーニングをして、自分の苦手な部分を見つけて、どこに問題があるのかを把握して、それに一生懸命取り組むのです。ディープラーニングの文脈では、我々は物事を単純化しないことを意味します。コースが終わる頃には微積分をやっているでしょう。線形代数もやっているでしょう。コードのソフトウェアエンジニアリングを行っているでしょう。これらのことを練習するのは難しいので、粘り強さと確約（コミットメント）が必要です。しかし、うまくいけば、なぜそれが重要なのか理解できるでしょう。それは何かを練習し始める前に、もうそれを使っていて、なぜそれが必要なのかが分かっているからです。あなたのモデルをより良くするためには、まずそのコンセプトを理解する必要があります。
伝統的な大学の環境に慣れている人にとっては、これはかなり奇妙に感じるでしょうし、多くの人から聞くのは「理論の勉強に時間をかけすぎて、モデルのトレーニングやコードを書くのに十分な時間をとらなかったことを後悔している（1年fastaiを勉強した後で）」という事。 これは、「違うやり方をしていればよかった」と言う人からのフィードバックの中で、一番多いものです。それはそれでいいんです。だから、できる限り、あなたがここにいるから、このアプローチに沿ってやってみてください。私たちはこのソフトウェアスタックを使うつもりです - レイチェル 何かありましたか？

アプローチについて、もう一つだけ言わせてください。 私たちの多くが長年ボトムアップの伝統的な教育アプローチで過ごしてきたので、最初は非常に違和感を覚えることがあると思います。私は今でも時々、この考えにコミットしているにもかかわらず、このアプローチに違和感を覚えることがあります。それは時々自分自身をキャッチして、「詳細を知らなくても平気でいなければいけない」というのには慣れていないと感じることもあるし、 間違っているとさえ感じることもあります。 例えば... "ちょっと待って、私は何かを細部まで理解していないまま使っている" とか。でも、そういう詳細は後で到達することを信用する必要があるんですよね。

私はそのようなことに多くの時間を費やしていないので、共感することはできません。しかし、一つ言えるのはこの方法で教えることはとてもとても難しいという事です。私はしばしば、基礎からのアプローチに戻ってしまうことがあります。なぜなら、「ああ、これを知っておく必要があるんだ。これを知っておく必要がある。これを知る必要がある。これをする必要がある。そうすれば、これを知ることができる。」というように教えるのは簡単です。 だから私はこの方法を教えるのはずっと難しいと思いますが、それだけの価値があるといいなと思います。私たちは、この形式をディープラーニングを取り入れる方法を考え出すのに長い時間を費やしました。しかし、ここで私たちを助けてくれるものの一つは、私たちが利用できるソフトウェアです。もしPythonを使ったことがないのであれば、Pythonは非常に柔軟で表現力があり、使いやすい言語です。私たちが好きではない部分もたくさんありますが、全体的にはとても気に入っています。そして、最も重要なことは、ディープラーニングの実践者や研究者の大多数がPythonを使っているということです。

Pythonの上には、PyTorchとTensorFlowの2つのライブラリがあります。ここには非常に急速な変化がありました。TensorFlow は数年前まで私たちが教えていたものです。数年前までは誰もが使っていたものです。

基本的にTensorFlowは非常に困難な状況に陥りました。そしてPyTorchというソフトウェアが登場したのですが、これはもっと簡単に使えて、研究者にとってはもっと便利なものでした。この12ヶ月間で、主要なカンファレンスでPyTorchを使った論文の割合が20%から80%に、TensorFlowを使った論文の割合が80%から20%になりました。つまり、 基本的には、実際に技術を構築している人たちはみんなPyTorchを使っていて、業界の動きはもう少しゆっくりですが、来年か再来年には業界でも同じようなことが起こるでしょう。

PyTorchの特徴は、非常に柔軟性が高く、柔軟性と開発者の利便性を考慮して設計されていますが、初心者向けではありません。 つまり、PyTorch を使って簡単に何かを素早く構築できるようにするための高レベルの API はありません。 

そこで、この問題に対処するために、PyTorchの上にあるfastaiというライブラリを用意しました。FastaiはPyTorchの高レベルAPIとして最も人気のあるものです。

私たちのコースがあまりにも人気があるために、fastaiは初心者のために設計されているとか、教えるために設計されていると勘違いしている人がいますが、初心者やティーチングのためだけに設計されているだけでなく、業界の実務者や研究者のためにも設計されています。

私たちは、どのレベルの人たちにとっても最高のAPIであるために、レイヤードAPIと呼ばれるものを使っています。シルヴァンと私が書いた査読付きの論文には、私たちがどのようにしてそれを行ったかが書かれています。これは、ソフトウェアエンジニアの人達にとっては珍しくも驚くこともないと思います。完全に標準的なソフトウェアエンジニアリングのプラクティスですが、私たちが見てきたディープラーニングライブラリにはないプラクティスです。基本的には多くのリファクタリングとデカップリングが行われており、そのアプローチを使うことで、超低レベルの研究を行うことができ、最先端の生産モデルを作成することができ、超簡単で初心者でも世界レベルのモデルを作成することができるライブラリを構築することができました。
これが基本的なソフトウェアスタックです。 他にも、途中で学ぶことになるソフトウェアがあります。しかしここで言及すべき主なことは、ソフトウェアスタックは実際にはあまり重要ではないということです。このソフトウェアスタックを学んだ後、仕事でTensorFlowとKerasを使う必要が出てきたとしても、1週間もしないうちに切り替えることができるでしょう。多くの学生がそうしてきましたが、問題になったことは一度もありません。重要なのはコンセプトを学ぶことなので、今回はそのコンセプトに焦点を当てていきます。使用しなければならないボイラプレートの量を最小限に抑えるAPIを使用することで、重要な部分に集中できることを意味します。実際のコードの行は、あなたが実装しているコンセプト（概念）にはるかに対応しています。

GPUマシンが必要になります。GPUはGraphics Processing Unitであり、具体的にはNvidia GPUが必要です。他のブランドのGPUは、どのディープラーニングのライブラリからもうまくサポートされていないのです。買わないでください。もし既に持っていても、それを使うべきではないでしょう。その代わりに、私たちがすでに用意しているプラットフォームを使ってください。GPUマシンのシステム管理やドライバのインストールなどに時間を割くのは気が散るだけです。Linuxで実行してください。私たちだけでなく、みんながLinuxで実行しています。自分の人生を楽にしてください。ディープラーニングを学習するのは、あらゆる種類の難解なハードウェアサポートの問題なしでも十分に難解です。
無料で使えるオプションがたくさんあるので、ぜひ使ってみてください。もし無料ではないオプションを使っている場合は、インスタンスをシャットダウンすることを忘れないでください。つまり、あなたは世界のどこかにあるサーバーを起動して、あなたのコンピュータからそれに接続して、トレーニングをしたり、モデルを構築したり実行することになります。ブラウザのウィンドウを閉じたからといって、サーバが全体的に停止するわけではありません。だからシャットダウンするのを忘れないでください。そうしないとお金を払うことになります。

Colabは無料の素晴らしいシステムです。有料のサブスクリプション版もあります。Colabには気をつけて欲しいことがあります。私たちがお勧めする他のシステムのほとんどは、あなたの作業を自動的に保存し、いつでも戻ってくることができます。Colabはそうではありません。そのため、フォーラムの「Colabプラットフォーム」のスレッドをチェックして、それについて学ぶようにしてください。

それで、フォーラムについても触れておきますが...　フォーラム（https://forums.fast.ai/）は本当に重要です。そこで、すべての議論や設定、すべてのことが行われます。例えばセットアップのヘルプスレッドがあり、Colabをどのようにセットアップするのがベストなのかを知ることができたりそれについてのディスカッションを読んだり質問もできます。質問をする前に検索することを忘れないでください。あなたがこのコースを今受講している人でない限り、おそらく以前にも質問されたことがあると思いますので。

では、[咳]... ステップ１は、フォーラムやコースのウェブサイトの指示に従って、サーバーをセットアップします。コースのウェブサイトには、各プラットフォームのステップバイステップの説明がたくさんあります。それぞれのプラットフォームは価格、速度、可用性などが異なります。これらの指示に従うことが終わると、その指示の最後のステップでは、次のようなものが表示されます：コースV4のフォルダ、つまり私たちのコースのバージョン4です。

このビデオをご覧になる頃にはもっとたくさんのものが入っていると思いますが、ノートブックスフォルダを意味する「NBS」が入っています。これをクリックすると、コースのすべてのノートが表示されます。下にスクロールして、app_jupyter.ipynbというファイルを見つけてください。それをクリックすると、ここからジュピターノートブックの学習を始めることができます。

ジュピターノートブックって何？ジュピターノートブックは何かを入力して、Shift-Enterキーを押すと答えが返ってきます。あなたが入力しているのはPythonのコードで、出てくるのはそのコードの結果です。Pythonだったら何でも入力することができます。XはXかける４。X足す１。見ての通り、表示すべき結果があればいつでも結果を表示します。なので、以前に少しコーディングをしたことがある人は、これがREPLだとわかるでしょう。R-E-P-L、読み込み（read）、評価（evaluate）、印刷（print）、ループ（loop）のことです。ほとんどの言語には何らかの REPL があります。ジュピター ノートブックの REPL は特に興味深いもので、見出し、グラフィカルな出力、インタラクティブで、マルチメディアのようなものも持っています。これは本当に驚くべきソフトウェアです。本当に大きな賞をいくつか受賞しています。Bashのようなシェル以外では最も広く使われているREPLだと思います。非常に強力なシステムです。私たちはこれを愛しています。私たちの本も全部このシステムで書いていますし、fastaiのライブラリも全部このシステムで書いていますし、私たちの授業も全部このシステムで行っています。IDEでほとんどの仕事をしてきた人にとっては、非常に馴染みのないものです。おそらく初めてGUIからコマンドラインに移行したときと同じくらいの気まずさを感じると思ってください。別物です。REPL ベースのシステムに慣れていない人にとっては、超変な感じになるでしょう。しかし、それは本当に素晴らしいことなので、続けてみてください。

ここで何が行われているかというと、私が見ているこのウェブページでは、サーバが行うべきことを私が入力すると、サーバが行った計算の結果を表示してくれます。つまり、サーバーはどこか別の場所にあるということです。私のコンピュータ上では実行されていないのですね。コンピュータ上で動いているのはこのウェブページだけです。

しかし、私が何かをしていると、例えば、「XはXの3倍に等しい」と言った場合、サーバーの状態が更新されます。ここで言うサーバーの状態とは現在のXの値が何であるということで、その値を 表示することができます。 今のXは以前とは違うものになっています。この行をここでやっても、先ほどのX＋1は変わらないんですよね。ということは、ジュピターのノートを見たときに、サーバの現在の状態が表示されていないということですね。印刷した時の状態を表示しているだけです。これはBashのようなシェルを使うのと同じで "ls" と入力します。そしてファイルを削除する。以前に印刷した "ls" はそれに従ってって変更されることはありません。それが REPL の一般的な動作のようなものです。これも含めて。

ジュピターノートブックには2つのモードがあります。一つは編集モードで、セルをクリックするとカーソルが点滅して、左右に移動して入力することができます。このモードではキーボードショートカットはあまりありません。ひとつ便利なのは「control」か「command」＋「/」で行をコメント化したり、コメントを外したりしてくれます。知っておくべき主なものは、実際にセルを実行するために「shift」+「enter」です。その時点でもうカーソルの点滅はありません。そレは、今はコマンドモードになっているということです。編集モードではありません。上に行ったり下に行ったりしながら、違うセルを選択しています。

コマンドモードで移動しながらセルを選択しているわけです。キーボードショートカットもたくさんあります。H を押すと、そのリストが表示されます。「control」や「command」で始まるものはあまりなく、文字だけで構成されていることがわかります。だからVimのようなものを使ったことがれば、この考え方の方に馴染みやすいと思います。だから例えば、私はコピーするために "C "を押して、貼り付けるために "V "を押すと、セルがコピーされます。または "X" で切り取ります。"A "で新しいセルを上に追加します。 それから様々な数字キーを押して 見出しを作ります。 だから、数字の2は見出しレベル２を作成します。見ての通り、私はコードだけでなく、実際にフォーマットされたテキストを入力することができます。フォーマットされたテキストはMarkdownです。こんな感じです。私の番号付きリストはうまくいかなかった。これです。これがMarkdownです。以前にMarkdownを使用したことがない人、これはフォーマットされたテキストを書くのに非常に便利な方法です。 非常に広く使われています。 とても便利なので学んでください。そして、ジュピターでの作業に必要です。

私たちの本のノートブックを見てください。例えば、ここにはすべての種類のフォーマットやコードの例があります。なので、app_jupyter.ipynb を見てみてください。ここでは、例えばプロットを作成する方法を見ることができます。また、リストを作成することもできます。ライブラリをインポートしたり、画像の表示などもできます。新しいノートブックを作成したい場合は、"New" "Python 3 "で新しいノートブックを作成することができます。デフォルトでは "Untitled "という名前になっているので、好きな名前に変更することができます。そうすると、リストの中に "newname "という名前が表示されます。ジュピターについて知っておくべきもう一つのことは、ターミナルにアクセスするのが簡単にできるということです。
ターミナルの使い方を知っていれば 。でもこのコースのためには（ 少なくとも前半では）必要はありません。
 
新しいターミナルに行く場合・・・ここで私が持っているターミナルを見ることができます。
 
これらのノートブックはGithubのリポジトリに添付されています。Githubを使ったことがなければ問題ありませんが、基本的にはサーバーに接続されていて、そのサーバーで随時私たちはノートブックを更新していきます。
 
そして、コースのウェブサイトやフォーラムで最新バージョンを確認する方法をお伝えします。あなたが最新版を手に入れるとき、あなたの変更と衝突したり、上書きしたりするを避けるたいものです。そのために、実験を始めるときには、ノートブックを選択してDuplicate（複製）をクリックして、コピーしたノートブックで作業を始めるのも悪くないでしょう。そうすれば、最新のコース教材の更新があったときに、あなたが実行していた実験に支障をきたすことはありません。
 
このように、2つの重要なリポジトリがあります。一つは先ほど見たファストブックのリポジトリで、これはすべての出力や長所などが掲載されているフルブックのようなものです。そしてもう一つはコースV4のリポジトリです。これはファストブックのリポジトリにあるノートと全く同じものです。このノートでは、すべての文章とすべての写真と出力を削除し、見出しとコードだけを残しています。この場合、いくつかの出力を見ることができますが、それは私がコードを実行したからです。他は何もない・・・いや、出力を残していると思います。それを残しておくかどうかはわかりません。なので、出力が見えても見えなくても構いません。


これを使うというアイデアは、これらがきっとあなたが実験したいバージョンです。なぜなら、本を読んで何も考えずに実行するのではなく、各ステップを実行しながら何が起こっているのかを考えることを強制されるからです。本に書かれていることは何だったのか、なぜこのようなことが起こったのか、何か忘れてしまったら本に戻るというように、小さな裸の環境でやってほしいと思っています。



もう一つは、コースV4版とファストブック版の両方にアンケートがあることです。かなりの数の人（例えば復習している人たち）が、実際に最初にアンケートを読んだと言ってくれました。シルヴァンと私は何週間もかけてアンケートを書いてきました。その理由は、それぞれのノートから何を読み取ってほしいかを考えているからです。最初にアンケートを読んでもらうと、私たちが大切だと思うことは何なのかが分かると思います。次に進む前に知っておくべきことは何か。最後にまとめのようなセクションがあって「最後に知っておくべきことはこれとこれ・・・」というのではなく、その同じ目的をアンケートで達成することにしました。なので、次の章に移る前に必ずアンケートをしてください。すべてを正しく理解する必要はありませんし、質問に答えるのは、ノートのその部分に戻って散文を読むくらい簡単です。だから、何か見逃したことがあったら、ちゃんと戻って読んでください。私たちはあなたがこれらを知っていることを前提としています。だから先に進む前にこれらのことを知らないと イライラすることになりかねません。とはいえ、何度かやってみてどうしても行き詰まったら、次の章に移って、あと2～3章をやってから戻ってきてください。 もう2、3章やった後にはもうちょっと視野が広がってるかもしれません。私たちは違う方法で何度も説明し直すようにしているので、もし試してみて行き詰ったとしても大丈夫です。

さて、それではノートの最初の部分を実行してみましょう。これは01_introのノートブック。これは第1章で、ここが最初のセルです。セルをクリックすると・・・デフォルトではツールバーにヘッダーが表示されます。これをオンにしたりオフにしたりすることができます。私はいつもオフにしています。このセルを実行するには、Run（再生）ボタンをクリックするか、先ほど言ったようにシフト・エンターキーを押してください。このセルではRunボタンをクリックします。すると星が表示されるので、実行中だというのがわかり、この進行状況バーがポップアップします。これは数秒かかります。実行中には、いくつかの結果が表示されます。私たちと全く同じ結果を期待しないでください。モデルのトレーニングにはランダム性がありますが、それは大丈夫です。我々と全く同じ時間を得ることは期待しないでください。もしあなたが本当に古いGPUを持っていない限り、この最初のセルに5分以上かかるなら、それはおそらく悪い兆候です。フォーラムに参加して、何が問題なのかを調べてたほうがいいです。	またはWindowsを使っている場合。今のところWindows ではあまりうまく動作しません。

まだすべてのコードが何をするかわからなくても心配しないでください。私たちはただモデルを訓練できるかどうかを確認しているだけです。

これで実行が終了しました。見ての通り、いくつかの情報が出力され、この場合、何かを行う際に0.005のエラー率があることを示しています。何をしているのでしょうか？

ここで何をしているのかというと、実際にデータセットを取得しているのですが、これはペットデータセットと呼ばれていて、猫と犬の写真のデータセットです。これは猫と犬の写真のデータセットです どれが猫でどれが犬なのかを調べています。 見ての通り、1分もしないうちに、0.5%の誤差率でこれを行うことができました。つまり、かなり完璧にできるということです。
　
これで最初のモデルを訓練しました。 どうやったのかは分かりませんし、何をしていたのかもわかりません。しかし、私たちはは確かにモデルを訓練しました。これで良いスタートが切れました。 見ての通り、1台のコンピュータでかなり速くモデルを訓練することができます。ご存知の通り、その多くは無料で手に入入ります。

もう一つ、もしあなたがMacを持っているのであれば・・・　ブラウザで何を実行しているかという点では、WindowsでもMacでもLinuxでも関係ありません。しかし、Macを持っている人は、そのGPUを使おうとしないでください。Macは、いえ、 AppleはもうNvidiaのGPUすらサポートしていません。だから、それは本当に良い選択肢にはなりません。だからLinuxを使って下さい。そうすれば、あなたの生活はずっと楽になります。

そうですね、最初にすべきことは実際に試してみることです。犬から猫を選ぶモデルを訓練したと言っても、本当にできるかどうか確認してみましょう。このセルを見てみよう。これは面白いでしょう？ widget.FileUploadオブジェクトを作成して表示すると、これは実際にクリック可能なボタンです。だから、先ほども言ったように、これは珍しいREPLです。このREPLではGUIを作成することもできます。だから、私はこのファイルのアップロードをクリックして、猫の写真を選択できます。  そして、アップロードしたデータを画像に変換することができます。猫がいます。そして予測をすることができ、結果は猫です。99.96%の確率で。 

ということで、自分で選んだ画像をアップロードできることがわかりますね。これをやってみてください。猫の写真をネットで探すか 自分で撮ってきて、 アップロードした写真が猫であ
ることを確認してください。

 これは猫の線画ではなく 猫の写真を認識できるものです 
これからのコースでわかるように、この種のモデルは、あなたが与えた情報からのみ学習することができます。これまでのところ、私たちは猫の写真しか与えていません。アニメの猫でもなく、描かれた猫でもなく、抽象的な猫の表現でもなく、ただの写真です。

 

では次に実際に何が起きたのか？というのをみていきます。

ご覧の通り、今のところ、私はここにいい情報を得ていません。もしこういう出力をノートに見たら、 File → Trust Notebook → Trustという操作をして下さい。これはジュピターに表示に必要なコードの実行を許可すると伝えているのです。セキュリティ上の問題がないと。これで出力が表示されます。時々、このような奇妙なコードを見ることがあります。これは以下の図を作成するコードです。このようなコードは基本的に隠します。しかし時には見せるかもしれません。しかし，一般にこのようなコードは無視して、出力されている図に集中してください。ですので，毎回こういったコードを紹介するつもりはありません。代わりにスライドを見てみましょう。ここでやっていることは、機械学習です。ディープラーニングは機械学習の一種です。機械学習とは、プログラミングと同じで、コンピュータに何かを処理させる方法です。しかし、写真に映っているのが猫か犬かを識別するプログラムを書くのは想像しにくいでしょう。写真の中の犬と猫を認識するプログラムを作るために、ループや変数の代入、条件式をどう使えば良いのでしょうか？これは大変難しいです。本当に難しいです。ディープラーニングの時代までは、一見簡単そうな作業（犬猫の識別）を正確にこなすモデルは誰も持っていませんでした。必要なステップを書き切れないからです。 通常は、いくつかの入力を処理し，いくつかの結果を返す関数を書きます。ですので，人間がステップを書き下す方式のプログラムは写真の認識などが難しいのです。1949年に、アーサー・サミュエルという人が、猫や犬の認識といった問題をプログラムに解かせる方法を見つけようとし始めました。1962年にその方法を説明しました。まず彼は問題に関して次のように述べました。この種の計算のためのプログラムを書くのは困難な作業である。なぜなら、処理に含まれる微細なステップをうんざりするほど詳細に綴らなければならないのだから。コーダーなら誰もが知っているように，コンピュータは巨大なマヌケである。そこで彼は、コンピュータに正確な手順を教えるのではなく、問題の例を与えて、その問題を解く方法をコンピュータに考えさせよう、と言いました。そして、1961年までに、彼はチェッカープログラムを作成し、 コネチカット州のチャンピオンを倒しました。「実際のパフォーマンスから重み（パラメータ）の有効性を自動でテストする手段とパフォーマンスを最大化するように重み（パラメータ）を変更するメカニズムを構築する」。この文章が肝心です。そして、かなりトリッキーな文章なので、時間をかけても大丈夫です。プログラムへの入力があって、プログラムが何かを出力するという基本的な考え方は同じです。ここではプログラムではなくモデルと呼びましょう。モデルへの入力と結果。そして、重みと呼ばれる2つ目のものがあります。基本的な考え方としては、このモデルは、入力，例えばチェッカーボードの状態だけでなく、モデルがどのように動作するかを記述する重みあるいはパラメータに基づいて出力を作成するものです。

チェッカーの作戦を列挙し、パラメータを使って，サミュエルは重みと呼びますが，それぞれを説明することができて，現在の重みの割り当てが、実際のパフォーマンスの面でどれだけ効果的かを検証する方法があれば、言い換えれば、チェッカーをプレイするための特定の戦略が、ゲームの勝ち負けにつながるかどうか、そして、パフォーマンスを最大化するように重み
の割り当てを変更する方法があるとします。そうすると、重みを一つずつ増やしたり減らしたりしてみて、少しでも良いチェッカーの戦略があるかどうかを調べるというのを何回も何回もやってみると、最終的にはそのような手順が完全に自動化されて、機械がその経験から学習するようにプログラムされます。これが機械学習で、処理を与えられるのではなく、処理を学習するのです。（図を指しながら）もしそのようなものがあったとしたら、我々は基本的に今、次のようなものを持っているでしょう：あなたは、入力と重みがモデルの入力となり結果，つまりチェッカーでの勝敗，を出力し，これらのパフォーマンスを評価します。この評価が重要なステップです。2つ目の重要なステップは、測定されたパフォーマンスに基づいて重みを更新する方法で、この一貫の処理を繰り返し行うこともあります。このプロセスが機械学習モデルの訓練（学習）です。これは抽象的なアイデアです。しばらく実行した後、重みがかなり良い感じになりました。ここで，どう訓練したかを無視すれば，機械学習モデルは従来のプログラムと同様なものになります。ここでは「プログラム」ではなく「モデル」という単語が使われています。訓練されたモデルは他のコンピュータプログラムと同じように使うことができます。つまり、我々はコンピュータプログラムを，タスクを実行するための必要なステップを設書き下すのではなく、タスクを実行するよう学習させることによってプログラムを構築していて、訓練の後には別のプログラムになります。このプログラムは推論と呼ばれます。推論とは、この訓練済みのモデルを使ってチェッカーなどのタスクを解かせることです。つまり，機械学習とは，処理を書き下すのではなく，コンピュータが経験から学習させるプログラムの訓練です。これを画像認識のためにどうするかというと、モデルと重みは何かというと、それらを変化させていくと、猫と犬の認識がどんどん良くなっていきます。
相手の駒が自分の駒からどれくらい離れているか」に応じて、その状況で何をすべきかを列挙することは想像に難くありません。どのように防御的な戦略と攻撃的な戦略を比較するべきか、などなど。画像認識のためにどうするのか、全く明らかではありません。私たちが本当に必要としているのはモデルに含まれる非常に柔軟な関数と、それを如何様にも機能させる重みです。本物の...世界で最も柔軟性のある関数のような...そんなものがあることがわかりました それはニューラルネットワークです。今後の授業で，その正確な数式を説明していきます。しかし，ニューラルネットワークを使うためには、その数学的な関数が何であるかは、実際には重要ではありません。これは、我々が言うところの「パラメータ化」された関数であり、重みを変えれば、異なるタスクを実行し、実際にはあらゆるタスクを実行するできます：普遍性定理と呼ばれるものは、この関数の形は、正しい重みを見つければ，任意の精度であらゆる問題を解けると数学的に証明しています。これは以前にミンスキー（マービン・ミンスキー）問題の対処方法に関して述べたことを言い換えてるようなものです。 ニューラルネットワークは非常に柔軟性があるので，正しい重みを見つけることができれば 「これは猫か犬か」などの問題に答えられます。つまり、（モデルの）訓練に力を注ぐ必要があります。良い重み，これはサミュエルの言い方ですが，を見つけて、それらの適切な配分を見つけることです。どうすれば良いのでしょうか？非常に汎用的な方法を求めています。例えば、猫と犬の認識がどれくらい優れているかというような、パフォーマンスに基づいて重みを更新することです。そして幸運なことに、そのような方法が存在することが判明しました！その方法とは、確率的勾配降下法あるいはSGDと呼ばれています。今後，実際にSGDがどのように動作するのかを確認し、スクラッチで作成しますが、今のところは心配する必要はありません。これだけは言っておきますが、SGD もニューラルネットも数学的には全く複雑ではありません。ほとんど足し算と掛け算だけです。トリックは、足し算と掛け算を、数十億，いやそれより多く、私たちが直感的に把握できるより多く行っていることです。彼らは極めて強力ですが，ロケットのように複雑ではありませんし，それらがどのように動くのかをしっかり確認します。ここまでがアーサー・サミュエル版です。今日では、用語は異なりますが、全く同じ考えを使っています。真ん中にあるのがアーキテクチャで重みを調整することで何かしらのタスクを適切に処理するようになる関数です。
それがアーキテクチャであり、モデルの関数形です。時々、モデルはアーキテクチャを意味すると言われることがありますが、あまり混乱させないようにしてください。 しかし、本当に正しい言葉はアーキテクチャーです。 私たちはそれらを「重み」とは呼ばず、「パラメータ」と呼んでいます。 重みには特定の意味があり、非常に特殊な種類のパラメータです。 モデルから出てくるもの、つまりパラメータを持つアーキテクチャの出力を、私たちは予測と呼んでいます。 予測は2種類の入力に基づいています：データである独立変数、猫や犬の写真のようなもの、そしてラベルとして知られている従属変数、これは「これは猫です」「これは犬です」といったもので、これが入力です。 つまり、結果は予測です。アーサー・サミュエルの言葉を借りれば、パフォーマンスの尺度は、損失と呼ばれています。 損失は予測値のラベルから 計算されます そして、パラメータの更新があります  さて、これは先ほどの図と同じですが、今日使う言葉を入れてみました。 もし忘れてしまった場合は、この図を見て、これがモデルを作成するためにこのアーキテクチャで使用されているパラメータだと言ったら、もう一度戻って、パラメータの意味を思い出してみてください。パラメータとは何か？予測値とは？損失とは何か？ パラメータを更新できるような方法でモデルのパフォーマンスを測定する関数の損失です。 ここで注意しなければならないのは、ディープラーニングや機械学習は魔法ではないということですね。 モデルは、学習しようとしていることの例を示すデータがあるところでしか作成できません。 それは訓練に使われた入力の中で見たパターンでしか動作を学習できないんですよね？ だから、猫や犬の画像がないと、アーキテクチャを作るパラメータの更新がないので、アーキテクチャとパラメータを合わせたものがモデルなんです。 つまり、モデルと言えば、モデルが猫や犬の線画を予測するのが上手になるということです。 また、この学習アプローチは予測値を作成するだけであることにも注意してください。 それに対して何をすべきかは教えてくれません。 これは、「誰かにどの製品を勧めるか」というようなレコメンデーションシステムのようなものを考えるときに非常に重要になるでしょう。 私たちはそんなことしませんよね？ 私たちが見せた製品について誰かが何を言うか予測することはできますが、私たちは行動を生み出しているわけではありません。 予測をしているのです。 これは非常に重要な違いです。犬や猫の絵などの入力データの例があるだけでは不十分なのです。


ラベルなしでは何もできません。企業がよく言うのは 「データが足りない」と言うことがよくあります。ほとんどの場合は、「ラベル付きのデータが足りない」という意味です。ほとんどの場合、「ラベル付けされたデータが十分ではない」という意味です。なぜなら、企業がディープラーニングを使って何かをしようとしている場合、多くの場合、すでに行っていることを自動化したり、改善したりしようとしているからです。つまり、理論的には、そのデータを持っているか、またはそのデータを取得する方法を持っていることを意味します。彼らはそれをやっているからだ。しかし、しばしば厄介なのは、それにラベルを付けることです。例えば医療における放射線学のモデルを作成するとします。思いつく限りのあらゆる医療画像を入手できることはほぼ間違いないでしょう。しかし、それらを腫瘍の悪性度や髄膜腫の有無などでラベル付けするのは非常に難しいかもしれません。このような違いは、戦略に大きな影響を与える重要なものです。モデルとは、PDPの本で見たように、モデルは環境の中で動作します。あなたはそれを展開して、何かをするのです。そして、PDPのフレームワークのこのような部分が非常に重要なのです。そうですよね？実際に何かをしているモデルがあります。例えば、どこで逮捕されるかを予測する（行動を推奨するわけではありません）警察のような予測モデルを構築したとします。これはアメリカの多くの管轄区域で使われているものです。今では、データに基づいて、ラベル付けされたデータに基づいて、それを予測しています。この場合、実際には（アメリカの）データを使っています。黒人か白人かにもよりますが、アメリカの黒人は白人に比べてマリファナ所持で逮捕される頻度が７倍くらい高いと思います。実際のマリファナ使用量は白人と黒人でほぼ同じですが、偏ったデータを元に取り締まり予測モデルを構築すると、その予測はこのような偏ったデータに基づいて 「ここに逮捕できる人がいるだろう」と 予測します。そうすると、法執行機関の職員は、モデルが予測した地域に警察活動を集中させることに決めるかもしれません。その結果、より多くの人を逮捕することになる。そして、それを使ってモデルに戻す。するとモデルは黒人居住区ではもっと多くの人を逮捕すべきだと予測するようになります。これは、モデルが環境と相互作用する中で、どのように正のフィードバックループと呼ばれるものが作られるかの一例です。モデルの利用増加に伴い、データはより偏ったものになり、モデルはさらに偏ったものになります。ですから、機械学習で注意しなければならないことの1つは、モデルが実際にどのように使用されているかを認識し、その結果として起こり得るかを認識することです。


これはプロキシの例であるということを伝えたいです。なぜなら、ここでは逮捕者が犯罪の代わりとして使われているからです。 そして、往々にして，プロキシと実際の値の差は大きいのです。

ありがとう、レイチェル。 本当に重要なポイントですね。

それでは最後に、このコードで何が起こっているのかを見てみましょう。 私たちが実行したコードは、基本的には、1,2,3,4,5,6行のコードです。 コードの最初の行はインポート行です。 Pythonでは、外部ライブラリをインポートしない限り、外部ライブラリを使うことはできません。 通常はライブラリから必要な関数やクラスだけをインポートします。 しかし、Pythonにはモジュールからすべてをインポートできる便利な機能があります。 ほとんどの場合、これは悪い考えです。 なぜなら、デフォルトでは、Pythonの動作方法は、import starを実行した場合、ライブラリの中であなたが使いたい重要なものだけをインポートするのではないからです。 実際には、使用した全てのライブラリや、使用した全てのライブラリがimportしているものもimportするため、結果的に名前空間を爆発させてしまい、多くのバグを引き起こしてしまいます。 fastaiはREPL環境で素早くプロトタイピングができるように設計されているので、私たちはこの問題を避けるために多くの時間を費やしました。

ですから、これをするかしないかはあなた次第です。 しかし、fastaiライブラリからstarをインポートする場合は、実際に必要な部分だけを取得できるように明示的に設計されているので安心してください。一つだけ言及しておきたいのは、ビデオの中で "fastai2"と呼ばれていることです。 収録時はプレリリース版を使っているからです。オンライン版、MOOC版を見ている時には、2は取れているでしょう。 

他にも言及しておきたいことがあります。fastaiには4つの定義済みアプリケーション、vision、text、テーブル、協調フィルタリングがあります。 それぞれについて、詳しく学んでいく予定です。 アプリケーションはそれぞれ、例えばここにビジョンがあるとすると、.allからインポートすることができます。 これで、一般的なvisionアプリケーションに必要なほぼ全てがimportされます。 

Jupyter NotebookのようなREPLシステムを使っていれば、必要なものはすべてそこにあるので、戻って調べたりする必要はありません。 Pythonユーザーがそれを行っているというのは
問題点です。 untar_dataのようなものを見たとしても、それがどこから来たのかはインポート行を見ればわかるでしょう。 しかしstart import (import *)すると、わからなくなってしまうのです。

REPLでは戻って調べる必要がないということです。調べたい記号を入力して [SHIFT - ENTER] を押すだけで、それがどこから来たのかを正確に教えてくれます。この通りです。大変便利なのです。
この場合、例えば、実際にデータセットを構築するために、ImageDataLoaders.from_name_func()を呼び出しました。doc関数を使えば簡単にドキュメントを確認できます。
 ご覧のように、デフォルト値を含めて全ての引数が確認できます。そして最も重要なことは、[SHOW IN DOCS]は、exampleを含む完全なドキュメントへのリンクになっています。
fastAIのドキュメントにはすべて例がついています。さらに、それらは全てJupyter Notebookで書かれているので、自分でコードを実行して、実際の動作や出力など確認できます。
また、ドキュメントにはたくさんのチュートリアルがあります。例えば、visionのチュートリアルには多くのことが書かれていますが、その1つは、レッスン１の内容をほぼカバーしています。 
fastAIにはたくさんのドキュメントがあります。ドキュメントは完全に検索可能ですし、おそらく最も重要なことは、これらのドキュメントのすべてが完全にインタラクティブなJupyter Notebookになっていることです。 
そこで、このコードをさらに見てみると、importの後の最初の行は、untar_dataを呼んでいます。これはデータセットをダウンロードして、解凍します。
すでにダウンロードされている場合は、再度ダウンロードすることはありませんし，解凍済みなら解凍しません。

ご覧のように、fastAIはすでに多くの有用なデータセットを定義しています。データセットは深層学習で想像できるように超重要です。これからたくさんのデータセットを見ていくでしょう。そして、これらのデータセットは多くのヒーロー（とヒロイン）によって作成され、基本的には数ヶ月から数年かけてデータを照合し、これらのモデルを構築するために使用することができます。
次のステップは、このデータが何であるかをfastAIに伝えることであり、データについて私たちは多くのことを学んでいきます。しかし、今回は、基本的には「これには画像が含まれている」と言っています。このパスにある画像が含まれています。 untar_dataは解凍済みの
データのパスを返します。あるいはすでに解凍されている場合は、以前に解凍された場所を教えてくれます。そのパスの中に実際にどんな画像があるのかを教えてくれます。
本当に興味深いものに label_func があります。ファイルごとに、それが猫なのか犬なのかをどうやって判別するのでしょうか。実際にオリジナルのデータセットのReadMEを見てみると、少し風変わりなものが使われています。しかし彼らが決めたことです。

そこで、ここにis_catという小さな関数を作って、最初の文字が大文字かどうかを返すようにしました。 これで猫かどうかを判断する方法をfastaiに伝えました。 この2つについては後で説明します。

次に、データが何であるかを伝えます  次にlearnerと呼ばれるものを作らなければなりません。learnerとは学習するものです。訓練をします。だから、どんなデータを使うか教えなければなりません。 そして、どのようなアーキテクチャを使うかを教えなければなりません。 このコースではアーキテクチャについてたくさんお話します。 しかし、基本的には、事前に定義されたニューラルネットワークのアーキテクチャがたくさんあり、それぞれ長所と短所があります。 コンピュータビジョン用のアーキテクチャはResNetと呼ばれています。 これは非常に素晴らしい出発点なので、ここでは小さ目なものを使うことにします。これらはすべて事前に定義されていて、すぐに使えます。 そして、訓練中にプリントアウトしたいものをfastaiに伝えることができます。 ここでは、「ああ、誤差を教えてください、トレーニング中に」と言っています。 そして、fine_tuneと呼ばれるとても重要なメソッドを呼び出すことができます。次のレッスンで説明しますが、fine_tuneが訓練を行います。 

valid_pctは非常に重要なことをします。 この場合、データの20%は、モデルの学習には使用しません。 その代わりに、モデルの誤差率を知るために使います。 したがって、常にfastaiでは、このメトリック、error_rateは常に訓練されていないデータの一部で計算されます。 この考え方については、今後のレッスンで詳しく説明します。 しかし、ここでの基本的な考え方は、過学習していないことを確認したいということです。 説明しましょう。過学習は次のようなものです。 例えば、これらの点にフィットする関数を作ろうとしているとしましょう。 素敵な関数はこのようになりますよね？しかし、この関数をはより正確にフィットしています。 見てください、これはこちらよりもはるかにすべての点に近づいています。ですから、明らかにこちらの方が優れた関数です。 ただし、点がある場所の外に出るとすぐに、特に端から外れてしまうと、明らかに意味がありません。 これが過学習と呼ばれるものです  過学習は様々な理由で起こります。 大きすぎるモデルを使ったり、十分なデータを使っていなかったり...この話はこれからですね。しかし、ディープラーニングの技術は、適切なフィットを持つモデルを作成することがすべてです。 モデルが適切にフィットしているかどうかを知る唯一の方法は、それを訓練するために使用されていないデータでうまく機能するかどうかを見ることです。 そのため、私たちは常にデータの一部を脇に置いて、バリデーション・セットと呼ばれるものを作成します。 バリデーション・セットとは、モデルを訓練しているときには全く触らないようにしているデータですが、モデルが実際に機能しているかどうかを把握するためだけに使用しています。 


シルヴァンが本の中で言っていたことですが、fastaiを勉強していて面白いことの一つは、多くの面白いプログラミングを学ぶことができるということです。私は子供の頃からプログラミングをしていたので、かれこれ40年くらい経ちます。 シルヴァンと私は二人ともpythonで多くのことを行うために本当に一生懸命働いていて、プログラミングのプラクティスを知っています。私たちのコードの中では、今まで見たことのないようなことをやっていることがよくあります。


以前のコースを受講した多くの学生は、このコースでコーディングやPythonコーディング、ソフトウェアエンジニアリングについて多くを学んだと言っています。 だから、何か新しいものを見たときはチェックして、なぜそのような方法で行われたのか気になったらフォーラムで気軽に質問してください。 

一つ言及しておきたいのは、先ほども述べたように、ほとんどのPythonプログラマーがimport *を行うことはありません。 私たちはそのようなことをたくさんやっています。

私たちは伝統的なPythonプログラミングのアプローチに従わないことをたくさんやっています。私は長年にわたって多くの言語を使ってきたので、特にPython的な方法ではなく、他の多くの言語や他の多くの表記法からのアイデアを取り入れて、データサイエンスに適したものをベースにPythonプログラミングへのアプローチを大幅にカスタマイズしています。つまり、あなたがfastaiで見たコードは、あなたの職場でPythonを使っている場合、おそらく、あなたの職場のスタイルガイドや通常のアプローチには合わないでしょう。 ですから、私たちのやり方に従うのではなく、あなたの組織のプログラミングのやり方に合うようにする必要があります。 しかし、おそらくあなた自身の趣味の仕事では、私たちのものに従ってみて、それが面白くて参考になるかどうかを見てみてもいいでしょうし、もしあなたが管理職で、そうすることに興味があるのであれば、あなたの会社でそれを試してみるのもいいでしょう。 

さて、最後に、かなり面白いものをお見せしましょう、それは、このコードを見てみてくださいuntar_data、ImageDataLoaders.from_name_funcから、learner、fine_tuneします。

untar_data、SegmentationDataLoaders.from_label_func、label_func、learner、fine_tune。 ほとんど同じコードですが、これは何かをするモデルを構築しました。 これは画像を撮影したものです。左側がラベル付きのデータです。 色は、それぞれ、車、木、建物、空、線、道路などを示しています。 右側は私たちのモデルですが、私たちのモデルは各ピクセルごとに、車なのか、ラインマークなのか、道路なのかを判別することに成功しました。たった20秒以内でこれを実現しました。 ですから、非常に小さなモデルです。 


いくつかのミスをしています -- この線のマークを見落としていたり、いくつかの車が家だと思っているような？しかし、数分間これを訓練すれば、ほぼ完璧になることがわかります。基本的な考え方は、ほぼ同じコードを使って、猫や犬を分類するのではなく、セグメンテーションと呼ばれるものを非常に速く作ることができるということです。 

見てください、同じものがあります。

Import *、TextDataLoaders.from_folder，learner、fine_tune
基本的なコードは同じです。これは、文章が表している感情が肯定的か否定的かを理解するできるようになりました。同じ3行のコードで15分の訓練による93%という正確度は、1000から3000語の何千もの映画レビューからなるIMDBデータセットの分類においては2015年時点で世界最高のものだったと思います。私たちは基本的に同じコードを使って、私たちのブラウザで、世界クラスのモデルを作成しています。

ここでも同じ基本的な手順です。

from import *、untar_data、TabularDataLoaders.from_csv、Learner、fit
これは今、これらの列を含むcsvに基づいて給与を予測するモデルを構築しています。これは表形式のデータですね。

ここでは基本的な手順は同じです。

Import *、untar_data、CollabDataLoaders.from_csv、learner，fine_tune

これはユーザーと映画の組み合わせごとに、視聴履歴をもとに，ユーザが映画をどう評価するかを予測するものを構築しています。これは協調フィルタリングと呼ばれ、レコメンデーションシステムで使用されています。

ここでは、fastaiの4つのアプリケーションの例を見てきました。このコースを通してお分かりになると思いますが、基本的に同じコードと、同じ数学とソフトウェア工学の基本的な概念によって、同じようなアプローチを使って、大きく異なることができるのです。なぜかというと、アーサー・サミュエルのおかげです。モデルをパラメータ化する方法と、重みを更新して損失関数を改善する更新手順があれば、何ができるのかという基本的な説明があるからです。
この場合、ニューラルネットワークを使うことができます。他のレッスンよりも少し短くなりますが、その理由は、先ほど述べたように、私たちは世界的なパンデミックの始まりの時期にあるからです（少なくとも欧米では（他の国ではもっと進んでいます）。
このことについては、コースの最初の方でお話しましたが、そのビデオは別の場所で見ることができます。
今後のレッスンでは、より深層学習について学ぶことになるでしょう。
次のレッスンに取り組む前に、次の週までに以下に取り組むことをお勧めします。GPUサーバをスピンアップして、終了したらシャットダウンして、ここにあるすべてのコードを実行できることを確認して、自分が認識できる方法でPythonを使っているかどうかを確認してください。自分のやり方を知ることができれば、快適に過ごせるようになるでしょう。この学習スタイル、トップダウン学習で最も重要なことは、実験を行うことであり、コードを実行できるようになる必要があります。
ですから、私のお勧めは、コードを実行できるようになるまでは先に進まないで、本の章を読んで、アンケートに答えてください。検証セットやテストセット、転送学習については、まだまだやるべきことがあります。ですから、まだすべてのことができるわけではありませんが、これまでのコースを見てきたことに基づいて、できる部分はすべてやってみてください。



レイチェル、何か追加したいことがあれば教えてください。
それでは、レッスン１に参加してくれてありがとうございました。次回は転移学習について学び、その後、実際にインターネット上で公開できるアプリの制作に移ります。
バイバイみんなさん。
