0:00:00.640,0:00:06.920
皆さんこんにちは、コーダーのための
ディープラーニング、レッスン１へようこそ。

0:00:06.920,0:00:18.140
今年で4年目になりますが、これまでとは違った
特別なバージョンになっています。

0:00:18.140,0:00:25.980
まず第一に、完全にシャットダウンされた初日から
生放送でお届けします。

0:00:25.980,0:00:29.860
完全なシャットダウンではありませんが
サンフランシスコはほぼ完全にシャットダウンしています。

0:00:29.860,0:00:34.960
この世界的なパンデミックの真っ只中で
２ヶ月間に渡って 録画する予定です。

0:00:34.960,0:00:39.520
もしこのコースで時々少し
慌ただしいように見えたら謝罪します。

0:00:39.520,0:00:42.000
しかし、このようなことが起きているのが理由です。

0:00:42.000,0:00:53.000
もう一つの特別な理由は、これを私たちの
決定版にしようとしているからです。

0:00:53.000,0:01:00.500
長い間続けてきたことで、やっと自分たちが
何を言ってるのか、わかった気がしてきました。

0:01:00.500,0:01:05.480
シルヴァンと私は実際に本を書いて、

0:01:05.480,0:01:12.000
ゼロからfastai2というソフトウェアも書きました。

0:01:12.020,0:01:17.660
このライブラリについては査読付きの論文も
書いています。

0:01:17.660,0:01:26.980
ですから、このバージョンのコースは、
うまくいけばしばらくは続くと思います。

0:01:26.980,0:01:32.000
シラバスはこの本に非常に密接に基づいています。

0:01:32.000,0:01:38.140
だから、もしあなたがちゃんと読みたいのであれば、
ぜひ買ってください。

0:01:38.140,0:01:45.860
そして「買ってください」と言っているのは、実際には
ジュピターノートブックという形で全てが無料で
手に入るからです。

0:01:45.920,0:01:54.420
これはオライリー・メディアの寛大な
ご厚意のおかげです。

0:01:57.440,0:02:03.680
コースのウェブサイトにアクセス方法が
掲載されていますが、

0:02:05.600,0:02:11.000
ここにあるfastbookのレポでは
全ての内容を読むことができます。

0:02:11.880,0:02:18.000
今のところ、ご覧のようにそれは草稿ですが、あなたが
これを読む頃にはそうはなっていないでしょう。

0:02:18.000,0:02:22.000
そこで、ここで大きな要望があります。

0:02:23.360,0:02:30.380
ジュピターノートブックとして無料で
読むことができますが、

0:02:30.380,0:02:36.000
Kindleや紙の本などで読むのと比べると
便利ではありません。

0:02:36.000,0:02:39.360
だから、これをPDFにするのはやめてください。

0:02:39.360,0:02:44.520
読むことを目的としたフォーマットには
しないでください。

0:02:44.520,0:02:50.440
そもそもの目的はよかったら
あなたに買ってもらうことです。

0:02:50.440,0:03:00.320
オライリーの寛大さを利用して、タダで貰えないと
分かっているものを作るのはやめてください。

0:03:00.320,0:03:06.320
実際にはそれが、私たちがこれを提供している
ライセンスなのです。

0:03:06.640,0:03:09.500
これは主にまともな人間であってくださいとの
お願いです。

0:03:09.500,0:03:15.240
もし他の誰かがまともな人間ではなく、
書籍版を盗んでいるのを見たら、

0:03:15.240,0:03:19.000
「そんなことしないでくれ、迷惑だから 」と
言ってあげてください。

0:03:19.000,0:03:20.700
そして、そんな人にならないようにしましょう。

0:03:21.280,0:03:27.000
ということで、どちらにしても、本の中のシラバスに
沿って読んでください。

0:03:28.000,0:03:34.000
このノートにはいくつかのバージョンがあります。

0:03:38.120,0:03:46.500
全部の文章、写真、全てが載っている
完全なノートがあります。

0:03:46.800,0:03:54.520
私たちは実際にジュピターノートブックを印刷する本にするためのシステムを作ったたのですが、

0:03:54.520,0:03:56.880
それがちょっと変な感じに見えることがあります。

0:03:56.880,0:04:00.920
例えば、ここに変な表がありますが、

0:04:01.020,0:04:04.680
実際の本の中を見てみると、

0:04:05.380,0:04:07.480
実際にはちゃんとした表のように見えますよね。

0:04:07.560,0:04:11.300
時々変なところが出てきますが、

0:04:11.300,0:04:13.300
これは間違いではありません。

0:04:13.420,0:04:20.000
本を素敵な本にするための情報を
追加するためのものですので、無視してください。

0:04:20.660,0:04:23.940
さて「私たち」と言えば、私たちとは誰でしょう？

0:04:23.940,0:04:30.000
「私たち」の重要な部分の一人がシルヴァンです。

0:04:30.000,0:04:39.000
シルヴァンは本とfastaiバージョン2ライブラリの
私の共著者ですので、彼は私の犯罪のパートナーです。

0:04:39.000,0:04:45.880
「私たち」のもう一人はレイチェル・トーマスです。

0:04:45.880,0:04:48.460
レイチェル、挨拶をしに来てください。

0:04:48.520,0:04:52.040
彼女はfastaiの共同設立者です。

0:04:52.040,0:04:55.600
こんにちは、私はfastaiの共同創設者です。

0:04:55.600,0:04:57.220
そして、あ、低く。

0:04:57.220,0:04:58.120
すみません。

0:04:58.120,0:04:59.740
私はジェレミーよりも背が高いんです。

0:04:59.740,0:05:05.500
そして私はサンフランシスコ大学の応用データ
倫理センターの創設ディレクターです。

0:05:05.500,0:05:08.000
このコースに参加できることを
本当に楽しみにしています。

0:05:08.000,0:05:12.000
そして私の声がフォーラムからの
質問を聞く声になります。

0:05:14.820,0:05:19.000
レイチェルとシルヴァンは、このグループの中で実際に数学を理解している人たちです。

0:05:19.000,0:05:21.480
私は単なる哲学の卒業生です。

0:05:21.480,0:05:23.300
レイチェルは博士号を持っています。

0:05:23.300,0:05:26.520
シルヴァンは数学について10冊の本を
書いているので、

0:05:26.520,0:05:32.000
もし数学の質問が来たら、私はそれを
委託すかもしれません。

0:05:32.000,0:05:38.000
でも、この課題をよく理解している人たちと一緒に仕事をする機会があるのは、とても嬉しいことです。

0:05:38.000,0:05:41.940
はい、はいレイチェル、

0:05:41.940,0:05:44.020
あぁ、ありがとうございます。

0:05:45.120,0:05:52.500
レイチェルが言ってたように、彼女が実際の
世界クラスの専門知識を持っているのは

0:05:52.500,0:05:58.860
データ倫理であり、彼女は応用データ倫理センターの
創設ディレクターです。

0:05:59.000,0:06:00.580
サンフランシスコ大学です

0:06:00.580,0:06:02.020
サンフランシスコ大学ですね、ありがとうございます。

0:06:02.020,0:06:05.820
私たちはコースを通してデータ倫理について話します。

0:06:05.820,0:06:10.000
なぜなら、私たちはたまたまそれが非常に重要だと
考えているからです。

0:06:10.500,0:06:13.960
私が一般的にそれらを発表することになりますが、

0:06:13.960,0:06:18.680
全体的にはレイチェルの仕事に基づいたものになります。

0:06:18.680,0:06:20.900
彼女は実際に何を言っているのかを知っているので。

0:06:21.140,0:06:26.260
でも、彼女のおかげで、私も私が話していることを
少しは知っています。

0:06:26.560,0:06:29.120
それはそれとして。

0:06:31.280,0:06:34.600
あなたはここにいるべきでしょうか。

0:06:34.600,0:06:38.500
あなたがを理解することにどんな目的があるのか

0:06:38.500,0:06:40.520
（私は正しいボタンを押したと思った）

0:06:40.520,0:06:43.460
深層学習を理解しようとすることに意味があるのでしょうか？

0:06:44.720,0:06:49.120
OK 、あなたは何を・・・あなたはここにいるべきでしょうか。

0:06:49.120,0:06:52.800
あなたが深層学習を学んでメリットがありますか？

0:06:52.800,0:06:55.320
あなたがあまり頭がよくないですか？

0:06:55.320,0:06:59.480
または十分高速なコンピューターを
持っていないですか？、などなど。

0:06:59.480,0:07:01.780
それは多くの人々が私たちに言ってくることです。

0:07:01.780,0:07:06.940
彼らは、博士号を持ったチームとGPUを搭載した
大規模なデータセンターが必要だと言っています。

0:07:06.940,0:07:08.540
そうでなければ無意味だと。

0:07:08.550,0:07:12.080
心配しないでください、それは全く
真実ではありません。

0:07:12.080,0:07:14.020
真実からこれ以上離れたものはありません。

0:07:14.020,0:07:21.740
実際、大多数の世界的な研究や世界的な
産業プロジェクトの多くは、

0:07:21.740,0:07:33.689
fastaiの卒業生やライブラリを使ったプロジェクト、
または他の場所から生まれたもので、

0:07:33.689,0:07:41.040
大学院レベルの技術的な専門知識を
持たない人たちから、

0:07:41.040,0:07:49.780
数十、数百のデータポイントを使って
単一のGPUで作られたものです。

0:07:49.780,0:07:52.540
私はといえば大学レベルの専門知識もありません。

0:07:52.550,0:07:55.080
私は哲学部です。

0:07:55.080,0:07:59.719
しかし、ディープラーニングを使って素晴らしいことをするためには、

0:07:59.719,0:08:03.770
たくさんの数学は必要ないし、たくさんのデータも必要ないし、

0:08:03.770,0:08:08.250
高価なコンピュータも必要ないという明確な
実証的証拠がたくさんあります。

0:08:08.250,0:08:10.240
だから我々と一緒に我慢して付いてきてみてください。

0:08:10.240,0:08:12.229
あなたは大丈夫です。

0:08:12.229,0:08:15.110
このコースを受けるには コードを書く必要があります。

0:08:15.110,0:08:18.750
できればPythonでコードを書く方法を 知っていることが望ましいです。

0:08:18.750,0:08:21.900
他の言語をやったことがあれば Pythonを学ぶことができます。

0:08:21.900,0:08:26.939
もしあなたがこれまでにやったことのある言語がMatlabのようなもので、

0:08:26.939,0:08:32.570
スクリプトのようなものしか使ったことがないのであれば、少し難しく感じるかもしれません。

0:08:32.570,0:08:35.880
しかし、それでいいんです。
頑張ってください。

0:08:35.880,0:08:40.990
Pythonはどんどん学習していけばいいのです。

0:08:40.990,0:08:42.690
深層学習を学ぶ意味はありますか？

0:08:42.690,0:08:46.220
何かディープラーニングの得意なことはありますか？

0:08:46.220,0:08:52.560
もしあなたが脳を作りたいと思っているのであれば、
それはAGIです。

0:08:52.560,0:08:55.600
なのでこのコースがあなたをの役に立つとは
約束できません。

0:08:55.680,0:08:58.860
AGIは 人工知能（Artificial General Intelligence）の
略です。

0:08:58.870,0:09:00.339
ありがとうございます。

0:09:00.339,0:09:06.870
しかし私が言えることは これらの分野のすべてにおいて 少なくとも多くのバージョンでは

0:09:06.870,0:09:12.760
ディープラーニングが最もよく知られたアプローチであるということです。

0:09:12.760,0:09:18.130
ですから、これが有用なツールであるかどうかは、
現時点では推測ではありません。

0:09:18.130,0:09:21.880
たくさんの、たくさんの、たくさんの、たくさんの場所で役に立つツールなのです。

0:09:21.880,0:09:23.040
極めて有用なツールです。

0:09:23.040,0:09:29.150
そして、これらの多くのケースでは、人間のパフォーマンスと同等かそれ以上であるのです。

0:09:29.150,0:09:35.320
（少なくとも、この種の分野で人間が行うことのいくつかの特定の狭い定義によると）

0:09:36.240,0:09:38.880
ディープラーニングは非常に素晴らしいものです。

0:09:38.900,0:09:45.680
ここでビデオを一時停止して、興味がありそうなものをいくつかピックアップしてみてください。

0:09:45.680,0:09:48.980
そしてそのキーワードとディープラーニングをGoogleで検索すれば、

0:09:48.980,0:09:53.180
たくさんの論文や事例などが見つかるはずです。

0:09:55.360,0:09:59.800
ディープラーニングは、ニューラルネットワークの背景から来ています。

0:09:59.800,0:10:05.240
見ての通り、ディープラーニングは
ニューラルネットワーク学習の一種です。

0:10:05.240,0:10:06.920
深いものです。

0:10:06.920,0:10:08.650
それが何を意味するのかは後ほど正確に説明します。

0:10:08.650,0:10:11.660
そして、ニューラルネットワークは確かに新しいものではありません。

0:10:11.660,0:10:14.420
少なくとも1943年までさかのぼります。

0:10:14.420,0:10:19.300
マックロックとピッツが 人工ニューロンの数学モデルを作成した時です。

0:10:19.300,0:10:23.080
そして、それがどこに到達するかについて非常に期待されました。

0:10:23.080,0:10:31.040
50年代には、フランク・ローゼンブラットが、その上に数学モデルを構築しました。

0:10:31.040,0:10:36.320
彼は基本的に、その数学モデルに微妙な変更を加えました。

0:10:36.320,0:10:38.400
そして彼は、これらの微妙な変更で、

0:10:38.400,0:10:44.980
「人間の訓練や制御なしに 周囲の環境を看取、認識、識別できる機械の誕生を 目撃することができる」

0:10:44.980,0:10:46.960
と考えました。

0:10:46.960,0:10:51.200
そして彼はこの驚異的なものの開発を監督したのです。

0:10:51.200,0:10:54.860
コーネル大学のマーク１パーセプトロンです

0:10:54.860,0:11:00.269
この写真は１９６１年だったと思います。

0:11:00.269,0:11:04.610
ありがたいことに今日では、ニューラルネットワークを構築するために、

0:11:04.610,0:11:07.089
ニューロンからニューロンへ（人工ニューロンから人工ニューロンへ）配線をを走らせる必要はありません。

0:11:07.089,0:11:10.800
しかし、多くの接続が行われているという考えが見えてきます。

0:11:10.800,0:11:14.220
このコースでは、接続（コネクション）という言葉をよく耳にするでしょう。

0:11:14.220,0:11:16.880
それはここからきています。

0:11:16.880,0:11:19.790
それから、私たちは最初のAIの冬を迎えました。

0:11:19.790,0:11:26.899
これは、MITのマービン・ミンスキー教授とパパートが ローゼンブラットの発明について

0:11:26.899,0:11:31.810
パーセプトロンという本を書きました。その中で 彼らが指摘したのは、

0:11:31.810,0:11:38.360
これらの人工ニューロンデバイスの単一の層は、実際にはいくつかの重要なことを

0:11:38.360,0:11:40.620
学ぶことができないということです。

0:11:40.620,0:11:45.990
ブール演算子のXOR演算子のような簡単なものを学ぶことは不可能でした。

0:11:45.990,0:11:51.570
同じ本の中で、彼らは、人工ニューロン装置を何層にも重ねて使えば、

0:11:51.570,0:11:52.570
実際に問題が解決することを示しています。

0:11:52.570,0:11:54.880
でも人々は無視 - 本のその部分に気づかなかったのです。

0:11:54.880,0:11:59.960
そして、制限にだけ気付き、人々は基本的にニューラルネットワークは

0:11:59.960,0:12:01.820
どこにも行かないと決めつけてしまいました。

0:12:01.829,0:12:07.000
そして何十年もの間、それらはほとんど姿を消してしまいました。

0:12:07.000,0:12:09.829
ある意味で1986年までは。

0:12:09.829,0:12:16.100
その間にいろいろなことがありましたが、1986年に大きなことがありました。

0:12:16.100,0:12:22.560
MITが「並列分散処理」という本を出版、2巻に分けてシリーズ化したものです。

0:12:22.560,0:12:28.020
その中で、並列分散処理（Parallel Distributed Processing）と呼ばれるものを説明していました。

0:12:28.020,0:12:35.769
並列分散処理とは、たくさんの処理装置と、
アクティベーションと、出力機能と、

0:12:35.769,0:12:41.709
接続性のパターンと、伝播ルールと、活性化ルール、

0:12:41.709,0:12:44.910
学習ルールを持った処理ユニットが、環境の中で動作しているというものです。

0:12:44.910,0:12:50.160
そして、これらの要件を満たすものが、どのようにして理論的にはあらゆる種類の

0:12:50.160,0:12:52.300
驚くべき仕事をすることができるのかを説明しました。

0:12:52.310,0:12:56.459
これは、多くの、多くの研究者が協力して取り組んだ結果です。

0:12:56.459,0:13:02.040
このプロジェクトにはグループ全体が関わっていて、
それがこの非常に重要な

0:13:02.040,0:13:03.139
本につながったのです。

0:13:03.139,0:13:09.069
そして、私にとって興味深いのは、このコースを受講した後に、

0:13:09.069,0:13:15.430
この写真を見ていただければ、私たちがまさにこのようなことをしていることがわかると思います。

0:13:15.430,0:13:21.380
私たちが学んでいることは、これらの8つのことをどのようにするかということです。

0:13:21.380,0:13:25.420
環境が含まれているのは興味深いことですが、
これはデータサイエンティストが

0:13:25.420,0:13:28.329
無視することが多いからです。

0:13:28.329,0:13:31.800
モデルを構築し、それを訓練し、
何かを学習しました。

0:13:31.800,0:13:33.459
それがどのようなコンテキストで動作するのか？

0:13:33.459,0:13:40.040
それについては、次のレッスンでも度々お話しします。

0:13:40.040,0:13:47.709
そして80年代に、これがリリースされた後、
人々は第二層のニューロンを構築し始めました。

0:13:47.709,0:13:51.660
ミンスキーの問題を回避するためです。

0:13:51.660,0:13:59.860
実際に、ニューロンの層を1層追加することで、
これらのニューラルネットワークを使って、

0:13:59.860,0:14:07.790
どのような数学モデルでも、どのような精度でも近似できることが

0:14:07.790,0:14:10.160
数学的に証明されました。

0:14:10.160,0:14:13.690
これは、ミンスキーとは正反対のことでした。

0:14:13.690,0:14:17.440
"我々にできないことは何もない"ということが

0:14:17.440,0:14:19.649
証明できたのです。

0:14:19.649,0:14:23.540
私がニューラルネットワークに関わり始めたのは、その頃でした。

0:14:23.540,0:14:25.190
いや、少し遅れていました。

0:14:25.190,0:14:28.560
私が関わったのは90年代前半だったと思います。

0:14:28.560,0:14:30.779
その頃、業界では非常に広く使われていました。

0:14:30.779,0:14:34.970
私は銀行のリテール向けのターゲティングマーケティングのような、非常につまらないものに使っていました。

0:14:34.970,0:14:40.319
それらを利用している企業は、大金を持った大企業という傾向がありました。

0:14:40.319,0:14:46.389
しかし、ネットワークが大きすぎたり、遅すぎたりして
役に立たないことが多いのも事実でした。

0:14:46.389,0:14:51.870
確かにいくつかのことには役立っていましたが、私には何かの理由で

0:14:51.870,0:14:55.820
期待通りの成果を上げているようには感じられませんでした。

0:14:55.820,0:15:02.070
私が知らなかったのは、また私が個人的に会った誰も知らなかったのは、実際には30年前に、

0:15:02.070,0:15:08.510
実用的な性能を得るためには、より多くのニューロンの層が必要だということを示した研究者が

0:15:08.510,0:15:09.510
いたということです。

0:15:09.510,0:15:14.430
数学的には、理論的には、たった一枚の層を増やせば、いくらでも精度を上げることができますが、実際には、

0:15:14.430,0:15:16.019
それ以上の層が必要なのです。

0:15:16.019,0:15:20.850
それを良いパフォーマンスで行うためには、より多くの層が必要です。

0:15:20.850,0:15:26.100
だから、ニューラルネットワークにもっと多くの層を追加すると、
ディープラーニングになります。

0:15:26.100,0:15:29.470
つまり、深いというのは神秘的な意味ではありません。

0:15:29.470,0:15:32.139
それは単に、より多くの層を意味します。

0:15:32.139,0:15:35.800
ただ一枚の層を追加するのではなく、より多くの層を意味します。

0:15:35.800,0:15:39.460
そのおかげで、ニューラルネットは今、その潜在能力を十分に発揮しています。

0:15:39.460,0:15:42.680
ディープラーニングの得意分野のスライドで見たように

0:15:42.680,0:15:46.360
ローゼンブラットが正しかったと 言えるようになりました。

0:15:46.360,0:15:52.600
我々は、人間の訓練や制御がなくても、周囲の環境を看取し、認識し、

0:15:52.600,0:15:54.720
識別することができる機械を持っている。

0:15:54.720,0:15:56.360
確かにその通りです。

0:15:56.360,0:16:01.300
現在の技術に基づいて、この発言に議論の余地はないと思います。

0:16:01.460,0:16:04.920
だから、私たちはその方法を学びます。

0:16:04.939,0:16:09.290
しかしあなた方の今までの数学や技術教育とは

0:16:09.290,0:16:13.770
正反対の方法で学習していきます。

0:16:13.770,0:16:24.949
シグモイド関数の２時間の授業や、線形代数の勉強、

0:16:24.949,0:16:29.750
微積分の復習から始めるのではありません。

0:16:29.750,0:16:37.709
その理由は、教え方や学び方を研究している人たちが、それがほとんどの人にとって

0:16:37.709,0:16:41.639
正しい方法ではないことを発見したからです。

0:16:41.639,0:16:50.019
ハーバード大学のデビッド・パーキンス教授や、
同じような研究をしている他の人たちが、

0:16:50.019,0:16:56.740
ゲーム全体をプレイするという考え方について話しています。

0:16:57.740,0:17:01.760
ゲーム全体をプレイすることは スポーツの例えに基づいています。

0:17:01.880,0:17:09.360
例えば誰かに野球を教えるとしたら、

0:17:09.360,0:17:19.120
教室に連れて行って放物線の物理学やボールの縫い方、

0:17:19.120,0:17:24.280
100年の野球政治の3部作の歴史などを教え、10年後には試合を見させる。

0:17:24.290,0:17:27.050
そして20年後にやっとゲームをさせる。

0:17:27.050,0:17:31.930
これは数学教育のやり方に似ていますよね。

0:17:31.930,0:17:37.180
実際、野球の場合は、第一段階として、「野球を見に行こう」ということになります。

0:17:37.180,0:17:38.180
どうですか？

0:17:38.180,0:17:39.180
面白かったでしょ？

0:17:39.180,0:17:42.060
あの人、あそこで走者を出して、相手がボールを投げる前に、

0:17:42.060,0:17:44.080
ボールを打ってみないか？

0:17:44.080,0:17:47.380
よし、君がボールを打って、僕がキャッチして、そして君があそこへ走って...と、

0:17:47.380,0:17:52.940
ステップ１からゲーム全部をプレイしていることになります。

0:17:52.940,0:17:58.920
さらに付け加えるならば、人々は始めたばかりの頃には、チームが揃っていなかったり、

0:17:58.920,0:18:03.620
9イニングをフルでプレーしていなかったりもすることが多いのですが、それでもゲームの全体像を

0:18:03.620,0:18:05.390
把握していることが多いです。

0:18:05.390,0:18:12.510
だから、ほとんどの人にこれが役立つ理由はたくさんありますが、

0:18:12.510,0:18:14.050
みんながみんなそうではありません。

0:18:14.050,0:18:19.430
基礎や原則から物事を構築していくのが好きな人がごく一部いて、

0:18:19.430,0:18:24.270
驚くことではありませんが、大学ではそういう人の割合が非常に高くなっています。

0:18:24.270,0:18:28.510
なぜなら、学者になる人たちは、（私によれば）物事の教え方が逆さまになっていることで

0:18:28.510,0:18:33.350
成功している人たちだからです。

0:18:33.350,0:18:40.450
しかし、大学の外では、ほとんどの人がトップダウンの方法で最もよく学びます。

0:18:40.450,0:18:42.390
完全なコンテクストから始める方法です。

0:18:42.390,0:18:47.290
ですから、7つの原則の第2段階では、最初の3つの原則にだけ言及しますが、

0:18:47.290,0:18:49.620
ゲームをプレイする価値のあるものにするということです。

0:18:49.620,0:18:53.590
野球をやっていると、競争があります。

0:18:53.590,0:19:00.140
スコアをつけて、勝利を目指して、地域のチームを集めて、

0:19:00.140,0:19:02.350
みんなで競い合います。

0:19:02.350,0:19:08.640
誰が一番多くの得点を取ったかなどの リーダーボードがあります。

0:19:08.640,0:19:14.800
これはあなたがやっていることをきちんとやっているかどうかを確認することです。

0:19:14.800,0:19:23.000
あなたはそれを全体的なものにしてコンテクストと関心を提供しています。

0:19:23.000,0:19:30.490
深層学習へのfastaiアプローチでは、今日はモデルを

0:19:30.490,0:19:33.200
最後まで訓練します。

0:19:33.200,0:19:38.350
私たちは実際にモデルを訓練します。
そしてそれはただのくだらないモデルではありません。

0:19:38.350,0:19:45.070
今日から最先端のワールドクラスのモデルです。

0:19:45.070,0:19:50.910
今日か次のレッスンのどちらかから(状況に応じて)、あなた自身の最先端の世界クラスのモデルを

0:19:50.910,0:19:52.810
構築することを試みます。

0:19:52.810,0:19:59.200
そして、ハーバード大学の7つの原則の3番目は、難しい部分に取り組むことです。

0:19:59.200,0:20:10.050
これは、この練習、意図的な練習の考え方に似ています。

0:20:10.050,0:20:19.490
難しい部分を鍛えるということは、ただ単に毎回バットを振り回していれば

0:20:19.490,0:20:22.340
いいというわけではありません。

0:20:22.340,0:20:27.460
適切なトレーニングをして、自分の苦手な部分を見つけて、

0:20:27.460,0:20:31.340
どこに問題があるのかを把握して、それに一生懸命取り組むのです。

0:20:31.340,0:20:40.530
ディープラーニングの文脈では、我々は物事を単純化しないことを意味します。

0:20:40.530,0:20:45.210
コースが終わる頃には微積分をやっているでしょう。

0:20:45.210,0:20:47.400
線形代数もやっているでしょう。

0:20:47.400,0:20:54.380
コードのソフトウェアエンジニアリングを行っているでしょう。

0:20:54.380,0:21:04.290
これらのことを練習するのは難しいので、粘り強さと確約（コミットメント）が必要です。

0:21:04.290,0:21:11.290
しかし、うまくいけば、なぜそれが重要なのか理解できるでしょう。
それは何かを練習し始める前に、もうそれを使っていて、

0:21:11.290,0:21:14.470
なぜそれが必要なのかが分かっているからです。

0:21:14.470,0:21:19.450
あなたのモデルをより良くするためには、まずそのコンセプトを理解する必要があります。

0:21:19.450,0:21:24.490
伝統的な大学の環境に慣れている人にとっては、これはかなり奇妙に感じるでしょうし、

0:21:24.490,0:21:31.190
多くの人から聞くのは（1年fastaiを勉強した後で）
「理論の勉強に時間をかけすぎて、

0:21:31.200,0:21:39.500
モデルのトレーニングやコードを書くのに十分な時間をとらなかったことを後悔している」という事。

0:21:39.500,0:21:44.560
これは、「違うやり方をしていればよかった」と言う人からの反省の中で、一番多いものです。

0:21:44.570,0:21:45.790
それはその通りです。

0:21:45.790,0:21:53.120
だから、できる限り、あなたがここにいるから、このアプローチに沿ってやってみてください。

0:21:53.120,0:21:59.400
私たちはこのソフトウェアスタックを使うつもりです。

0:21:59.400,0:22:00.400
レイチェル、何かありましたか？

0:22:00.400,0:22:02.510
アプローチについて、もう一つだけ言わせてください。

0:22:02.510,0:22:07.390
私たちの多くが長年ボトムアップの伝統的な教育アプローチで過ごしてきたので、

0:22:07.390,0:22:12.030
最初は非常に違和感を覚えることがあると思います。

0:22:12.030,0:22:16.620
私は今でも時々、この考えにコミットしているにもかかわらず、
このアプローチに違和感を覚えることがあります。

0:22:16.620,0:22:22.850
それは時々自分自身をキャッチして、「詳細を知らなくても平気でいなければいけない」というのには

0:22:22.850,0:22:28.190
慣れていないと感じることもあるし、 間違っているとさえ感じることもあります。

0:22:28.190,0:22:31.930
例えば... 「ちょっと待って、私は何かを細部まで理解していないまま使っている」 とか。

0:22:31.930,0:22:36.370
でも、そういう詳細は後で到達することを信用する必要があるんですよね。

0:22:36.370,0:22:39.560
私はそのようなことに多くの時間を費やしていないので、
共感することはできないのですが。

0:22:39.560,0:22:44.050
しかし、一つ言えるのはこの方法で教えることはとてもとても難しいという事です。

0:22:44.050,0:22:49.430
私はしばしば、基礎からのアプローチに戻ってしまうことがあります。

0:22:49.430,0:22:51.810
なぜなら、「ああ、これを知っておく必要があるんだ、

0:22:51.810,0:22:52.810
これを知っておく必要がある、

0:22:52.810,0:22:53.810
これをする必要がある、

0:22:53.810,0:22:55.140
そうすれば、これを知ることができる。」というように

0:22:55.140,0:22:56.540
教えるのは簡単です。

0:22:56.540,0:23:01.760
だから私はこの方法を教えるのはずっと難しいのですが、
それだけの価値があるといいなと思います。

0:23:01.760,0:23:06.980
私たちは、この形式をディープラーニングを取り入れる方法を
考え出すのに長い時間を費やしました。

0:23:06.980,0:23:11.620
しかし、ここで私たちを助けてくれるものの一つは、
私たちが利用できるソフトウェアです。

0:23:11.620,0:23:23.010
もしPythonを使ったことがないのであれば、Pythonは
非常に柔軟で表現力があり、使いやすい言語です。

0:23:23.010,0:23:28.210
私たちが好きではない部分もたくさんありますが、
基本的にはとても気に入っています。

0:23:28.210,0:23:33.790
そして、最も重要なことは、ディープラーニングの実践者や研究者の大多数が

0:23:33.790,0:23:38.000
Pythonを使っているということです。

0:23:38.000,0:23:44.140
Pythonの上には、PyTorchとTensorFlowの2つのライブラリがあります。

0:23:44.140,0:23:47.550
ここには非常に急速な変化がありました。

0:23:47.550,0:23:51.090
TensorFlow は数年前まで私たちが教えていたものです。

0:23:51.090,0:23:54.970
数年前までは誰もが使っていたものです。

0:23:54.970,0:24:00.370
基本的にTensorFlowは非常に困難な状況に陥りました。

0:24:00.370,0:24:05.930
そしてPyTorchというソフトウェアが登場したのですが、これはもっと簡単に使えて、

0:24:05.930,0:24:10.320
研究者にとってはもっと便利なものでした。

0:24:10.320,0:24:21.280
この12ヶ月間で、主要なカンファレンスでPyTorchを使った論文の割合が20%から80%に、

0:24:21.280,0:24:24.960
TensorFlowを使った論文の割合が80%から20%になりました。

0:24:24.960,0:24:30.800
つまり、 基本的には、実際に技術を構築している人たちはみんなPyTorchを使っていて、

0:24:30.800,0:24:35.340
業界の動きはもう少しゆっくりですが、来年か再来年には

0:24:35.340,0:24:38.900
業界でも同じようなことが起こるでしょう。

0:24:38.900,0:24:44.800
PyTorchの特徴は、非常に柔軟性が高く、

0:24:44.800,0:24:52.560
柔軟性と開発者の利便性を考慮して設計されていますが、初心者向けではありません。

0:24:52.570,0:24:57.950
つまり、PyTorch を使って簡単に何かを素早く

0:24:57.950,0:25:05.630
構築できるようにするための高レベルの API はありません。

0:25:05.630,0:25:13.700
そこで、この問題に対処するために、PyTorchの上にある
fastaiというライブラリがありす。

0:25:13.700,0:25:20.330
FastaiはPyTorchの高レベルAPIとして最も人気のあるものです。

0:25:20.330,0:25:27.520
私たちのコースがあまりにも人気があるために、
fastaiは初心者のために設計されているとか、

0:25:27.520,0:25:34.260
教えるために設計されていると勘違いしている人がいますが、

0:25:34.260,0:25:43.840
初心者やティーチングのためだけに設計されているだけでなく、
業界の実務者や研究者のためにも設計されています。

0:25:43.840,0:25:50.290
私たちは、どのレベルの人たちにとっても最高のAPIであるために、

0:25:50.290,0:25:59.340
レイヤードAPIと呼ばれるものを使っています。
シルヴァンと私が書いた査読付きの論文には、

0:25:59.340,0:26:04.600
私たちがどのようにしてそれを行ったかが書かれています。
これは、ソフトウェアエンジニアの人達にとっては

0:26:04.600,0:26:08.130
珍しくも驚くこともないと思います。

0:26:08.130,0:26:12.590
完全に標準的なソフトウェアエンジニアリングのプラクティスですが、

0:26:12.590,0:26:17.030
私たちが見てきたディープラーニングライブラリにはないプラクティスです。

0:26:17.030,0:26:24.500
基本的には多くのリファクタリングとデカップリングが行われており、そのアプローチを使うことで、

0:26:24.500,0:26:32.410
超低レベルの研究を行うことができ、最先端の生産モデルを作成することができ、超簡単で初心者でも

0:26:32.410,0:26:43.850
世界レベルのモデルを作成することができるライブラリを構築することができました。

0:26:43.850,0:26:49.940
これが基本的なソフトウェアスタックです。
他にも、途中で学ぶことになるソフトウェアはあります。

0:26:49.940,0:26:54.570
しかしここで言及すべき主なことは、ソフトウェアスタックは
実際にはあまり重要ではないということです。

0:26:54.570,0:27:01.300
このソフトウェアスタックを学んだ後、仕事でTensorFlowと
Kerasを使う必要が出てきたとしても、

0:27:01.300,0:27:06.380
1週間もしないうちに切り替えることができるでしょう。

0:27:06.380,0:27:13.070
多くの学生がそうしてきましたが、
問題になったことは一度もありません。

0:27:13.070,0:27:20.700
重要なのはコンセプトを学ぶことなので、
今回はそのコンセプトに焦点を当てていきます。

0:27:20.700,0:27:27.870
使用しなければならないボイラプレートの量を
最小限に抑えるAPIを使用することで、

0:27:27.870,0:27:29.730
重要な部分に集中できることを意味します。

0:27:29.730,0:27:38.150
実際のコードの行は、あなたが実装しているコンセプト（概念）に
直に対応しています。

0:27:38.150,0:27:41.450
GPUマシンが必要になります。

0:27:41.450,0:27:49.420
GPUはGraphics Processing Unitであり、
具体的にはNvidia GPUが必要です。

0:27:49.420,0:27:55.520
他のブランドのGPUは、どのディープラーニングのライブラリからも
うまくサポートされていないのです。

0:27:55.520,0:27:56.970
買わないでください。

0:27:56.970,0:28:00.130
もし既に持っていても、それを使うべきではないでしょう。

0:28:00.130,0:28:05.400
その代わりに、私たちがすでに用意しているプラットフォームを使ってください。

0:28:05.400,0:28:10.410
GPUマシンのシステム管理やドライバのインストールなどに

0:28:10.410,0:28:16.330
時間を割くのは気が散るだけです。

0:28:16.330,0:28:19.000
そしてLinuxで実行してください。

0:28:19.000,0:28:22.260
私たちだけでなく、みんながLinuxで実行しています。

0:28:22.260,0:28:23.370
自分の人生を楽にしてください。

0:28:23.370,0:28:28.200
ディープラーニングを学習するのは、あらゆる種類の難解な

0:28:28.200,0:28:31.550
ハードウェアサポートの問題なしでも十分に難解です。

0:28:31.550,0:28:43.480
無料で使えるオプションがたくさんあるので、ぜひ使ってみてください。

0:28:43.480,0:28:48.280
もし無料ではないオプションを使っている場合は、
インスタンスをシャットダウンすることを忘れないでください。

0:28:48.280,0:28:51.730
つまり、あなたは世界のどこかにあるサーバーを起動して、

0:28:51.730,0:28:57.260
あなたのコンピュータからそれに接続して、トレーニングをしたり、

0:28:57.260,0:29:01.360
モデルを構築したり実行することになります。

0:29:01.360,0:29:06.880
ブラウザのウィンドウを閉じたからといって、
サーバが全体的に停止するわけではありません。

0:29:06.880,0:29:11.320
だからシャットダウンするのを忘れないでください。
そうしないとお金を払うことになります。

0:29:11.320,0:29:16.120
Colabは無料の素晴らしいシステムです。

0:29:16.120,0:29:18.650
有料のサブスクリプション版もあります。

0:29:18.650,0:29:21.200
Colabには気をつけて欲しいことがあります。

0:29:21.200,0:29:26.590
私たちがお勧めする他のシステムのほとんどは、
あなたの作業を自動的に保存し、

0:29:26.590,0:29:28.460
いつでも戻ってくることができます。

0:29:28.460,0:29:29.460
Colabはそうではありません。

0:29:29.460,0:29:37.110
そのため、フォーラムの「Colabプラットフォーム」のスレッドを
チェックして、その使い方学ぶようにしてください。

0:29:37.110,0:29:44.020
それで、フォーラムについても触れておきますが...

0:29:44.020,0:29:51.970
フォーラム（https://forums.fast.ai/）は本当に重要です。
そこで、すべての議論や設定、

0:29:51.970,0:29:54.030
すべてのことが行われます。

0:29:54.030,0:29:56.240
例えばセットアップのヘルプスレッドがあり、

0:29:56.240,0:30:03.820
Colabをどのようにセットアップするのがベストなのかを知ることができたり

0:30:03.820,0:30:09.150
それについてのディスカッションを読んだり質問もできます。

0:30:09.150,0:30:12.310
質問をする前に検索することを忘れないでください。

0:30:12.310,0:30:18.620
あなたがこのコースを最初の頃に受講している人でない限り、おそらく以前にも

0:30:18.620,0:30:22.530
質問されたことがあると思いますので。

0:30:22.530,0:30:24.820
では、[咳]...

0:30:24.820,0:30:31.570
ステップ１は、フォーラムやコースのウェブサイトの指示に従って、

0:30:31.570,0:30:33.620
サーバーをセットアップします。

0:30:33.620,0:30:39.880
コースのウェブサイトには、各プラットフォームの一歩一歩の説明がたくさんあります。

0:30:39.880,0:30:46.400
それぞれのプラットフォームは価格、速度、可用性などが異なります。

0:30:46.400,0:30:52.560
これらの指示に従うことが終わると、その指示の最後のステップでは、

0:30:52.760,0:30:57.790
次のようなものが表示されます。
コースV4のフォルダ、

0:30:57.790,0:31:01.290
つまりこのコースのバージョンです。

0:31:01.290,0:31:04.820
このビデオをご覧になる頃にはもっとたくさんのものが入っていると思いますが、

0:31:04.820,0:31:07.670
ノートブックスフォルダを意味する「NBS」が入っています。

0:31:07.670,0:31:15.260
これをクリックすると、コースのすべてのノートが表示されます。

0:31:15.260,0:31:21.060
下にスクロールして、app_jupyter.ipynbというファイルを見つけてください。

0:31:21.060,0:31:27.760
それをクリックすると、ここからジュピターノートブックの学習を始めることができます。

0:31:27.760,0:31:30.220
ジュピターノートブックって何？

0:31:30.220,0:31:39.630
ジュピターノートブックは何かを入力して、Shift-Enterキーを押すと

0:31:39.630,0:31:41.250
答えが返ってきます。

0:31:41.250,0:31:47.440
あなたが入力しているのはPythonのコードで、出てくるのは

0:31:47.440,0:31:48.780
そのコードの結果です。

0:31:48.780,0:31:52.490
Pythonだったら何でも入力することができます。

0:31:52.490,0:31:56.620
XはXかける４。

0:31:56.620,0:32:05.240
X足す１。見ての通り、表示すべき結果があれば
いつでも結果を表示します。

0:32:05.240,0:32:11.490
なので、以前に少しコーディングをしたことがある人は、これがREPLだとわかるでしょう。

0:32:11.490,0:32:16.370
R-E-P-L、読み込み（Read）、評価（Evaluate）、印刷（Print）、ループ（Loop）のこと。

0:32:16.370,0:32:18.940
ほとんどの言語には何らかの REPL があります。

0:32:18.940,0:32:29.320
ジュピター ノートブックの REPL は特に興味深いもので、見出し、グラフィカルな出力、

0:32:29.320,0:32:34.850
インタラクティブで、マルチメディアのようなものも持っています。

0:32:34.850,0:32:37.680
これは本当に驚くべきソフトウェアです。

0:32:37.680,0:32:39.850
本当に大きな賞をいくつか受賞しています。

0:32:39.850,0:32:48.200
Bashのようなシェル以外では最も広く使われているREPLだと思います。

0:32:48.200,0:32:50.280
非常に強力なシステムです。

0:32:50.280,0:32:51.330
私たちはこれを愛しています。

0:32:51.330,0:32:55.940
私たちの本も全部このシステムで書いていますし、
fastaiのライブラリも全部このシステムで書いていますし、

0:32:55.940,0:32:58.970
私たちの授業も全部このシステムで行っています。

0:32:58.970,0:33:06.780
IDEでほとんどの仕事をしてきた人にとっては、非常に馴染みのないものです。

0:33:06.780,0:33:11.070
おそらく初めてGUIからコマンドラインに移行したときと同じくらいの

0:33:11.070,0:33:13.200
気まずさを感じると思ってください。

0:33:13.200,0:33:14.200
別物です。

0:33:14.200,0:33:22.080
REPL ベースのシステムに慣れていない人にとっては、
とても違和感を感じるでしょう。

0:33:22.080,0:33:25.170
しかし、それは本当に素晴らしいものなので、続けてみてください。

0:33:25.170,0:33:31.980
ここで何が行われているかというと、私が見ているこのウェブページでは、

0:33:31.980,0:33:37.460
サーバが行うべきことを私が入力すると、サーバが行った計算の結果を表示してくれます。

0:33:37.460,0:33:40.170
つまり、サーバーはどこか別の場所にあるということです。

0:33:40.170,0:33:42.650
私のコンピュータ上では実行されていないのです。

0:33:42.650,0:33:45.810
コンピュータ上で動いているのはこのウェブページだけです。

0:33:45.810,0:33:53.360
しかし、私が何かをしていると、例えば、「XはXの3倍に等しい」と言った場合、

0:33:53.360,0:33:55.930
サーバーの状態が更新されます。

0:33:55.930,0:33:56.950
ここで言うサーバーの状態とは

0:33:56.950,0:34:00.530
現在のXの値が何であるということで、その値を 表示することができます。

0:34:00.530,0:34:03.530
今のXは以前とは違うものになっています。

0:34:03.530,0:34:09.669
この行をここでやっても、先ほどのX＋1は変わらないでしょう？

0:34:09.669,0:34:14.730
ということは、ジュピターのノートを見たときに、サーバの現在の状態が

0:34:14.730,0:34:16.559
表示されていないということですね。

0:34:16.559,0:34:21.679
印刷した時の状態を表示しているだけです。

0:34:21.679,0:34:24.889
これはBashのようなシェルを使うのと同じで

0:34:24.889,0:34:26.780
"ls" と入力します。

0:34:26.780,0:34:28.629
そしてファイルを削除する。

0:34:28.629,0:34:31.990
以前に印刷した "ls" はそれに従ってって更新されることはありません。

0:34:31.990,0:34:35.559
それが REPL の一般的な動作です。

0:34:35.559,0:34:38.679
これも含めて。

0:34:38.679,0:34:43.280
ジュピターノートブックには2つのモードがあります。

0:34:43.280,0:34:48.880
一つは編集モードで、セルをクリックするとカーソルが点滅して、

0:34:48.880,0:34:53.389
左右に移動して入力することができます。

0:34:53.389,0:34:55.740
このモードではキーボードショートカットはあまりありません。

0:34:55.740,0:35:01.609
ひとつ便利なのは「control」か「command」＋「/」で行を
コメント化したり、コメントを外したりしてくれます。

0:35:01.609,0:35:07.509
知っておくべき主なものは、実際にセルを実行するために「shift」+「enter」です。

0:35:07.509,0:35:09.950
その時点でもうカーソルの点滅はありません。

0:35:09.950,0:35:12.319
それは、今はコマンドモードになっているということです。

0:35:12.319,0:35:13.640
編集モードではありません。

0:35:13.640,0:35:17.460
上に行ったり下に行ったりしながら、違うセルを選択しています。

0:35:17.460,0:35:23.480
今はコマンドモードで移動しながらセルを選択しているわけです。

0:35:23.480,0:35:26.829
キーボードショートカットもたくさんあります。

0:35:26.829,0:35:30.600
"H" を押すと、そのリストが表示されます。

0:35:30.600,0:35:36.150
「control」や「command」で始まるものはあまりなく、

0:35:36.150,0:35:38.049
文字だけで構成されていることがわかります。

0:35:38.049,0:35:41.799
だからVimのようなものを使ったことがれば、この考え方の方に馴染みやすいと思います。

0:35:41.799,0:35:45.680
だから例えば、私はコピーするために "C "を押して、貼り付けるために "V "を押すと、

0:35:45.680,0:35:47.440
セルがコピーされます。

0:35:47.440,0:35:50.769
または X" で切り取ります。

0:35:50.769,0:35:55.150
"A "で新しいセルを上に追加します。

0:35:55.150,0:35:58.780
それから様々な数字キーを押して 見出しを作ります。

0:35:58.780,0:36:01.890
だから、数字の2は見出しレベル２を作成します。

0:36:01.890,0:36:08.319
見ての通り、私はコードだけでなく、実際にフォーマットされた
テキストを入力することができます。

0:36:08.320,0:36:15.560
フォーマットされたテキストはMarkdownです。

0:36:18.880,0:36:22.840
こんな感じです。

0:36:22.840,0:36:24.360
私の番号付きリストはうまくいかなかった。

0:36:26.040,0:36:26.800
これです。

0:36:26.800,0:36:28.880
これがMarkdownです。

0:36:28.880,0:36:35.950
以前にMarkdownを使用したことがない人、これはフォーマット済み
テキストを書くのに非常に便利な方法です。

0:36:35.950,0:36:38.359
非常に広く使われています。

0:36:38.359,0:36:41.760
とても便利なので学んでください。

0:36:41.760,0:36:46.390
そして、ジュピターでの作業に必要です。

0:36:46.390,0:36:51.930
私たちの本のノートブックを見てください。

0:36:51.930,0:36:57.839
例えば、ここにはすべての種類のフォーマットやコードの例があります。

0:36:58.839,0:37:05.240
なので、app_jupyter.ipynb を見てみてください

0:37:05.240,0:37:08.869
ここでは、例えばプロットを作成する方法を見ることができます。

0:37:08.869,0:37:10.900
また、リストを作成することもできます。

0:37:10.900,0:37:12.869
ライブラリをインポートしたり、

0:37:12.869,0:37:18.430
画像の表示などもできます。

0:37:18.430,0:37:25.740
新しいノートブックを作成したい場合は、"New" "Python 3 "で新しいノートブックを

0:37:25.740,0:37:29.940
作成することができます。

0:37:29.940,0:37:35.450
デフォルトでは "Untitled "という名前になっているので、好きな名前に

0:37:35.450,0:37:38.460
変更することができます。

0:37:38.460,0:37:45.480
そうすると、リストの中に "newname "という名前が表示されます。

0:37:45.480,0:37:50.269
ジュピターについて知っておくべきもう一つのことは、ターミナルに
アクセスするのが簡単にできるということです。

0:37:50.269,0:37:51.450
もしターミナルの使い方を知っていれば。

0:37:51.450,0:37:54.480
でもこのコースのためには（少なくとも前半では）必要はありません。

0:37:54.520,0:37:57.600
新しいターミナルに行く場合・・・

0:37:59.440,0:38:04.800
見ての通りこれでターミナルが表示されました。

0:38:08.280,0:38:19.140
これらのノートブックはGithubのリポジトリに接続されています。

0:38:19.140,0:38:21.950
Githubを使ったことがなければ問題ありませんが、

0:38:21.950,0:38:27.290
基本的にはサーバーに接続されていて、そのサーバーで随時私たちはノートブックを

0:38:27.290,0:38:29.480
更新していきます。

0:38:29.480,0:38:33.329
そして、コースのウェブサイトやフォーラムで最新バージョンであるかの

0:38:33.329,0:38:35.890
確認方法をお伝えします。

0:38:35.890,0:38:41.280
あなたが最新版を手に入れるとき、あなたの変更と衝突したり、
上書きしたりするを避けるたいものです。

0:38:41.640,0:38:50.320
そのために、実験を始めるときには、ノートブックを選択してDuplicate（複製）をクリックして、

0:38:50.329,0:38:52.500
コピーしたノートブックで作業を始めるのも悪くないでしょう。

0:38:52.500,0:38:59.099
そうすれば、最新のコース教材の更新があったときに、あなたが実行していた実験に

0:38:59.099,0:39:04.000
支障をきたすことはありません。

0:39:06.089,0:39:09.480
このように、2つの重要なリポジトリがあります。

0:39:09.490,0:39:20.089
一つは先ほど見たファストブックのリポジトリで、これはすべての出力や長所などが

0:39:20.089,0:39:26.099
掲載されている完全な本です。

0:39:26.099,0:39:30.390
そしてもう一つはコースV4のリポジトリです。

0:39:30.390,0:39:34.690
これはファストブックのリポジトリにあるノートと全く同じものですが

0:39:34.690,0:39:42.130
このノートでは、すべての文章とすべての写真と出力を削除し、

0:39:42.130,0:39:46.210
見出しとコードだけを残しています。

0:39:46.210,0:39:51.029
この場合、いくつかの出力を見ることができますが、それは私がコードを実行したからです。

0:39:51.029,0:39:53.410
他は何もない・・・

0:39:53.410,0:39:56.300
いや、出力を残していると思います。

0:39:56.300,0:39:58.009
それを残しておくかどうかはわかりません。

0:39:58.009,0:40:01.849
なので、出力が見えても見えなくても構いません。

0:40:01.849,0:40:04.259
これを使うというアイデアは、

0:40:04.259,0:40:10.480
これらがあなたが実験するために使うべきバージョンです。

0:40:10.480,0:40:15.170
なぜなら、本を読んで何も考えずに実行するのではなく、各ステップを実行しながら

0:40:15.170,0:40:18.799
何が起こっているのかを考えることを強制されるからです。

0:40:18.799,0:40:24.059
本に書かれていることは何だったのか、なぜこのようなことが起こったのか、

0:40:24.059,0:40:31.321
何か忘れてしまったら本に戻るというように、小さな裸の環境でやってほしいと思っています。

0:40:31.321,0:40:36.910
もう一つは、コースV4版とファストブック版の両方に

0:40:36.910,0:40:42.000
アンケートがあることです。

0:40:42.000,0:40:46.529
かなりの数の人（例えば復習している人たち）が、実際に最初にアンケートを

0:40:46.529,0:40:49.990
読んだと言ってくれました。

0:40:49.990,0:40:57.869
シルヴァンと私は何週間もかけてアンケートを書いてきました。

0:40:57.869,0:41:04.380
その理由は、それぞれのノートから何を読み取ってほしいかを

0:41:04.380,0:41:07.680
考えているからです。

0:41:07.680,0:41:10.029
最初にアンケートを読んでもらうと、私たちが大切だと思うことは

0:41:10.029,0:41:14.940
何なのかが分かると思います。次に進む前に知っておくべきことは何か。

0:41:14.940,0:41:19.560
最後にまとめのようなセクションがあって「最後に
知っておくべきことはこれとこれ」というのではなく、

0:41:19.560,0:41:24.920
その同じ目的をアンケートで達成することにしました。

0:41:24.920,0:41:27.730
なので、次の章に移る前に必ずアンケートをしてください。

0:41:27.730,0:41:31.600
すべてを正しく理解する必要はありませんし、質問に答えるのは、

0:41:31.600,0:41:37.480
ノートのその部分に戻って散文を読むくらい簡単です。

0:41:37.480,0:41:41.400
だから、何か見逃したことがあったら、ちゃんと戻って読んでください。

0:41:41.400,0:41:44.760
なぜなら私たちはあなたがこれらを知っていることを前提としています。

0:41:44.760,0:41:50.400
だから先に進む前にこれらのことを知らないと イライラすることになりかねません。

0:41:50.420,0:41:57.480
とはいえ、何度かやってみてどうしても行き詰まったら、次の章に移って、

0:41:57.499,0:42:00.809
あと2～3章をやってから戻ってきてください。

0:42:00.809,0:42:05.480
もう2、3章やった後にはもうちょっと視野が広がってるかもしれません。

0:42:05.480,0:42:13.099
私たちは違う方法で何度も説明し直すようにしているので、

0:42:13.099,0:42:17.329
もし試してみて行き詰ったとしても大丈夫です。

0:42:17.329,0:42:26.410
さて、それではノートの最初の部分を実行してみましょう。

0:42:26.410,0:42:37.490
これは01_introのノートブック。
これは第1章で、これが最初のセルです。

0:42:37.490,0:42:40.800
セルをクリックすると・・・

0:42:40.800,0:42:46.040
デフォルトではツールバーにヘッダーが表示されます。

0:42:46.040,0:42:47.480
これをオンにしたりオフにしたりすることができます。

0:42:47.480,0:42:49.440
私はいつもオフにしています。

0:42:49.440,0:42:54.560
このセルを実行するには、Run（再生）ボタンをクリックするか、

0:42:54.640,0:42:56.920
先ほど言ったようにシフト・エンターキーを押してください。

0:42:56.920,0:43:00.040
このセルではRunボタンをクリックします。

0:43:00.560,0:43:03.120
すると星が表示されるので、

0:43:03.360,0:43:07.720
実行中だというのがわかり、この進行状況バーがポップアップします。

0:43:07.720,0:43:15.680
これは数秒かかります。
実行中には、いくつかの結果が表示されます。

0:43:15.680,0:43:21.920
私たちと全く同じ結果を期待しないでください。
モデルのトレーニングにはランダム性がありますが、

0:43:21.920,0:43:23.760
それは大丈夫です。

0:43:23.779,0:43:26.530
我々と全く同じ時間を得ることは期待しないでください。

0:43:26.530,0:43:32.510
もしあなたが本当に古いGPUを持っていない限り、この最初のセルに5分以上かかるなら、

0:43:32.510,0:43:33.510
それはおそらく悪い兆候です。

0:43:33.510,0:43:38.609
フォーラムに参加して、何が問題なのかを調べてたほうがいいです。
またはWindowsを使っている場合。

0:43:38.609,0:43:43.400
今のところWindows ではあまりうまく動作しません。

0:43:43.400,0:43:45.210
まだすべてのコードが何をするかわからなくても心配しないでください。

0:43:45.210,0:43:52.410
私たちはただモデルを訓練できるかどうかを確認しているだけです。
これで実行が終了しました。

0:43:52.410,0:43:58.079
見ての通り、いくつかの情報が出力され、この場合、何かを行う際に

0:43:58.079,0:44:05.269
0.005のエラー率があることを示しています。

0:44:05.269,0:44:06.569
何をしているのでしょうか？

0:44:06.569,0:44:14.089
ここで何をしているのかというと、実際にデータセットを取得しているのですが、

0:44:14.089,0:44:18.849
これはペットデータセットと呼ばれていて、猫と犬の写真のデータセットです。

0:44:18.849,0:44:25.829
これはどれが猫でどれが犬なのかを判断しようとしています。

0:44:25.829,0:44:32.599
見ての通り、1分もしないうちに、0.5%の誤差率で

0:44:32.599,0:44:34.809
これを行うことができました。

0:44:34.809,0:44:37.039
つまり、ほぼ完璧にできるということです。

0:44:37.039,0:44:39.509
これで最初のモデルを訓練しました。

0:44:39.509,0:44:40.539
どうやったのかは分かりませんし、

0:44:40.539,0:44:41.799
何をしていたのかもわかりません。

0:44:41.799,0:44:44.259
しかし、私たちはは確かにモデルを訓練しました。

0:44:44.259,0:44:46.559
これで良いスタートが切れました。

0:44:46.559,0:44:51.309
見ての通り、1台のコンピュータでかなり速く
モデルを訓練することができます。

0:44:51.309,0:44:55.089
ご存知の通り、その多くは無料で手に入入ります。

0:44:55.089,0:45:00.869
もう一つ、もしあなたがMacを持っているのであれば・・・　ブラウザで何を

0:45:00.869,0:45:05.069
実行しているかという点では、WindowsでもMacでもLinuxでも関係ありません。

0:45:05.069,0:45:11.470
しかし、Macを持っている人は、そのGPUを使おうとしないでください。

0:45:11.470,0:45:16.150
Macは、いえ、 AppleはもうNvidiaのGPUすらサポートしていません。

0:45:16.150,0:45:19.470
だから、それは本当に良い選択肢にはなりません。

0:45:19.470,0:45:20.869
だからLinuxを使って下さい。

0:45:20.869,0:45:25.010
そうすれば、あなたの生活はずっと楽になります。

0:45:25.010,0:45:30.400
そうですね、最初にすべきことは実際に試してみることです。

0:45:30.400,0:45:34.720
犬から猫を判別するモデルを訓練したと言っても、

0:45:34.760,0:45:37.640
本当にできるかどうか確認してみましょう。

0:45:37.640,0:45:41.859
このセルを見てみよう。

0:45:41.859,0:45:43.859
これは面白いでしょう？

0:45:43.859,0:45:47.940
widget.FileUploadオブジェクトを作成して表示すると、

0:45:47.940,0:45:50.690
これは実際にクリック可能なボタンです。

0:45:50.690,0:45:52.619
だから、先ほども言ったように、これは珍しいREPLです。

0:45:52.619,0:45:55.319
このREPLではGUIを作成することもできます。

0:45:55.319,0:45:58.359
だから、私はこのファイルのアップロードをクリックして、

0:45:58.359,0:46:00.170
猫の写真を選択できます。

0:46:00.170,0:46:04.130
この通り。

0:46:04.130,0:46:11.230
そして、アップロードしたデータを画像に変換することができます。

0:46:11.230,0:46:14.319
猫がいます。

0:46:14.320,0:46:21.400
そして予測をすることができ、結果は猫です。

0:46:21.400,0:46:26.400
99.96%の確率で。

0:46:26.400,0:46:29.910
ということで、自分で選んだ画像をアップロード
できることがわかりますね。

0:46:29.910,0:46:31.930
これをやってみてください。

0:46:31.930,0:46:32.930
猫の写真をネットで探すか

0:46:32.930,0:46:35.579
自分で撮ってきて、 アップロードした写真が

0:46:35.579,0:46:38.910
猫であることを確認してください。

0:46:38.910,0:46:43.520
これは猫の線画ではなく 猫の写真を認識できるものです。

0:46:43.520,0:46:46.940
これからのコースでわかるように、

0:46:46.940,0:46:52.050
この種のモデルは、あなたが与えた情報からのみ
学習することができます。

0:46:52.050,0:46:57.130
これまでのところ、私たちは猫の写真しか与えていません。

0:46:57.130,0:47:06.700
アニメの猫でもなく、描かれた猫でもなく、
抽象的な猫の表現でもなく、ただの写真です。

0:47:06.700,0:47:11.470
では次に実際に何が起きたのか？というのをみていきます。

0:47:11.470,0:47:15.930
ご覧の通り、今のところ、私はここに十分な情報を得ていません。

0:47:15.930,0:47:26.259
もしこういう出力をノートに見たら、 File → Trust Notebook　
→ Trustという操作をして下さい。

0:47:26.259,0:47:30.559
これはジュピターに表示に必要なコードの実行を許可すると伝えているのです。

0:47:30.559,0:47:33.509
セキュリティ上の問題がないと。

0:47:33.509,0:47:35.880
これで出力が表示されます。

0:47:35.880,0:47:39.880
時々、このような奇妙なコードを見ることがあります。

0:47:39.880,0:47:43.609
これは以下の図を作成するコードです。

0:47:43.609,0:47:46.349
このようなコードは基本的に隠します。

0:47:46.349,0:47:47.800
しかし時には見せることもあります。

0:47:47.800,0:47:51.940
しかし，一般にこのようなコードは無視して、出力されている図に

0:47:51.940,0:47:52.940
集中してください。

0:47:52.940,0:47:54.300
ですので，毎回こういったコードを紹介するつもりはありません。

0:47:54.300,0:48:00.660
代わりにスライドを見てみましょう。

0:48:00.660,0:48:04.710
ここでやっていることは、機械学習です。

0:48:04.710,0:48:07.750
ディープラーニングは機械学習の一種です。

0:48:07.750,0:48:09.170
機械学習とは何か。

0:48:09.170,0:48:16.070
機械学習とは、プログラミングと同じで、
コンピュータに何かを処理させる方法です。

0:48:16.070,0:48:22.250
しかし、写真に映っているのが猫か犬かを識別するプログラムを書くのは

0:48:22.250,0:48:24.000
想像しにくいでしょう。

0:48:24.000,0:48:28.319
写真の中の犬と猫を認識するプログラムを作るために、ループや変数の代入、

0:48:28.319,0:48:31.789
条件式をどう使えば良いのでしょうか？

0:48:31.789,0:48:33.190
これは大変難しいです。

0:48:33.190,0:48:34.589
本当に難しいです。

0:48:34.589,0:48:41.420
ディープラーニングの時代までは、一見簡単そうな作業（犬猫の識別）を正確にこなすモデルは

0:48:41.420,0:48:43.910
誰も持っていませんでした。

0:48:43.910,0:48:46.970
必要なステップを書き切れないからです。

0:48:46.970,0:48:55.569
通常は、いくつかの入力を処理し，いくつかの結果を返す関数を書きます。

0:48:55.569,0:49:02.530
ですので，人間がステップを書き下す方式のプログラムは

0:49:02.530,0:49:06.970
写真の認識などが難しいのです。

0:49:06.970,0:49:12.470
1949年に、アーサー・サミュエルという人が、猫や犬の認識といった問題を

0:49:12.470,0:49:16.030
プログラムに解かせる方法を見つけようとし始めました。

0:49:16.030,0:49:23.000
1962年にその方法を説明しました。

0:49:23.000,0:49:26.270
まず彼は問題に関して次のように述べました。
「この種の計算のためのプログラムを書くのは

0:49:26.270,0:49:31.070
困難な作業である。

0:49:31.070,0:49:37.589
なぜなら、処理に含まれる微細なステップを
うんざりするほど詳細に綴らなければならないのだから。

0:49:37.589,0:49:42.290
コーダーなら誰もが知っているように，
コンピュータは巨大なマヌケである。」

0:49:42.290,0:49:46.769
そこで彼は、コンピュータに正確な手順を教えるのではなく、問題の例を与えて、

0:49:46.769,0:49:50.460
その問題を解く方法をコンピュータに考えさせよう、と言いました。

0:49:50.460,0:49:56.269
そして、1961年までに、彼はチェッカープログラムを作成し、
コネチカット州のチャンピオンを倒しました。

0:49:56.280,0:50:02.920
コンピューターにチェッカーの遊び方を教えたのではなく

0:50:02.920,0:50:09.990
「実際のパフォーマンスから重み（パラメータ）の有効性を

0:50:09.990,0:50:15.690
自動でテストする手段とパフォーマンスを
最大化するように重み（パラメータ）を

0:50:15.690,0:50:18.880
変更するメカニズムを構築する」。

0:50:18.880,0:50:21.560
この文章が肝心です。

0:50:21.560,0:50:24.440
そして、かなりトリッキーな文章なので、
時間をかけても大丈夫です。

0:50:24.440,0:50:26.040
基本的な考え方は何かというと、

0:50:26.040,0:50:31.400
プログラムへの入力があって、プログラムが何かを出力するというのではなく

0:50:31.960,0:50:34.680
その代わりに入力を・・・

0:50:34.680,0:50:38.120
ここではプログラムではなくモデルと呼びましょう。

0:50:38.140,0:50:40.349
モデルへの入力と結果。

0:50:40.349,0:50:43.760
そして、重みと呼ばれる2つ目のものがあります。

0:50:43.760,0:50:50.569
基本的な考え方としては、このモデルは、
例えばチェッカーボードの状態だけでなく、

0:50:50.569,0:50:58.410
モデルがどのように動作するかを記述する重み、あるいはパラメータに基づいて

0:50:58.410,0:51:02.320
出力を作成するものです。

0:51:02.320,0:51:09.039
もし私たちがチェッカーの全ての作戦を列挙できたとして、

0:51:09.040,0:51:14.120
その各の方法をパラメータを使って説明することができて（サミュエルは重みと呼びますが）

0:51:15.280,0:51:19.640
現在の重みの割り当てが、実際のパフォーマンスの面で

0:51:19.640,0:51:23.080
どれだけ効果的かを検証する方法があれば、

0:51:23.120,0:51:30.760
言い換えれば、チェッカーをプレイするための特定の戦略が、ゲームの勝ち負けにつながるかどうか、

0:51:30.760,0:51:33.140
そして、パフォーマンスを最大化するように重みの割り当てを

0:51:33.140,0:51:35.599
変更する方法があるとします。

0:51:35.600,0:51:40.120
そうすると、重みを一つずつ増やしたり減らしたりしてみて、

0:51:40.720,0:51:45.190
少しでも良いチェッカーの戦略があるかどうかを調べるというのを

0:51:45.190,0:51:51.950
何回も何回もやってみると、最終的にはそのような手順が完全に自動化されて、

0:51:51.960,0:51:56.680
機械がその経験から学習するようにプログラムされます。

0:51:56.680,0:51:59.800
このパラグラフそのものです。

0:52:00.200,0:52:04.760
これが機械学習で、処理を与えられるのではなく、

0:52:04.760,0:52:07.440
処理を学習するのです。

0:52:11.360,0:52:16.920
もしそのようなものがあったとしたら、我々は基本的に
今、このようなものを持っているでしょう。

0:52:16.930,0:52:22.549
入力と重みがモデルの入力となり、結果、つまりチェッカーでの勝敗，を出力し，

0:52:22.549,0:52:26.769
これらのパフォーマンスを評価します。

0:52:26.769,0:52:28.760
この評価が重要なステップです。

0:52:28.760,0:52:30.880
2つ目の重要なステップは、測定されたパフォーマンスに基づいて

0:52:30.880,0:52:35.440
重みを更新する方法で、この一貫の処理を繰り返し行うことで

0:52:35.720,0:52:39.800
機械学習モデルの訓練（学習）ができます。

0:52:40.080,0:52:43.440
これは抽象的なアイデアです。

0:52:43.440,0:52:49.240
しばらく実行した後、重みがかなり良い感じになりました。

0:52:49.240,0:52:55.080
ここで，どう訓練したかを無視すれば，機械学習モデルは
従来のプログラムと同様なものになります。

0:52:55.920,0:52:57.920
このようなものです。

0:52:58.040,0:53:02.359
ここでは「プログラム」ではなく「モデル」という単語が使われています。

0:53:02.359,0:53:07.239
訓練されたモデルは他のコンピュータプログラムと
同じように使うことができます。

0:53:07.240,0:53:10.880
つまり、我々はコンピュータプログラムを，

0:53:10.880,0:53:14.240
タスクを実行するための必要なステップを設書き下すのではなく、

0:53:14.240,0:53:17.680
タスクを実行するよう学習させることによってプログラムを構築していて、

0:53:17.680,0:53:20.680
訓練の後にはただのプログラムになります。

0:53:21.080,0:53:25.640
 
このプログラムは推論と呼ばれます。

0:53:25.640,0:53:32.440
推論とは、この訓練済みのモデルを使ってチェッカーなどのタスクを解かせることです。

0:53:33.160,0:53:43.960
つまり，機械学習とは，処理を書き下すのではなく，
コンピュータが経験から学習させるプログラムの訓練です。

0:53:45.680,0:53:50.400
これを画像認識の場合、どうやってやったらいいのでしょうか。

0:53:50.400,0:53:59.680
それらを変化させていくと、猫と犬の認識がどんどん良くなっていく「モデル」と「重み」は何でしょう。

0:53:59.680,0:54:02.800
チェッカーの場合

0:54:02.800,0:54:07.440
「相手の駒が自分の駒からどれくらい離れているか」に応じて、

0:54:07.440,0:54:12.280
その状況で何をすべきかを列挙することは想像に難くありません。

0:54:12.289,0:54:16.079
どのように防御的な戦略と攻撃的な戦略を比較するべきか、などなど。

0:54:16.079,0:54:20.410
画像認識のためにどうするのか、全く明らかではありません。

0:54:20.410,0:54:24.200
私たちが本当に必要としているのは

0:54:24.200,0:54:29.000
ここに、モデルに含まれる非常に柔軟な関数と、

0:54:29.240,0:54:33.200
それを如何様にも機能させる重みです。

0:54:33.760,0:54:41.040
本物の...世界で最も柔軟性のある関数のような...
そんなものがあることがわかりました。

0:54:41.059,0:54:44.140
それはニューラルネットワークです。

0:54:44.140,0:54:50.329
今後の授業で，その正確な数式を説明していきます。

0:54:50.329,0:54:56.160
しかし，ニューラルネットワークを使うためには、その数学的な関数が
何であるかは、実際には重要ではありません。

0:54:56.160,0:55:03.640
これは、我々が言うところの重みで「パラメータ化」された関数であり、

0:55:03.640,0:55:10.440
もし重みを変えれば、異なるタスクを実行し、

0:55:10.440,0:55:17.960
実際にはあらゆるタスクを実行するできます。普遍性定理と呼ばれるものは、

0:55:17.970,0:55:26.319
この関数の形は、正しい重みを見つければ，任意の精度で

0:55:26.319,0:55:28.589
あらゆる問題を解けると数学的に証明しています。

0:55:28.600,0:55:35.000
これは以前にミンスキー（マービン・ミンスキー）問題の対処方法に関して
述べたことを言い換えてるようなものです。

0:55:35.000,0:55:39.680
ニューラルネットワークは非常に柔軟性があるので，正しい重みを見つけることができれば

0:55:39.700,0:55:46.289
「これは猫か犬か」などの問題に答えられます。

0:55:46.289,0:55:51.130
つまり、（モデルの）訓練に力を注ぐ必要があります。

0:55:51.130,0:55:57.010
良い重み，これはサミュエルの言い方ですが，を見つけて、それらの適切な配分を見つけることです。

0:55:57.010,0:55:59.770
どうすれば良いのでしょうか？

0:55:59.770,0:56:09.239
非常に汎用的な方法を求めています。
例えば、猫と犬の認識がどれくらい優れているかというような、

0:56:09.239,0:56:14.200
パフォーマンスに基づいて重みを更新することです。

0:56:14.200,0:56:16.839
そして幸運なことに、そのような方法が存在することが判明しました！

0:56:16.839,0:56:21.539
その方法とは、確率的勾配降下法あるいはSGDと呼ばれています。

0:56:21.539,0:56:26.540
今後，実際にSGDがどのように動作するのかを確認し、
スクラッチで作成しますが、

0:56:26.540,0:56:28.529
今のところは心配する必要はありません。

0:56:28.529,0:56:35.210
これだけは言っておきますが、SGDも
ニューラルネットも数学的には全く複雑ではありません。

0:56:35.210,0:56:38.829
ほとんど足し算と掛け算だけです。

0:56:38.829,0:56:46.109
トリックは、足し算と掛け算を、数十億，いやそれより多く、
私たちが直感的に把握できるより多く行っていることです。

0:56:46.109,0:56:52.940
彼らは極めて強力ですが，ロケットのように複雑ではありませんし，

0:56:52.940,0:56:58.609
それらがどのように動くのかをしっかり確認します。

0:56:58.609,0:57:03.049
ここまでがアーサー・サミュエル版です。

0:57:03.049,0:57:08.580
今日では、用語は異なりますが、全く同じ考えを使っています。

0:57:08.580,0:57:12.660
真ん中にあるのがアーキテクチャで

0:57:12.660,0:57:14.549
重みを調整することで何かしらのタスクを

0:57:14.549,0:57:20.779
適切に処理するようになる関数です。

0:57:20.779,0:57:24.190
それがアーキテクチャであり、モデルの関数形です。

0:57:24.190,0:57:28.849
時々、モデルはアーキテクチャを意味すると言われることがありますが、
あまり混乱させないようにしてください。

0:57:28.849,0:57:30.559
しかし、本当に正しい言葉はアーキテクチャーです。

0:57:30.559,0:57:34.619
私たちはそれらを「重み」とは呼ばず、「パラメータ」と呼んでいます。

0:57:34.619,0:57:40.410
重みには特定の意味があり、非常に特殊な種類のパラメータです。

0:57:40.410,0:57:46.609
モデルから出てくるもの、つまりパラメータを持つ
アーキテクチャの出力を、

0:57:46.609,0:57:49.809
私たちは予測と呼んでいます。

0:57:49.809,0:57:55.660
予測は2種類の入力に基づいています：データである独立変数、

0:57:55.660,0:58:03.309
猫や犬の写真のようなもの、そしてラベルとして知られている従属変数、

0:58:03.309,0:58:08.400
これは「これは猫です」「これは犬です」といったもので、

0:58:08.400,0:58:09.779
これが入力です。

0:58:09.779,0:58:12.769
つまり、結果は予測です。

0:58:12.769,0:58:18.670
アーサー・サミュエルの言葉を借りれば、
パフォーマンスの尺度は、損失と呼ばれています。

0:58:18.670,0:58:24.020
損失は予測値のラベルから 計算されます。

0:58:24.020,0:58:26.720
そして、パラメータの更新があります。

0:58:26.720,0:58:34.210
さて、これは先ほどの図と同じですが、
今日使う言葉を入れてみました。

0:58:34.210,0:58:40.039
私が「これがモデルを作成するためにこのアーキテクチャで
使用されているパラメータだ」と言った時、

0:58:40.039,0:58:44.220
もし忘れてしまった場合は、もう一度この図に戻って、
それらの意味を思い出してみてください。

0:58:44.220,0:58:45.220
パラメータとは何か？

0:58:45.220,0:58:46.500
予測値とは？

0:58:46.500,0:58:47.500
損失とは何か？

0:58:47.500,0:58:53.970
パラメータを更新できるような方法でモデルのパフォーマンスを

0:58:53.970,0:58:56.790
測定する関数の損失です。

0:58:56.790,0:59:06.170
ここで注意しなければならないのは、ディープラーニングや
機械学習は魔法ではないということですね。

0:59:06.170,0:59:13.380
モデルは、学習しようとしていることの例を示すデータがあるところでしか

0:59:13.380,0:59:14.869
作成できません。

0:59:14.869,0:59:22.030
それは訓練に使われた入力の中で見たパターンでしか
動作を学習できないんですよね？

0:59:22.030,0:59:26.340
だから、猫や犬の線画像がないと、

0:59:26.340,0:59:30.800
そのアーキテクチャを作るためのパラメータの更新がないので

0:59:30.860,0:59:34.260
（アーキテクチャとパラメータを合わせたものがモデル）

0:59:34.420,0:59:39.650
モデルが猫や犬の線画を予測するのが上手になることはありません。

0:59:39.650,0:59:46.680
それはそういう入力を受け取らなかったため、
その重みの更新を受け取らなかったからです。

0:59:46.680,0:59:51.239
また、この学習アプローチは予測値を作成するだけで
あることにも注意してください。

0:59:51.239,0:59:54.319
それに対して何をすべきかは教えてくれません。

0:59:54.319,0:59:58.019
これは、「誰かにどの製品を勧めるか」というような
レコメンデーションシステムのようなものを考えるときに

0:59:58.019,1:00:01.230
非常に重要になるでしょう。

1:00:01.230,1:00:04.950
私たちはそんなことしませんよね？

1:00:04.950,1:00:09.759
私たちが見せた製品について誰かが何を言うか予測することはできますが、

1:00:09.759,1:00:11.140
私たちは行動を生み出しているわけではありません。

1:00:11.140,1:00:12.310
予測をしているのです。

1:00:12.310,1:00:16.619
これは非常に重要な違いです。

1:00:16.619,1:00:22.470
犬や猫の絵などの入力データの例が
あるだけでは不十分なのです。

1:00:22.470,1:00:26.309
ラベルなしでは何もできません。

1:00:26.309,1:00:31.359
企業がよく言うのは 「データが足りない」
と言うことがよくあります。

1:00:31.359,1:00:35.150
ほとんどの場合は、「ラベル付きのデータが
足りない」という意味です。

1:00:35.150,1:00:39.549
なぜなら、企業がディープラーニングを使って
何かをしようとしている場合、多くの場合、

1:00:39.549,1:00:43.180
すでに行っていることを自動化したり、
改善したりしようとしているからです。

1:00:43.180,1:00:49.420
つまり、理論的には、そのデータを持っているか、
またはそのデータを取得する方法を持っていることを意味します。

1:00:49.420,1:00:50.420
それは彼らはもうそれをやっているからです。

1:00:50.420,1:00:51.420
そうでしょう?

1:00:51.420,1:00:55.089
しかし、しばしば厄介なのは、それにラベルを付けることです。

1:00:55.089,1:00:57.569
例えば医療における

1:00:57.569,1:01:00.789
放射線学のモデルを作成するとします。

1:01:00.789,1:01:06.579
思いつく限りのあらゆる医療画像を
入手できることはほぼ間違いないでしょう。

1:01:06.579,1:01:11.769
しかし、それらを腫瘍の悪性度や髄膜腫の有無などで

1:01:11.769,1:01:18.480
ラベル付けするのは非常に難しいかもしれません。
なぜなら、少なくとも米国の医療システムでは、
1:01:18.480,1:01:24.289
この種のラベルは必ずしも構造化された方法では捉えられていないからです。

1:01:24.289,1:01:31.700
このような違いは、戦略に大きな影響を与える重要なものです。

1:01:31.700,1:01:40.000
モデルとは、PDPの本で見たように、モデルは環境の中で動作します。

1:01:40.000,1:01:44.420
あなたはそれを展開して、何かをするのです。

1:01:44.420,1:01:49.640
そして、並列分散処理のフレームワークの中のこの部分が
非常に重要なのです。（PDP＝Parallel Distributed Processing）

1:01:49.640,1:01:50.640
そうですよね？

1:01:50.640,1:01:53.769
実際に何かをしているモデルがあります。

1:01:53.769,1:01:59.069
例えば、どこで逮捕されるかを予測する（行動を推奨するわけではありません）

1:01:59.069,1:02:02.460
警察のような予測モデルを構築したとします。

1:02:02.460,1:02:06.670
これはアメリカの多くの管轄区域で使われているものです。

1:02:06.670,1:02:10.809
今では、データに基づいて、ラベル付けされたデータに基づいて、
それを予測しています。

1:02:10.809,1:02:20.779
この場合、実際には（アメリカの）データを使っています。

1:02:20.779,1:02:25.099
黒人か白人かにもよりますが、アメリカの黒人は白人に比べて

1:02:25.099,1:02:31.480
マリファナ所持で逮捕される頻度が７倍くらい高いと思います。

1:02:31.480,1:02:38.579
実際のマリファナ使用量は白人と黒人でほぼ同じなのにもかかわらすです。

1:02:38.579,1:02:42.079
偏ったデータを元に取り締まり予測モデルを構築すると、

1:02:42.079,1:02:49.430
その予測はこのような偏ったデータに基づいて「ここに逮捕できる人がいるだろう」と

1:02:49.430,1:02:50.430
予測します。

1:02:50.430,1:02:56.279
そうすると、法執行機関の職員は、モデルが予測した地域に警察活動を

1:02:56.279,1:02:58.039
集中させることに決めるかもしれません。

1:02:58.039,1:03:01.940
その結果、そこでより多くの人を逮捕することになる。

1:03:01.940,1:03:05.430
そして、それを使ってモデルに戻す。

1:03:05.430,1:03:09.789
するとモデルは黒人居住区ではもっと多くの人を逮捕すべきだと

1:03:09.789,1:03:12.640
予測するようになり、それが繰り返されるのです。

1:03:12.640,1:03:17.000
これは、モデルが環境と相互作用する中で、どのように
正帰還フィードバックループと呼ばれるものを

1:03:17.000,1:03:19.460
作るのかの一例です。

1:03:19.460,1:03:23.930
モデルの利用増加に伴い、データはより偏ったものになり、

1:03:23.930,1:03:26.440
モデルはさらに偏ったものになります。

1:03:26.440,1:03:32.299
ですから、機械学習で注意しなければならないことの1つは、

1:03:32.299,1:03:39.009
モデルが実際にどのように使用されているかを認識し、
その結果として起こり得るかを認識することです。

1:03:39.009,1:03:48.910
これはプロキシの例であるということを伝えたいです。

1:03:48.910,1:03:56.210
なぜなら、ここでは逮捕が犯罪の代わりとして使われているからです。
ほとんどのケースで、実際に持っているデータは、

1:03:56.210,1:04:00.049
あなたが本当に関心のある値の代理だと思います。

1:04:00.049,1:04:06.109
そして、往々にして，プロキシと実際の値の差は大きいのです。

1:04:06.109,1:04:10.770
ありがとう、レイチェル。

1:04:10.770,1:04:13.430
本当に重要なポイントですね。

1:04:13.430,1:04:24.219
それでは最後に、このコードで何が起こっているのかを見てみましょう。

1:04:24.219,1:04:34.880
私たちが実行したコードは、基本的には、1,2,3,4,5,6行のコードです。

1:04:34.880,1:04:39.829
コードの最初の行はインポート行です。

1:04:39.829,1:04:46.690
Pythonでは、外部ライブラリをインポートしない限り、
それらを使うことはできません。

1:04:46.690,1:04:55.030
通常はライブラリから必要な関数やクラスだけをインポートします。

1:04:55.030,1:05:01.989
しかし、Pythonはモジュールからすべてをインポートできる
便利な機能を提供していて、

1:05:01.989,1:05:04.410
これはそこに星をつけることで実現しています。

1:05:04.410,1:05:07.029
ほとんどの場合、これは悪い考えです。

1:05:07.029,1:05:12.479
なぜなら、デフォルトでは、Pythonの動作方法は、
import *　を実行した場合、

1:05:12.479,1:05:18.520
ライブラリの中であなたが使いたい重要なものだけを
インポートするのではないからです。

1:05:18.520,1:05:23.369
実際には、使用した全てのライブラリや、使用した全てのライブラリが
importしているものもimportするため、

1:05:23.369,1:05:28.760
結果的に名前空間を爆発させてしまい、多くのバグを引き起こしてしまいます。

1:05:28.760,1:05:36.510
fastaiはREPL環境で素早くプロトタイピングができるように設計されていて、

1:05:36.510,1:05:41.779
私たちはこの問題を避けるために多くの時間を費やしました。

1:05:41.779,1:05:45.410
安全にimport * ができるように。

1:05:45.410,1:05:49.630
ですから、これをするかしないかはあなた次第です。

1:05:49.630,1:05:56.609
しかし、fastaiライブラリからimport * する場合は、

1:05:56.609,1:06:01.999
実際に必要な部分だけを取得できるように明示的に
設計されているので安心してください。

1:06:01.999,1:06:05.799
一つだけ言及しておきたいのは、ビデオの中で 
"fastai2"と呼ばれていることです。

1:06:05.799,1:06:09.849
その理由は収録時はプレリリース版を使っているからです。

1:06:09.849,1:06:19.049
オンライン版、MOOC版を見ている時には、2は取れているでしょう。

1:06:19.049,1:06:25.609
他にも言及しておきたいことがあります。
fastaiには4つの定義済みアプリケーションがあり、

1:06:25.609,1:06:31.099
それはvision、text、テーブル、協調フィルタリングです。

1:06:31.099,1:06:35.130
それぞれについて、詳しく学んでいく予定です。

1:06:35.130,1:06:41.989
アプリケーションはそれぞれ、例えばビジョンにしたら、
.allからインポートすることができます。

1:06:41.989,1:06:42.989
メタモデルのようなもの、と呼んでもいいかもしれませんね。

1:06:42.989,1:06:48.079
これで、一般的なvisionアプリケーションに
必要なほぼ全てがimportされます。

1:06:48.079,1:06:55.779
Jupyter NotebookのようなREPLシステムを使っていれば、

1:06:55.779,1:07:01.619
必要なものはすべてそこにあるので、戻って調べたりする必要はありません。

1:07:01.619,1:07:06.559
これの問題点の一つは、pythonユーザーの多くが

1:07:06.559,1:07:12.450
例えばuntar_dataがどこから来ているのかを知るのに、

1:07:12.450,1:07:14.219
インポートの行を見ているのです。

1:07:14.219,1:07:15.970
しかし　import *　すると、それがわからなくなってしまうのです。

1:07:15.970,1:07:19.829
REPLでは戻って調べる必要がないということです。

1:07:19.829,1:07:28.670
You can literally just type the symbol, press
SHIFT - ENTER and it will tell you exactly
調べたい記号を入力して [SHIFT - ENTER] を押すだけで、
それがどこから来たのかを正確に教えてくれます。

1:07:28.670,1:07:29.940
この通りです。

1:07:29.940,1:07:33.510
大変便利なのです。

1:07:33.510,1:07:43.859
この場合、例えば、実際にデータセットを構築するために、
ImageDataLoaders.from_name_func()を呼び出しました。

1:07:43.859,1:07:51.170
このdoc関数を使えば簡単にドキュメントを確認できます。

1:07:51.170,1:07:57.349
ご覧のように、デフォルト値を含めて全ての引数が確認できます。

1:07:57.349,1:08:11.190
そして最も重要なことは、[SHOW IN DOCS]は、使用例を含む
完全なドキュメントへのリンクになっています。

1:08:11.190,1:08:17.180
fastAIのドキュメントにはすべて例がついています。

1:08:17.180,1:08:25.770
さらに、それらは全てJupyter Notebookで書かれているので、

1:08:25.770,1:08:33.280
自分でコードを実行して、実際の動作や出力など確認できます。

1:08:33.280,1:08:36.760
また、ドキュメントにはたくさんのチュートリアルがあります。

1:08:36.760,1:08:40.501
例えば、visionのチュートリアルには多くのことが書かれていますが、

1:08:40.501,1:08:47.710
その1つは、レッスン１の内容をほぼカバーしています。

1:08:47.710,1:08:54.300
fastAIにはたくさんのドキュメントがあり、それを活用するのは
とても良いアイデアだと思います。

1:08:54.300,1:08:59.310
ドキュメントは完全に検索可能ですし、おそらく最も重要なことは、

1:08:59.310,1:09:05.050
これらのドキュメントのすべてが完全にインタラクティブな
Jupyter Notebookになっていることです。

1:09:05.050,1:09:13.839
そこで、このコードをさらに見てみると、importの後の最初の行は、

1:09:13.839,1:09:14.910
untar_dataを呼んでいます。

1:09:14.910,1:09:20.020
これはデータセットをダウンロードして、解凍します。

1:09:20.020,1:09:22.770
すでにダウンロードされている場合は、
再度ダウンロードすることはありませんし，

1:09:22.770,1:09:25.710
解凍済みなら解凍しません。

1:09:25.710,1:09:32.540
ご覧のように、fastAIはすでに多くの有用なデータセットを定義しています。

1:09:32.540,1:09:34.180
このペットデータセットは一例です。

1:09:34.180,1:09:39.920
データセットは深層学習で想像できるように超重要です。

1:09:39.920,1:09:41.900
これからたくさんのデータセットを見ていくでしょう。

1:09:41.900,1:09:47.690
そして、これらのデータセットは多くのヒーロー（とヒロイン）によって作成され、

1:09:47.690,1:09:51.260
基本的には数ヶ月から数年かけてデータを照合し、
これらのモデルを構築するために使用することができます。

1:09:51.260,1:10:01.230
次のステップは、このデータが何であるかをfastAIに伝えることであり、
データについて私たちは多くのことを学んでいきます。

1:10:01.230,1:10:05.560
しかし、今回は、基本的には「これには画像が含まれている」と言っています。

1:10:05.560,1:10:07.630
このパスにある画像が含まれています。

1:10:07.630,1:10:14.100
untar_dataは解凍済みのデータのパスを返します。

1:10:14.100,1:10:18.900
あるいはすでに解凍されている場合は、
以前に解凍された場所を教えてくれます。

1:10:18.900,1:10:23.560
そのパスの中に実際にどんな画像があるのかを
説明しなくてはいけません。

1:10:23.560,1:10:27.170
本当に興味深いものに label_func があります。

1:10:27.170,1:10:33.600
ファイルごとに、それが猫なのか犬なのかを
どうやって伝えるのでしょうか。

1:10:33.600,1:10:37.330
実際にオリジナルのデータセットのReadMEを見てみると、

1:10:37.330,1:10:42.430
少し風変わりで、「ああ、ファイル名の最初の文字が大文字のものは

1:10:42.430,1:10:45.000
何でも猫だ」ということです。

1:10:45.000,1:10:46.430
これは彼らが決めたことです。

1:10:46.430,1:10:51.180
そこで、ここにis_catという小さな関数を作って、
最初の文字が大文字かどうかを

1:10:51.180,1:10:52.480
返すようにしました。

1:10:52.480,1:10:57.420
これで猫かどうかを判断する方法をfastaiに伝えました。

1:10:57.420,1:11:01.260
この2つについては後で説明します。

1:11:01.260,1:11:04.640
次に、データが何であるかを伝えます。

1:11:04.640,1:11:06.350
そしてlearnerと呼ばれるものを作らなければなりません。

1:11:06.350,1:11:10.180
learnerとは学習するものです。
訓練をします。

1:11:10.180,1:11:12.510
だから、どのデータを使うか教えなければなりません。

1:11:12.510,1:11:16.570
そして、どのようなアーキテクチャを使うかを
設定しなければなりません。

1:11:16.570,1:11:19.810
このコースではアーキテクチャについてたくさんお話します。

1:11:19.810,1:11:25.460
しかし、基本的には、事前に定義されたニューラルネットワークの
アーキテクチャがたくさんあり、

1:11:25.460,1:11:26.660
それぞれ長所と短所があります。

1:11:26.660,1:11:30.360
コンピュータビジョン用のアーキテクチャは
ResNetと呼ばれています。

1:11:30.360,1:11:34.580
これは非常に素晴らしい出発点なので、ここでは小さ目なものを

1:11:34.580,1:11:35.820
使うことにします。

1:11:35.820,1:11:39.710
これらはすべて事前に定義されていて、すぐに使えます。

1:11:39.710,1:11:43.380
そして、訓練中にプリントアウトしたいものを
fastaiに伝えることができます。

1:11:43.380,1:11:47.730
ここでは、「トレーニング中に、誤差を教えてください」と言っています。

1:11:47.730,1:11:51.230
そして、fine_tuneと呼ばれるとても重要なメソッドを呼び出すことができます。

1:11:51.230,1:11:57.050
次のレッスンで説明しますが、fine_tuneが訓練を行います。

1:11:57.050,1:12:00.700
valid_pctは非常に重要なことをします。

1:12:00.700,1:12:09.230
この場合、データの20%は、モデルの学習には使用しません。

1:12:09.230,1:12:12.730
その代わりに、モデルの誤差率を知るために使います。

1:12:12.730,1:12:19.890
したがって、常にfastaiでは、このメトリック、error_rateは

1:12:19.890,1:12:22.290
訓練されていないデータの一部で計算されます。

1:12:22.290,1:12:27.040
この考え方については、今後のレッスンで詳しく説明します。

1:12:27.040,1:12:31.220
しかし、ここでの基本的な考え方は、過学習していないことを
確認したいということです。

1:12:31.220,1:12:33.540
説明しましょう。

1:12:33.540,1:12:35.090
過学習は次のようなものです。

1:12:35.090,1:12:38.860
例えば、これらの点にフィットする関数を
作ろうとしているとしましょう。

1:12:38.860,1:12:44.050
素敵な関数はこのようになりますよね？

1:12:44.050,1:12:47.940
しかし、この関数をはより正確にフィットしています。

1:12:47.940,1:12:50.970
見てください、これはこちらよりも
はるかにすべての点に近づいています。

1:12:50.970,1:12:53.240
ですから、明らかにこちらの方が優れた関数です。

1:12:53.240,1:13:00.010
ただし、点がある場所の外に出るとすぐに、特に端から外れてしまうと、

1:13:00.010,1:13:01.640
明らかに意味がありません。

1:13:01.640,1:13:06.060
これが過学習と呼ばれるものです。

1:13:06.060,1:13:08.570
過学習は様々な理由で起こります。

1:13:08.570,1:13:11.970
大きすぎるモデルを使ったり、十分なデータを使っていなかったり...

1:13:11.970,1:13:14.510
この話はこれからですね。

1:13:14.510,1:13:22.370
しかし、ディープラーニングの技術は、
適切なフィットを持つモデルを作成することが

1:13:22.370,1:13:23.370
すべてです。

1:13:23.370,1:13:27.480
モデルが適切にフィットしているかどうかを知る唯一の方法は、

1:13:27.480,1:13:31.620
それを訓練するために使用されていないデータで
うまく機能するかどうかを見ることです。

1:13:31.620,1:13:36.830
そのため、私たちは常にデータの一部を脇に置いて、
バリデーション・セットと呼ばれるものを作成します。

1:13:36.830,1:13:41.270
バリデーション・セットとは、モデルを訓練しているときには
全く触らないようにしているデータですが、

1:13:41.270,1:13:51.100
モデルが実際に機能しているかどうかを
把握するためだけに使用しています。

1:13:51.100,1:13:57.080
シルヴァンが本の中で言っていたことですが、
fastaiを勉強していて面白いことの一つは、

1:13:57.080,1:14:04.500
多くの面白いプログラミングを学ぶことができるということです。

1:14:04.500,1:14:11.000
私は子供の頃からプログラミングをしていたので、
かれこれ40年くらい経ちます。

1:14:11.000,1:14:18.060
そして、Sylvain と私は python に多くの仕事をさせ、

1:14:18.060,1:14:22.960
生産性を高め、何年も後になっても自分のコードを理解できるような

1:14:22.960,1:14:27.120
プログラミングの実践をするために、本当に一生懸命働いています。

1:14:27.120,1:14:34.700
私たちのコードの中ではよく、今まで見たことの
ないようなことをやっていることがあります。

1:14:34.700,1:14:39.260
以前のコースを受講した多くの学生は、
このコースでコーディングやPythonコーディング、

1:14:39.260,1:14:44.100
ソフトウェアエンジニアリングについて
多くを学んだと言っています。

1:14:44.100,1:14:49.350
だから、何か新しいものを見たときはチェックして、
なぜそのような方法で行われたのか

1:14:49.350,1:14:53.410
気になったらフォーラムで気軽に質問してください。

1:14:53.410,1:14:59.250
一つ言及しておきたいのは、先ほども述べたように、
ほとんどのPythonプログラマーが

1:14:59.250,1:15:05.890
import *を使うことはありません。ほとんどのライブラリが
適切にそれをサポートしていないからです。

1:15:05.890,1:15:07.130
私たちはそのようなことをたくさんやっています。

1:15:07.130,1:15:11.240
私たちは伝統的なPythonプログラミングのアプローチに
従わないことをたくさんやっています。

1:15:11.240,1:15:19.940
私は長年にわたって多くの言語を使ってきたので、特にPython的な方法ではなく、

1:15:19.940,1:15:23.940
他の多くの言語や他の多くの表記法からのアイデアを取り入れて、データサイエンスに適したものをベースに

1:15:23.940,1:15:31.230
Pythonプログラミングへのアプローチを大幅にカスタマイズしています。

1:15:34.000,1:15:40.750
つまり、あなたがfastaiで見たコードは、あなたの職場でPythonを使っている場合、

1:15:40.750,1:15:45.600
おそらく、あなたの職場のスタイルガイドや通常のアプローチには合わないでしょう。

1:15:45.600,1:15:52.440
ですから、私たちのやり方に従うのではなく、あなたの組織のプログラミングの

1:15:52.440,1:15:56.420
やり方に合うようにする必要があります。

1:15:56.420,1:16:00.910
しかし、おそらくあなた自身の趣味の仕事では、私たちのものに従ってみて、

1:16:00.910,1:16:05.270
それが面白くて参考になるかどうかを見てみてもいいでしょうし、
もしあなたが管理職で、そうすることに興味があるのであれば、

1:16:05.270,1:16:08.480
あなたの会社でそれを試してみるのもいいでしょう。

1:16:08.480,1:16:17.001
さて、最後に、かなり面白いものをお見せしましょう。

1:16:17.001,1:16:27.580
それは、このコードを見てみてください。untar_data、
ImageDataLoaders.from_name_funcから、

1:16:27.580,1:16:33.740
label_func、learner、fine_tuneします。

1:16:33.740,1:16:39.070
ほとんど同じコードですが、これは何か違う事をするモデルを構築しました。

1:16:39.070,1:16:42.520
これは画像を撮影したものです。

1:16:42.520,1:16:45.420
左側がラベル付きのデータです。

1:16:45.420,1:16:51.850
車なのか、木なのか、建物なのか、空なのか、ラインマークなのか、道路なのかがわかるように、

1:16:51.850,1:16:54.201
カラーコード付きの画像になっています。

1:16:54.201,1:16:59.410
右側は私たちのモデルですが、私たちのモデルは各ピクセルごとに、車なのか、

1:16:59.410,1:17:02.960
ラインマークなのか、道路なのかを判別することに成功しました。

1:17:02.960,1:17:06.520
たった20秒以内でこれを実現しました。

1:17:06.520,1:17:08.480
ですから、非常に小さなモデルです。

1:17:08.480,1:17:13.060
いくつかのミスをしています -- この線のマークを見落としていたり、

1:17:13.060,1:17:15.710
いくつかの車を家だと思っていたりなど。

1:17:15.710,1:17:21.400
しかし、数分間これを訓練すれば、ほぼ完璧になることがわかります。

1:17:21.400,1:17:26.300
基本的な考え方は、ほぼ同じコードを使って、猫や犬を分類するのではなく、

1:17:26.300,1:17:31.890
すべてのピクセルと画像が何であるかを把握するセグメンテーションと呼ばれるものを

1:17:31.890,1:17:34.950
非常に速く作ることができるということです。

1:17:34.950,1:17:36.980
見てください、同じものがあります。

1:17:36.980,1:17:39.520
Import *、TextDataLoaders.from_folder，

1:17:39.520,1:17:42.530
learner、fine_tune

1:17:42.530,1:17:43.830
基本的なコードは同じです。

1:17:43.830,1:17:49.250
これは、文章が表している感情が肯定的か

1:17:49.250,1:17:55.380
否定的かを理解するできるようになりました。

1:17:55.380,1:18:04.440
この同じ3行のコードで15分の訓練による93%という正確度は、

1:18:04.440,1:18:08.960
1000から3000語の何千もの映画レビューからなるIMDBデータセットの分類においては

1:18:08.960,1:18:13.500
2015年時点で世界最高のものだったと思います。

1:18:21.670,1:18:31.700
私たちは基本的に同じコードを使って、私たちのブラウザで、
世界クラスのモデルを作成したのです。

1:18:31.700,1:18:33.390
ここでも同じ基本的な手順です。

1:18:33.390,1:18:35.110
from import *、untar_data、

1:18:35.110,1:18:38.270
TabularDataLoaders.from_csv、

1:18:38.270,1:18:39.970
Learner、fit

1:18:39.970,1:18:52.030
これは今、これらの列を含むcsvに
基づいて給与を予測するモデルを

1:18:52.030,1:18:53.030
構築しています。

1:18:53.030,1:18:56.620
これは表形式のデータです。

1:18:56.620,1:18:57.620
ここでは基本的な手順は同じです。

1:18:57.620,1:18:59.600
Import *、untar_data、

1:18:59.600,1:19:02.700
CollabDataLoaders.from_csv、learner，fine_tune

1:19:02.700,1:19:10.850
これはユーザーと映画の組み合わせごとに、視聴履歴をもとに，

1:19:10.850,1:19:17.430
ユーザが映画をどう評価するかを予測するものを構築しています。

1:19:18.880,1:19:23.230
これは協調フィルタリングと呼ばれ、
レコメンデーションシステムで使用されています。

1:19:23.230,1:19:29.520
ここでは、fastaiの4つのアプリケーションの例を見てきました。

1:19:29.520,1:19:34.420
このコースを通してお分かりになると思いますが、基本的に同じコードと、

1:19:34.420,1:19:41.700
同じ数学とソフトウェア工学の基本的な概念によって、同じようなアプローチを使って、

1:19:41.700,1:19:43.980
幅広いいろいろなことができるのです。

1:19:43.980,1:19:47.180
なぜかというと、アーサー・サミュエルのおかげです。

1:19:47.180,1:19:56.840
もしもモデルをパラメータ化する方法と、重みを更新して

1:19:56.840,1:20:01.630
損失関数を改善する更新手順があれば、一体何ができるのか

1:20:01.630,1:20:09.850
という基本的な説明があるからです。
この場合、柔軟な関数であるニューラルネットワークを

1:20:09.850,1:20:14.350
使うことができます。

1:20:14.350,1:20:18.250
ということで、今回の最初のレッスンはここまで。

1:20:18.250,1:20:23.221
他のレッスンよりも少し短くなりますが、その理由は、先ほど述べたように、

1:20:23.221,1:20:29.850
私たちは世界的なパンデミックの始まりの時期にあるからです。

1:20:29.850,1:20:32.710
（少なくとも欧米では（他の国ではもっと進んでいます）

1:20:32.710,1:20:36.520
このことについては、コースの最初の方でお話しましたが、

1:20:36.520,1:20:39.500
そのビデオは別の場所で見ることができます。

1:20:39.500,1:20:45.730
今後のレッスンでは、より深層学習について学ぶことになるでしょう。

1:20:45.730,1:20:53.780
次のレッスンに取り組む前に、次の週までに以下に取り組むことをお勧めします。

1:20:53.780,1:20:58.470
GPUサーバをスピンアップして、終了したらシャットダウンして、

1:20:58.470,1:21:06.010
ここにあるすべてのコードを実行できることを確認して、

1:21:06.010,1:21:14.750
コードを読みながら自分が認識できる方法でPythonを使っているかを見たり、

1:21:14.750,1:21:20.480
ドキュメンテーションを使ってこれは何をするのかを調べたり、

1:21:20.480,1:21:22.270
fastaiのドキュメンテーションのノートブックを実際に実行したりしてみて下さい。

1:21:22.270,1:21:26.090
自分のやり方を知ることができれば、快適に過ごせるようになるでしょう。

1:21:26.090,1:21:30.340
この学習スタイル、トップダウン学習で最も重要なことは、

1:21:30.340,1:21:34.710
実験を行うことであり、コードを実行できるようになる必要があります。

1:21:34.710,1:21:41.360
ですから、私のお勧めは、コードを実行できるようになるまでは先に進まないで、

1:21:41.360,1:21:47.850
本の章を読んで、アンケートに答えてください。

1:21:47.850,1:21:52.980
検証セットやテストセット、転送学習については、まだまだやるべきことがあります。

1:21:52.980,1:21:58.360
ですから、まだすべてのことができるわけではありませんが、
これまでのコースを見てきたことに基づいて、

1:21:58.360,1:22:01.020
できる部分はすべてやってみてください。

1:22:01.020,1:22:04.130
レイチェル、何か追加したいことがあれば教えてください。

1:22:04.130,1:22:08.350
それでは、レッスン１に参加してくれてありがとうございました。

1:22:08.350,1:22:14.880
次回お会いできるのを本当に楽しみにしています。

1:22:14.880,1:22:22.150
次は転移学習について学び、その後、

1:22:22.150,1:22:27.500
実際にインターネット上で公開できるアプリの制作に移ります。

1:22:29.960,1:22:30.440
バイバイみんなさん。
